==============================================
1stage training start.
 dataset size: (17432, 10)
==============================================
len train loader :1
train tot iters :300
input_dim = 10, hidden_dim = 8, output_dim = 1

Ep: [10/300] (TRAIN) ---------------------
Loss: 349.804, L_rx: 336.574, L_kl: 1.138, L_c: 0.014, 
L_clf: 0.995, L_pred: 8.703
Ep: [10/300] (VALID)  ---------------------
L_rx: 336.915, L_rs: 0.752, L_pred: 6.409, L_clf: 0.965, 
L_clf_acc: 0.006

Ep: [20/300] (TRAIN) ---------------------
Loss: 827.897, L_rx: 331.121, L_kl: 0.863, L_c: 1.906, 
L_clf: 0.879, L_pred: 1.169
Ep: [20/300] (VALID)  ---------------------
L_rx: 331.499, L_rs: 0.687, L_pred: 1.035, L_clf: 0.894, 
L_clf_acc: 0.005

Ep: [30/300] (TRAIN) ---------------------
Loss: 826.260, L_rx: 329.567, L_kl: 1.779, L_c: 0.230, 
L_clf: 0.864, L_pred: 0.864
Ep: [30/300] (VALID)  ---------------------
L_rx: 330.382, L_rs: 0.697, L_pred: 0.883, L_clf: 0.871, 
L_clf_acc: 0.003

Ep: [40/300] (TRAIN) ---------------------
Loss: 824.755, L_rx: 329.625, L_kl: 0.447, L_c: 0.051, 
L_clf: 0.856, L_pred: 0.711
Ep: [40/300] (VALID)  ---------------------
L_rx: 330.452, L_rs: 0.685, L_pred: 0.857, L_clf: 0.862, 
L_clf_acc: 0.002

Ep: [50/300] (TRAIN) ---------------------
Loss: 823.930, L_rx: 329.493, L_kl: 0.273, L_c: 0.043, 
L_clf: 0.850, L_pred: 0.391
Ep: [50/300] (VALID)  ---------------------
L_rx: 330.346, L_rs: 0.666, L_pred: 0.909, L_clf: 0.851, 
L_clf_acc: 0.003

Ep: [60/300] (TRAIN) ---------------------
Loss: 823.484, L_rx: 329.402, L_kl: 0.227, L_c: 0.024, 
L_clf: 0.847, L_pred: 0.178
Ep: [60/300] (VALID)  ---------------------
L_rx: 330.334, L_rs: 0.583, L_pred: 1.679, L_clf: 0.857, 
L_clf_acc: 0.004

Ep: [70/300] (TRAIN) ---------------------
Loss: 822.935, L_rx: 329.140, L_kl: 0.243, L_c: 0.019, 
L_clf: 0.837, L_pred: 0.086
Ep: [70/300] (VALID)  ---------------------
L_rx: 330.320, L_rs: 0.315, L_pred: 1.296, L_clf: 0.838, 
L_clf_acc: 0.003

Ep: [80/300] (TRAIN) ---------------------
Loss: 822.494, L_rx: 328.906, L_kl: 0.223, L_c: 0.011, 
L_clf: 0.835, L_pred: 0.079
Ep: [80/300] (VALID)  ---------------------
L_rx: 330.324, L_rs: 0.093, L_pred: 1.552, L_clf: 0.842, 
L_clf_acc: 0.003

Ep: [90/300] (TRAIN) ---------------------
Loss: 822.386, L_rx: 328.867, L_kl: 0.220, L_c: 0.017, 
L_clf: 0.829, L_pred: 0.042
Ep: [90/300] (VALID)  ---------------------
L_rx: 330.320, L_rs: 0.044, L_pred: 1.313, L_clf: 0.840, 
L_clf_acc: 0.004

Ep: [100/300] (TRAIN) ---------------------
Loss: 822.359, L_rx: 328.846, L_kl: 0.181, L_c: 0.037, 
L_clf: 0.835, L_pred: 0.048
Ep: [100/300] (VALID)  ---------------------
L_rx: 330.316, L_rs: 0.008, L_pred: 1.380, L_clf: 0.835, 
L_clf_acc: 0.004

Ep: [110/300] (TRAIN) ---------------------
Loss: 822.275, L_rx: 328.797, L_kl: 0.192, L_c: 0.013, 
L_clf: 0.799, L_pred: 0.086
Ep: [110/300] (VALID)  ---------------------
L_rx: 330.321, L_rs: 0.011, L_pred: 1.531, L_clf: 0.803, 
L_clf_acc: 0.003

Ep: [120/300] (TRAIN) ---------------------
Loss: 822.319, L_rx: 328.793, L_kl: 0.196, L_c: 0.015, 
L_clf: 0.793, L_pred: 0.131
Ep: [120/300] (VALID)  ---------------------
L_rx: 330.317, L_rs: 0.005, L_pred: 0.924, L_clf: 0.799, 
L_clf_acc: 0.001

Ep: [130/300] (TRAIN) ---------------------
Loss: 822.315, L_rx: 328.794, L_kl: 0.193, L_c: 0.012, 
L_clf: 0.789, L_pred: 0.129
Ep: [130/300] (VALID)  ---------------------
L_rx: 330.320, L_rs: 0.005, L_pred: 1.517, L_clf: 0.784, 
L_clf_acc: 0.003

Ep: [140/300] (TRAIN) ---------------------
Loss: 822.225, L_rx: 328.792, L_kl: 0.183, L_c: 0.011, 
L_clf: 0.783, L_pred: 0.055
Ep: [140/300] (VALID)  ---------------------
L_rx: 330.316, L_rs: 0.004, L_pred: 1.553, L_clf: 0.784, 
L_clf_acc: 0.003

Ep: [150/300] (TRAIN) ---------------------
Loss: 822.218, L_rx: 328.791, L_kl: 0.179, L_c: 0.011, 
L_clf: 0.780, L_pred: 0.055
Ep: [150/300] (VALID)  ---------------------
L_rx: 330.314, L_rs: 0.004, L_pred: 1.047, L_clf: 0.796, 
L_clf_acc: 0.003

Ep: [160/300] (TRAIN) ---------------------
Loss: 822.243, L_rx: 328.788, L_kl: 0.185, L_c: 0.009, 
L_clf: 0.779, L_pred: 0.083
Ep: [160/300] (VALID)  ---------------------
L_rx: 330.309, L_rs: 0.004, L_pred: 1.507, L_clf: 0.786, 
L_clf_acc: 0.002

Ep: [170/300] (TRAIN) ---------------------
Loss: 822.180, L_rx: 328.786, L_kl: 0.176, L_c: 0.009, 
L_clf: 0.777, L_pred: 0.035
Ep: [170/300] (VALID)  ---------------------
L_rx: 330.305, L_rs: 0.004, L_pred: 1.507, L_clf: 0.793, 
L_clf_acc: 0.003

Ep: [180/300] (TRAIN) ---------------------
Loss: 822.174, L_rx: 328.783, L_kl: 0.173, L_c: 0.008, 
L_clf: 0.777, L_pred: 0.037
Ep: [180/300] (VALID)  ---------------------
L_rx: 330.304, L_rs: 0.007, L_pred: 1.153, L_clf: 0.785, 
L_clf_acc: 0.003

Ep: [190/300] (TRAIN) ---------------------
Loss: 822.181, L_rx: 328.789, L_kl: 0.175, L_c: 0.009, 
L_clf: 0.770, L_pred: 0.031
Ep: [190/300] (VALID)  ---------------------
L_rx: 330.307, L_rs: 0.003, L_pred: 1.373, L_clf: 0.784, 
L_clf_acc: 0.002

Ep: [200/300] (TRAIN) ---------------------
Loss: 822.152, L_rx: 328.783, L_kl: 0.166, L_c: 0.007, 
L_clf: 0.770, L_pred: 0.024
Ep: [200/300] (VALID)  ---------------------
L_rx: 330.306, L_rs: 0.002, L_pred: 1.375, L_clf: 0.780, 
L_clf_acc: 0.002

Ep: [210/300] (TRAIN) ---------------------
Loss: 822.144, L_rx: 328.781, L_kl: 0.162, L_c: 0.007, 
L_clf: 0.773, L_pred: 0.023
Ep: [210/300] (VALID)  ---------------------
L_rx: 330.302, L_rs: 0.003, L_pred: 1.330, L_clf: 0.783, 
L_clf_acc: 0.003

Ep: [220/300] (TRAIN) ---------------------
Loss: 822.133, L_rx: 328.779, L_kl: 0.158, L_c: 0.007, 
L_clf: 0.770, L_pred: 0.023
Ep: [220/300] (VALID)  ---------------------
L_rx: 330.300, L_rs: 0.002, L_pred: 1.306, L_clf: 0.782, 
L_clf_acc: 0.002

Ep: [230/300] (TRAIN) ---------------------
Loss: 822.126, L_rx: 328.777, L_kl: 0.154, L_c: 0.007, 
L_clf: 0.769, L_pred: 0.022
Ep: [230/300] (VALID)  ---------------------
L_rx: 330.301, L_rs: 0.002, L_pred: 1.332, L_clf: 0.787, 
L_clf_acc: 0.003

Ep: [240/300] (TRAIN) ---------------------
Loss: 822.120, L_rx: 328.777, L_kl: 0.152, L_c: 0.007, 
L_clf: 0.765, L_pred: 0.021
Ep: [240/300] (VALID)  ---------------------
L_rx: 330.300, L_rs: 0.002, L_pred: 1.347, L_clf: 0.778, 
L_clf_acc: 0.003

Ep: [250/300] (TRAIN) ---------------------
Loss: 822.116, L_rx: 328.777, L_kl: 0.148, L_c: 0.007, 
L_clf: 0.766, L_pred: 0.021
Ep: [250/300] (VALID)  ---------------------
L_rx: 330.299, L_rs: 0.001, L_pred: 1.329, L_clf: 0.782, 
L_clf_acc: 0.002

Ep: [260/300] (TRAIN) ---------------------
Loss: 822.113, L_rx: 328.776, L_kl: 0.147, L_c: 0.007, 
L_clf: 0.767, L_pred: 0.021
Ep: [260/300] (VALID)  ---------------------
L_rx: 330.299, L_rs: 0.001, L_pred: 1.331, L_clf: 0.773, 
L_clf_acc: 0.002

Ep: [270/300] (TRAIN) ---------------------
Loss: 822.111, L_rx: 328.776, L_kl: 0.145, L_c: 0.007, 
L_clf: 0.765, L_pred: 0.021
Ep: [270/300] (VALID)  ---------------------
L_rx: 330.299, L_rs: 0.001, L_pred: 1.320, L_clf: 0.780, 
L_clf_acc: 0.002

Ep: [280/300] (TRAIN) ---------------------
Loss: 822.109, L_rx: 328.776, L_kl: 0.144, L_c: 0.007, 
L_clf: 0.767, L_pred: 0.021
Ep: [280/300] (VALID)  ---------------------
L_rx: 330.299, L_rs: 0.001, L_pred: 1.340, L_clf: 0.776, 
L_clf_acc: 0.003

Ep: [290/300] (TRAIN) ---------------------
Loss: 822.108, L_rx: 328.776, L_kl: 0.143, L_c: 0.007, 
L_clf: 0.767, L_pred: 0.020
Ep: [290/300] (VALID)  ---------------------
L_rx: 330.300, L_rs: 0.002, L_pred: 1.309, L_clf: 0.774, 
L_clf_acc: 0.003

Ep: [300/300] (TRAIN) ---------------------
Loss: 822.108, L_rx: 328.776, L_kl: 0.143, L_c: 0.007, 
L_clf: 0.768, L_pred: 0.021
Ep: [300/300] (VALID)  ---------------------
L_rx: 330.299, L_rs: 0.001, L_pred: 1.324, L_clf: 0.777, 
L_clf_acc: 0.003
input_dim = 10, hidden_dim = 8, output_dim = 1
==============================================
1stage training start.
 dataset size: (17432, 10)
==============================================
len train loader :1
train tot iters :300
input_dim = 10, hidden_dim = 8, output_dim = 1

Ep: [10/300] (TRAIN) ---------------------
Loss: 351.110, L_rx: 337.628, L_kl: 0.580, L_c: 0.014, 
L_clf: 1.082, L_pred: 9.501
Ep: [10/300] (VALID)  ---------------------
L_rx: 339.657, L_rs: 0.857, L_pred: 7.657, L_clf: 1.081, 
L_clf_acc: 0.002

Ep: [20/300] (TRAIN) ---------------------
Loss: 839.428, L_rx: 335.639, L_kl: 0.288, L_c: 2.191, 
L_clf: 0.958, L_pred: 1.794
Ep: [20/300] (VALID)  ---------------------
L_rx: 337.401, L_rs: 0.801, L_pred: 1.532, L_clf: 0.934, 
L_clf_acc: 0.002

Ep: [30/300] (TRAIN) ---------------------
Loss: 828.498, L_rx: 330.087, L_kl: 0.519, L_c: 1.968, 
L_clf: 0.871, L_pred: 1.343
Ep: [30/300] (VALID)  ---------------------
L_rx: 332.192, L_rs: 0.709, L_pred: 1.164, L_clf: 0.885, 
L_clf_acc: 0.001

Ep: [40/300] (TRAIN) ---------------------
Loss: 825.919, L_rx: 329.736, L_kl: 1.049, L_c: 0.338, 
L_clf: 0.866, L_pred: 0.720
Ep: [40/300] (VALID)  ---------------------
L_rx: 332.005, L_rs: 0.689, L_pred: 0.966, L_clf: 0.877, 
L_clf_acc: 0.003

Ep: [50/300] (TRAIN) ---------------------
Loss: 824.680, L_rx: 329.737, L_kl: 0.377, L_c: 0.044, 
L_clf: 0.864, L_pred: 0.432
Ep: [50/300] (VALID)  ---------------------
L_rx: 332.016, L_rs: 0.683, L_pred: 1.049, L_clf: 0.875, 
L_clf_acc: 0.001

Ep: [60/300] (TRAIN) ---------------------
Loss: 824.199, L_rx: 329.710, L_kl: 0.301, L_c: 0.047, 
L_clf: 0.858, L_pred: 0.075
Ep: [60/300] (VALID)  ---------------------
L_rx: 332.008, L_rs: 0.675, L_pred: 1.674, L_clf: 0.866, 
L_clf_acc: 0.002

Ep: [70/300] (TRAIN) ---------------------
Loss: 823.972, L_rx: 329.650, L_kl: 0.236, L_c: 0.017, 
L_clf: 0.858, L_pred: 0.049
Ep: [70/300] (VALID)  ---------------------
L_rx: 332.016, L_rs: 0.579, L_pred: 1.610, L_clf: 0.869, 
L_clf_acc: 0.001

Ep: [80/300] (TRAIN) ---------------------
Loss: 823.568, L_rx: 329.437, L_kl: 0.217, L_c: 0.008, 
L_clf: 0.851, L_pred: 0.050
Ep: [80/300] (VALID)  ---------------------
L_rx: 332.008, L_rs: 0.351, L_pred: 1.696, L_clf: 0.855, 
L_clf_acc: 0.001

Ep: [90/300] (TRAIN) ---------------------
Loss: 823.124, L_rx: 329.146, L_kl: 0.220, L_c: 0.008, 
L_clf: 0.842, L_pred: 0.112
Ep: [90/300] (VALID)  ---------------------
L_rx: 332.006, L_rs: 0.213, L_pred: 1.175, L_clf: 0.849, 
L_clf_acc: 0.001

Ep: [100/300] (TRAIN) ---------------------
Loss: 823.329, L_rx: 329.294, L_kl: 0.215, L_c: 0.015, 
L_clf: 0.836, L_pred: 0.055
Ep: [100/300] (VALID)  ---------------------
L_rx: 332.010, L_rs: 0.039, L_pred: 1.196, L_clf: 0.845, 
L_clf_acc: 0.001

Ep: [110/300] (TRAIN) ---------------------
Loss: 822.905, L_rx: 329.062, L_kl: 0.220, L_c: 0.006, 
L_clf: 0.824, L_pred: 0.042
Ep: [110/300] (VALID)  ---------------------
L_rx: 332.013, L_rs: 0.081, L_pred: 1.238, L_clf: 0.835, 
L_clf_acc: 0.001

Ep: [120/300] (TRAIN) ---------------------
Loss: 822.906, L_rx: 329.052, L_kl: 0.220, L_c: 0.005, 
L_clf: 0.803, L_pred: 0.064
Ep: [120/300] (VALID)  ---------------------
L_rx: 332.008, L_rs: 0.032, L_pred: 1.500, L_clf: 0.813, 
L_clf_acc: 0.001

Ep: [130/300] (TRAIN) ---------------------
Loss: 822.871, L_rx: 329.058, L_kl: 0.195, L_c: 0.013, 
L_clf: 0.802, L_pred: 0.035
Ep: [130/300] (VALID)  ---------------------
L_rx: 332.009, L_rs: 0.024, L_pred: 1.225, L_clf: 0.819, 
L_clf_acc: 0.002

Ep: [140/300] (TRAIN) ---------------------
Loss: 822.826, L_rx: 329.039, L_kl: 0.194, L_c: 0.008, 
L_clf: 0.797, L_pred: 0.031
Ep: [140/300] (VALID)  ---------------------
L_rx: 332.008, L_rs: 0.002, L_pred: 1.170, L_clf: 0.813, 
L_clf_acc: 0.001

Ep: [150/300] (TRAIN) ---------------------
Loss: 822.805, L_rx: 329.034, L_kl: 0.179, L_c: 0.010, 
L_clf: 0.793, L_pred: 0.031
Ep: [150/300] (VALID)  ---------------------
L_rx: 332.007, L_rs: 0.003, L_pred: 1.339, L_clf: 0.801, 
L_clf_acc: 0.002

Ep: [160/300] (TRAIN) ---------------------
Loss: 822.803, L_rx: 329.036, L_kl: 0.172, L_c: 0.011, 
L_clf: 0.791, L_pred: 0.032
Ep: [160/300] (VALID)  ---------------------
L_rx: 332.006, L_rs: 0.002, L_pred: 1.337, L_clf: 0.812, 
L_clf_acc: 0.002

Ep: [170/300] (TRAIN) ---------------------
Loss: 822.793, L_rx: 329.033, L_kl: 0.167, L_c: 0.011, 
L_clf: 0.790, L_pred: 0.033
Ep: [170/300] (VALID)  ---------------------
L_rx: 332.007, L_rs: 0.001, L_pred: 1.229, L_clf: 0.810, 
L_clf_acc: 0.001

Ep: [180/300] (TRAIN) ---------------------
Loss: 822.782, L_rx: 329.032, L_kl: 0.165, L_c: 0.009, 
L_clf: 0.791, L_pred: 0.030
Ep: [180/300] (VALID)  ---------------------
L_rx: 332.006, L_rs: 0.001, L_pred: 1.255, L_clf: 0.799, 
L_clf_acc: 0.002

Ep: [190/300] (TRAIN) ---------------------
Loss: 822.776, L_rx: 329.031, L_kl: 0.162, L_c: 0.008, 
L_clf: 0.789, L_pred: 0.030
Ep: [190/300] (VALID)  ---------------------
L_rx: 332.006, L_rs: 0.001, L_pred: 1.281, L_clf: 0.791, 
L_clf_acc: 0.001

Ep: [200/300] (TRAIN) ---------------------
Loss: 822.770, L_rx: 329.030, L_kl: 0.158, L_c: 0.008, 
L_clf: 0.785, L_pred: 0.030
Ep: [200/300] (VALID)  ---------------------
L_rx: 332.004, L_rs: 0.001, L_pred: 1.265, L_clf: 0.791, 
L_clf_acc: 0.001

Ep: [210/300] (TRAIN) ---------------------
Loss: 822.766, L_rx: 329.029, L_kl: 0.155, L_c: 0.008, 
L_clf: 0.783, L_pred: 0.030
Ep: [210/300] (VALID)  ---------------------
L_rx: 332.004, L_rs: 0.001, L_pred: 1.300, L_clf: 0.801, 
L_clf_acc: 0.001

Ep: [220/300] (TRAIN) ---------------------
Loss: 822.760, L_rx: 329.028, L_kl: 0.153, L_c: 0.008, 
L_clf: 0.786, L_pred: 0.029
Ep: [220/300] (VALID)  ---------------------
L_rx: 332.003, L_rs: 0.001, L_pred: 1.294, L_clf: 0.796, 
L_clf_acc: 0.001

Ep: [230/300] (TRAIN) ---------------------
Loss: 822.755, L_rx: 329.027, L_kl: 0.151, L_c: 0.008, 
L_clf: 0.785, L_pred: 0.029
Ep: [230/300] (VALID)  ---------------------
L_rx: 332.001, L_rs: 0.001, L_pred: 1.298, L_clf: 0.789, 
L_clf_acc: 0.001

Ep: [240/300] (TRAIN) ---------------------
Loss: 822.751, L_rx: 329.026, L_kl: 0.150, L_c: 0.007, 
L_clf: 0.781, L_pred: 0.029
Ep: [240/300] (VALID)  ---------------------
L_rx: 332.001, L_rs: 0.001, L_pred: 1.293, L_clf: 0.792, 
L_clf_acc: 0.001

Ep: [250/300] (TRAIN) ---------------------
Loss: 822.746, L_rx: 329.025, L_kl: 0.149, L_c: 0.007, 
L_clf: 0.779, L_pred: 0.028
Ep: [250/300] (VALID)  ---------------------
L_rx: 332.000, L_rs: 0.001, L_pred: 1.286, L_clf: 0.799, 
L_clf_acc: 0.001

Ep: [260/300] (TRAIN) ---------------------
Loss: 822.742, L_rx: 329.024, L_kl: 0.149, L_c: 0.007, 
L_clf: 0.779, L_pred: 0.028
Ep: [260/300] (VALID)  ---------------------
L_rx: 331.997, L_rs: 0.000, L_pred: 1.318, L_clf: 0.793, 
L_clf_acc: 0.001

Ep: [270/300] (TRAIN) ---------------------
Loss: 822.740, L_rx: 329.023, L_kl: 0.148, L_c: 0.007, 
L_clf: 0.785, L_pred: 0.028
Ep: [270/300] (VALID)  ---------------------
L_rx: 331.998, L_rs: 0.000, L_pred: 1.283, L_clf: 0.793, 
L_clf_acc: 0.002

Ep: [280/300] (TRAIN) ---------------------
Loss: 822.737, L_rx: 329.022, L_kl: 0.148, L_c: 0.007, 
L_clf: 0.779, L_pred: 0.028
Ep: [280/300] (VALID)  ---------------------
L_rx: 331.994, L_rs: 0.000, L_pred: 1.312, L_clf: 0.788, 
L_clf_acc: 0.001

Ep: [290/300] (TRAIN) ---------------------
Loss: 822.736, L_rx: 329.021, L_kl: 0.148, L_c: 0.007, 
L_clf: 0.779, L_pred: 0.027
Ep: [290/300] (VALID)  ---------------------
L_rx: 331.993, L_rs: 0.000, L_pred: 1.267, L_clf: 0.779, 
L_clf_acc: 0.001

Ep: [300/300] (TRAIN) ---------------------
Loss: 822.735, L_rx: 329.021, L_kl: 0.148, L_c: 0.007, 
L_clf: 0.779, L_pred: 0.027
Ep: [300/300] (VALID)  ---------------------
L_rx: 331.993, L_rs: 0.000, L_pred: 1.283, L_clf: 0.784, 
L_clf_acc: 0.001
input_dim = 10, hidden_dim = 8, output_dim = 1
==============================================
1stage training start.
 dataset size: (17432, 10)
==============================================
len train loader :1
train tot iters :300
input_dim = 10, hidden_dim = 8, output_dim = 1

Ep: [10/300] (TRAIN) ---------------------
Loss: 340.984, L_rx: 335.557, L_kl: 0.797, L_c: 0.014, 
L_clf: 1.046, L_pred: 1.251
Ep: [10/300] (VALID)  ---------------------
L_rx: 335.057, L_rs: 0.964, L_pred: 1.419, L_clf: 1.020, 
L_clf_acc: 0.002

Ep: [20/300] (TRAIN) ---------------------
Loss: 828.363, L_rx: 331.428, L_kl: 0.756, L_c: 2.061, 
L_clf: 0.917, L_pred: 0.996
Ep: [20/300] (VALID)  ---------------------
L_rx: 331.140, L_rs: 1.019, L_pred: 1.023, L_clf: 0.902, 
L_clf_acc: 0.001

Ep: [30/300] (TRAIN) ---------------------
Loss: 827.487, L_rx: 329.861, L_kl: 1.168, L_c: 1.552, 
L_clf: 0.884, L_pred: 0.641
Ep: [30/300] (VALID)  ---------------------
L_rx: 330.052, L_rs: 0.692, L_pred: 0.933, L_clf: 0.877, 
L_clf_acc: 0.001

Ep: [40/300] (TRAIN) ---------------------
Loss: 825.168, L_rx: 329.761, L_kl: 0.724, L_c: 0.115, 
L_clf: 0.876, L_pred: 0.446
Ep: [40/300] (VALID)  ---------------------
L_rx: 329.992, L_rs: 0.709, L_pred: 1.039, L_clf: 0.871, 
L_clf_acc: 0.002

Ep: [50/300] (TRAIN) ---------------------
Loss: 824.414, L_rx: 329.741, L_kl: 0.416, L_c: 0.043, 
L_clf: 0.868, L_pred: 0.104
Ep: [50/300] (VALID)  ---------------------
L_rx: 329.990, L_rs: 0.676, L_pred: 1.610, L_clf: 0.866, 
L_clf_acc: 0.001

Ep: [60/300] (TRAIN) ---------------------
Loss: 824.176, L_rx: 329.719, L_kl: 0.276, L_c: 0.024, 
L_clf: 0.851, L_pred: 0.062
Ep: [60/300] (VALID)  ---------------------
L_rx: 329.992, L_rs: 0.642, L_pred: 1.679, L_clf: 0.840, 
L_clf_acc: 0.001

Ep: [70/300] (TRAIN) ---------------------
Loss: 823.978, L_rx: 329.630, L_kl: 0.235, L_c: 0.012, 
L_clf: 0.822, L_pred: 0.078
Ep: [70/300] (VALID)  ---------------------
L_rx: 329.984, L_rs: 0.545, L_pred: 1.499, L_clf: 0.835, 
L_clf_acc: 0.001

Ep: [80/300] (TRAIN) ---------------------
Loss: 823.695, L_rx: 329.394, L_kl: 0.253, L_c: 0.021, 
L_clf: 0.811, L_pred: 0.178
Ep: [80/300] (VALID)  ---------------------
L_rx: 329.988, L_rs: 0.281, L_pred: 1.415, L_clf: 0.837, 
L_clf_acc: 0.002

Ep: [90/300] (TRAIN) ---------------------
Loss: 823.189, L_rx: 329.170, L_kl: 0.286, L_c: 0.011, 
L_clf: 0.809, L_pred: 0.045
Ep: [90/300] (VALID)  ---------------------
L_rx: 329.981, L_rs: 0.094, L_pred: 1.634, L_clf: 0.825, 
L_clf_acc: 0.002

Ep: [100/300] (TRAIN) ---------------------
Loss: 823.463, L_rx: 329.275, L_kl: 0.309, L_c: 0.011, 
L_clf: 0.803, L_pred: 0.118
Ep: [100/300] (VALID)  ---------------------
L_rx: 329.973, L_rs: 0.337, L_pred: 2.339, L_clf: 0.825, 
L_clf_acc: 0.002

Ep: [110/300] (TRAIN) ---------------------
Loss: 823.083, L_rx: 329.086, L_kl: 0.255, L_c: 0.014, 
L_clf: 0.800, L_pred: 0.112
Ep: [110/300] (VALID)  ---------------------
L_rx: 329.983, L_rs: 0.016, L_pred: 1.785, L_clf: 0.808, 
L_clf_acc: 0.002

Ep: [120/300] (TRAIN) ---------------------
Loss: 823.015, L_rx: 329.078, L_kl: 0.250, L_c: 0.009, 
L_clf: 0.802, L_pred: 0.071
Ep: [120/300] (VALID)  ---------------------
L_rx: 329.979, L_rs: 0.013, L_pred: 1.198, L_clf: 0.826, 
L_clf_acc: 0.002

Ep: [130/300] (TRAIN) ---------------------
Loss: 823.012, L_rx: 329.079, L_kl: 0.235, L_c: 0.011, 
L_clf: 0.798, L_pred: 0.078
Ep: [130/300] (VALID)  ---------------------
L_rx: 329.978, L_rs: 0.013, L_pred: 1.611, L_clf: 0.816, 
L_clf_acc: 0.002

Ep: [140/300] (TRAIN) ---------------------
Loss: 822.965, L_rx: 329.076, L_kl: 0.228, L_c: 0.008, 
L_clf: 0.791, L_pred: 0.048
Ep: [140/300] (VALID)  ---------------------
L_rx: 329.980, L_rs: 0.012, L_pred: 1.253, L_clf: 0.807, 
L_clf_acc: 0.002

Ep: [150/300] (TRAIN) ---------------------
Loss: 822.974, L_rx: 329.075, L_kl: 0.224, L_c: 0.010, 
L_clf: 0.798, L_pred: 0.061
Ep: [150/300] (VALID)  ---------------------
L_rx: 329.978, L_rs: 0.012, L_pred: 1.529, L_clf: 0.819, 
L_clf_acc: 0.003

Ep: [160/300] (TRAIN) ---------------------
Loss: 822.940, L_rx: 329.073, L_kl: 0.215, L_c: 0.009, 
L_clf: 0.792, L_pred: 0.041
Ep: [160/300] (VALID)  ---------------------
L_rx: 329.978, L_rs: 0.009, L_pred: 1.309, L_clf: 0.814, 
L_clf_acc: 0.002

Ep: [170/300] (TRAIN) ---------------------
Loss: 822.927, L_rx: 329.072, L_kl: 0.211, L_c: 0.009, 
L_clf: 0.788, L_pred: 0.035
Ep: [170/300] (VALID)  ---------------------
L_rx: 329.977, L_rs: 0.009, L_pred: 1.687, L_clf: 0.815, 
L_clf_acc: 0.002

Ep: [180/300] (TRAIN) ---------------------
Loss: 822.939, L_rx: 329.069, L_kl: 0.207, L_c: 0.009, 
L_clf: 0.782, L_pred: 0.055
Ep: [180/300] (VALID)  ---------------------
L_rx: 329.976, L_rs: 0.007, L_pred: 1.160, L_clf: 0.807, 
L_clf_acc: 0.002

Ep: [190/300] (TRAIN) ---------------------
Loss: 822.921, L_rx: 329.068, L_kl: 0.203, L_c: 0.009, 
L_clf: 0.783, L_pred: 0.043
Ep: [190/300] (VALID)  ---------------------
L_rx: 329.976, L_rs: 0.007, L_pred: 1.225, L_clf: 0.801, 
L_clf_acc: 0.002

Ep: [200/300] (TRAIN) ---------------------
Loss: 822.908, L_rx: 329.069, L_kl: 0.199, L_c: 0.008, 
L_clf: 0.784, L_pred: 0.034
Ep: [200/300] (VALID)  ---------------------
L_rx: 329.975, L_rs: 0.007, L_pred: 1.186, L_clf: 0.803, 
L_clf_acc: 0.002

Ep: [210/300] (TRAIN) ---------------------
Loss: 822.907, L_rx: 329.069, L_kl: 0.197, L_c: 0.009, 
L_clf: 0.780, L_pred: 0.035
Ep: [210/300] (VALID)  ---------------------
L_rx: 329.975, L_rs: 0.006, L_pred: 1.533, L_clf: 0.801, 
L_clf_acc: 0.002

Ep: [220/300] (TRAIN) ---------------------
Loss: 822.901, L_rx: 329.067, L_kl: 0.192, L_c: 0.008, 
L_clf: 0.780, L_pred: 0.037
Ep: [220/300] (VALID)  ---------------------
L_rx: 329.975, L_rs: 0.006, L_pred: 1.303, L_clf: 0.802, 
L_clf_acc: 0.002

Ep: [230/300] (TRAIN) ---------------------
Loss: 822.892, L_rx: 329.067, L_kl: 0.192, L_c: 0.008, 
L_clf: 0.781, L_pred: 0.029
Ep: [230/300] (VALID)  ---------------------
L_rx: 329.975, L_rs: 0.005, L_pred: 1.236, L_clf: 0.806, 
L_clf_acc: 0.002

Ep: [240/300] (TRAIN) ---------------------
Loss: 822.891, L_rx: 329.067, L_kl: 0.189, L_c: 0.008, 
L_clf: 0.778, L_pred: 0.032
Ep: [240/300] (VALID)  ---------------------
L_rx: 329.975, L_rs: 0.005, L_pred: 1.462, L_clf: 0.803, 
L_clf_acc: 0.002

Ep: [250/300] (TRAIN) ---------------------
Loss: 822.889, L_rx: 329.066, L_kl: 0.187, L_c: 0.008, 
L_clf: 0.774, L_pred: 0.032
Ep: [250/300] (VALID)  ---------------------
L_rx: 329.975, L_rs: 0.005, L_pred: 1.259, L_clf: 0.795, 
L_clf_acc: 0.002

Ep: [260/300] (TRAIN) ---------------------
Loss: 822.883, L_rx: 329.065, L_kl: 0.186, L_c: 0.008, 
L_clf: 0.773, L_pred: 0.029
Ep: [260/300] (VALID)  ---------------------
L_rx: 329.974, L_rs: 0.005, L_pred: 1.299, L_clf: 0.799, 
L_clf_acc: 0.002

Ep: [270/300] (TRAIN) ---------------------
Loss: 822.880, L_rx: 329.066, L_kl: 0.184, L_c: 0.008, 
L_clf: 0.774, L_pred: 0.028
Ep: [270/300] (VALID)  ---------------------
L_rx: 329.975, L_rs: 0.005, L_pred: 1.342, L_clf: 0.787, 
L_clf_acc: 0.001

Ep: [280/300] (TRAIN) ---------------------
Loss: 822.880, L_rx: 329.065, L_kl: 0.184, L_c: 0.008, 
L_clf: 0.778, L_pred: 0.029
Ep: [280/300] (VALID)  ---------------------
L_rx: 329.974, L_rs: 0.005, L_pred: 1.340, L_clf: 0.788, 
L_clf_acc: 0.002

Ep: [290/300] (TRAIN) ---------------------
Loss: 822.879, L_rx: 329.065, L_kl: 0.183, L_c: 0.008, 
L_clf: 0.774, L_pred: 0.028
Ep: [290/300] (VALID)  ---------------------
L_rx: 329.974, L_rs: 0.005, L_pred: 1.297, L_clf: 0.788, 
L_clf_acc: 0.002

Ep: [300/300] (TRAIN) ---------------------
Loss: 822.878, L_rx: 329.065, L_kl: 0.183, L_c: 0.008, 
L_clf: 0.777, L_pred: 0.028
Ep: [300/300] (VALID)  ---------------------
L_rx: 329.975, L_rs: 0.005, L_pred: 1.304, L_clf: 0.792, 
L_clf_acc: 0.002
input_dim = 10, hidden_dim = 8, output_dim = 1
==============================================
1stage training start.
 dataset size: (17432, 10)
==============================================
len train loader :1
train tot iters :300
input_dim = 10, hidden_dim = 8, output_dim = 1

Ep: [10/300] (TRAIN) ---------------------
Loss: 345.420, L_rx: 339.594, L_kl: 0.453, L_c: 0.016, 
L_clf: 1.267, L_pred: 1.952
Ep: [10/300] (VALID)  ---------------------
L_rx: 337.926, L_rs: 0.800, L_pred: 1.799, L_clf: 1.240, 
L_clf_acc: 0.003

Ep: [20/300] (TRAIN) ---------------------
Loss: 838.463, L_rx: 335.340, L_kl: 0.310, L_c: 2.457, 
L_clf: 0.938, L_pred: 1.273
Ep: [20/300] (VALID)  ---------------------
L_rx: 333.353, L_rs: 0.801, L_pred: 1.351, L_clf: 0.954, 
L_clf_acc: 0.006

Ep: [30/300] (TRAIN) ---------------------
Loss: 829.416, L_rx: 330.183, L_kl: 1.262, L_c: 1.776, 
L_clf: 0.873, L_pred: 1.491
Ep: [30/300] (VALID)  ---------------------
L_rx: 328.781, L_rs: 0.770, L_pred: 1.266, L_clf: 0.903, 
L_clf_acc: 0.005

Ep: [40/300] (TRAIN) ---------------------
Loss: 826.616, L_rx: 329.839, L_kl: 1.499, L_c: 0.051, 
L_clf: 0.859, L_pred: 0.993
Ep: [40/300] (VALID)  ---------------------
L_rx: 328.597, L_rs: 0.718, L_pred: 0.979, L_clf: 0.902, 
L_clf_acc: 0.005

Ep: [50/300] (TRAIN) ---------------------
Loss: 825.463, L_rx: 329.908, L_kl: 0.330, L_c: 0.108, 
L_clf: 0.849, L_pred: 0.770
Ep: [50/300] (VALID)  ---------------------
L_rx: 328.658, L_rs: 0.681, L_pred: 0.926, L_clf: 0.886, 
L_clf_acc: 0.006

Ep: [60/300] (TRAIN) ---------------------
Loss: 824.873, L_rx: 329.809, L_kl: 0.269, L_c: 0.015, 
L_clf: 0.846, L_pred: 0.571
Ep: [60/300] (VALID)  ---------------------
L_rx: 328.612, L_rs: 0.670, L_pred: 0.894, L_clf: 0.892, 
L_clf_acc: 0.004

Ep: [70/300] (TRAIN) ---------------------
Loss: 824.234, L_rx: 329.715, L_kl: 0.251, L_c: 0.013, 
L_clf: 0.843, L_pred: 0.122
Ep: [70/300] (VALID)  ---------------------
L_rx: 328.596, L_rs: 0.546, L_pred: 1.193, L_clf: 0.887, 
L_clf_acc: 0.004

Ep: [80/300] (TRAIN) ---------------------
Loss: 823.364, L_rx: 329.239, L_kl: 0.293, L_c: 0.014, 
L_clf: 0.835, L_pred: 0.043
Ep: [80/300] (VALID)  ---------------------
L_rx: 328.610, L_rs: 0.073, L_pred: 1.541, L_clf: 0.874, 
L_clf_acc: 0.005

Ep: [90/300] (TRAIN) ---------------------
Loss: 823.187, L_rx: 329.143, L_kl: 0.286, L_c: 0.007, 
L_clf: 0.812, L_pred: 0.052
Ep: [90/300] (VALID)  ---------------------
L_rx: 328.584, L_rs: 0.008, L_pred: 1.247, L_clf: 0.859, 
L_clf_acc: 0.005

Ep: [100/300] (TRAIN) ---------------------
Loss: 823.138, L_rx: 329.135, L_kl: 0.234, L_c: 0.014, 
L_clf: 0.798, L_pred: 0.060
Ep: [100/300] (VALID)  ---------------------
L_rx: 328.583, L_rs: 0.005, L_pred: 1.349, L_clf: 0.847, 
L_clf_acc: 0.005

Ep: [110/300] (TRAIN) ---------------------
Loss: 823.172, L_rx: 329.180, L_kl: 0.218, L_c: 0.020, 
L_clf: 0.795, L_pred: 0.028
Ep: [110/300] (VALID)  ---------------------
L_rx: 328.585, L_rs: 0.005, L_pred: 1.276, L_clf: 0.837, 
L_clf_acc: 0.004

Ep: [120/300] (TRAIN) ---------------------
Loss: 823.084, L_rx: 329.125, L_kl: 0.210, L_c: 0.008, 
L_clf: 0.786, L_pred: 0.056
Ep: [120/300] (VALID)  ---------------------
L_rx: 328.582, L_rs: 0.012, L_pred: 1.170, L_clf: 0.823, 
L_clf_acc: 0.003

Ep: [130/300] (TRAIN) ---------------------
Loss: 823.044, L_rx: 329.124, L_kl: 0.197, L_c: 0.009, 
L_clf: 0.788, L_pred: 0.028
Ep: [130/300] (VALID)  ---------------------
L_rx: 328.582, L_rs: 0.003, L_pred: 1.483, L_clf: 0.835, 
L_clf_acc: 0.004

Ep: [140/300] (TRAIN) ---------------------
Loss: 823.035, L_rx: 329.122, L_kl: 0.192, L_c: 0.008, 
L_clf: 0.785, L_pred: 0.031
Ep: [140/300] (VALID)  ---------------------
L_rx: 328.581, L_rs: 0.002, L_pred: 1.353, L_clf: 0.836, 
L_clf_acc: 0.004

Ep: [150/300] (TRAIN) ---------------------
Loss: 823.030, L_rx: 329.123, L_kl: 0.185, L_c: 0.008, 
L_clf: 0.782, L_pred: 0.032
Ep: [150/300] (VALID)  ---------------------
L_rx: 328.582, L_rs: 0.002, L_pred: 1.356, L_clf: 0.835, 
L_clf_acc: 0.004

Ep: [160/300] (TRAIN) ---------------------
Loss: 823.027, L_rx: 329.122, L_kl: 0.181, L_c: 0.007, 
L_clf: 0.782, L_pred: 0.035
Ep: [160/300] (VALID)  ---------------------
L_rx: 328.581, L_rs: 0.003, L_pred: 1.487, L_clf: 0.829, 
L_clf_acc: 0.004

Ep: [170/300] (TRAIN) ---------------------
Loss: 823.012, L_rx: 329.122, L_kl: 0.174, L_c: 0.008, 
L_clf: 0.784, L_pred: 0.026
Ep: [170/300] (VALID)  ---------------------
L_rx: 328.582, L_rs: 0.002, L_pred: 1.250, L_clf: 0.834, 
L_clf_acc: 0.003

Ep: [180/300] (TRAIN) ---------------------
Loss: 823.078, L_rx: 329.125, L_kl: 0.171, L_c: 0.008, 
L_clf: 0.781, L_pred: 0.089
Ep: [180/300] (VALID)  ---------------------
L_rx: 328.583, L_rs: 0.004, L_pred: 1.680, L_clf: 0.827, 
L_clf_acc: 0.004

Ep: [190/300] (TRAIN) ---------------------
Loss: 823.007, L_rx: 329.121, L_kl: 0.170, L_c: 0.008, 
L_clf: 0.784, L_pred: 0.027
Ep: [190/300] (VALID)  ---------------------
L_rx: 328.581, L_rs: 0.001, L_pred: 1.233, L_clf: 0.824, 
L_clf_acc: 0.004

Ep: [200/300] (TRAIN) ---------------------
Loss: 823.005, L_rx: 329.122, L_kl: 0.170, L_c: 0.007, 
L_clf: 0.782, L_pred: 0.026
Ep: [200/300] (VALID)  ---------------------
L_rx: 328.582, L_rs: 0.002, L_pred: 1.456, L_clf: 0.822, 
L_clf_acc: 0.004

Ep: [210/300] (TRAIN) ---------------------
Loss: 823.004, L_rx: 329.122, L_kl: 0.166, L_c: 0.007, 
L_clf: 0.778, L_pred: 0.027
Ep: [210/300] (VALID)  ---------------------
L_rx: 328.581, L_rs: 0.001, L_pred: 1.329, L_clf: 0.833, 
L_clf_acc: 0.005

Ep: [220/300] (TRAIN) ---------------------
Loss: 823.000, L_rx: 329.122, L_kl: 0.164, L_c: 0.007, 
L_clf: 0.778, L_pred: 0.026
Ep: [220/300] (VALID)  ---------------------
L_rx: 328.581, L_rs: 0.002, L_pred: 1.368, L_clf: 0.823, 
L_clf_acc: 0.003

Ep: [230/300] (TRAIN) ---------------------
Loss: 822.997, L_rx: 329.121, L_kl: 0.162, L_c: 0.007, 
L_clf: 0.781, L_pred: 0.025
Ep: [230/300] (VALID)  ---------------------
L_rx: 328.581, L_rs: 0.001, L_pred: 1.392, L_clf: 0.831, 
L_clf_acc: 0.004

Ep: [240/300] (TRAIN) ---------------------
Loss: 822.995, L_rx: 329.121, L_kl: 0.160, L_c: 0.007, 
L_clf: 0.779, L_pred: 0.025
Ep: [240/300] (VALID)  ---------------------
L_rx: 328.581, L_rs: 0.001, L_pred: 1.378, L_clf: 0.823, 
L_clf_acc: 0.004

Ep: [250/300] (TRAIN) ---------------------
Loss: 822.992, L_rx: 329.121, L_kl: 0.159, L_c: 0.007, 
L_clf: 0.779, L_pred: 0.025
Ep: [250/300] (VALID)  ---------------------
L_rx: 328.582, L_rs: 0.001, L_pred: 1.372, L_clf: 0.819, 
L_clf_acc: 0.005

Ep: [260/300] (TRAIN) ---------------------
Loss: 822.991, L_rx: 329.121, L_kl: 0.157, L_c: 0.007, 
L_clf: 0.776, L_pred: 0.025
Ep: [260/300] (VALID)  ---------------------
L_rx: 328.581, L_rs: 0.001, L_pred: 1.375, L_clf: 0.829, 
L_clf_acc: 0.004

Ep: [270/300] (TRAIN) ---------------------
Loss: 822.990, L_rx: 329.121, L_kl: 0.156, L_c: 0.007, 
L_clf: 0.778, L_pred: 0.025
Ep: [270/300] (VALID)  ---------------------
L_rx: 328.581, L_rs: 0.001, L_pred: 1.409, L_clf: 0.832, 
L_clf_acc: 0.004

Ep: [280/300] (TRAIN) ---------------------
Loss: 822.989, L_rx: 329.121, L_kl: 0.156, L_c: 0.007, 
L_clf: 0.782, L_pred: 0.025
Ep: [280/300] (VALID)  ---------------------
L_rx: 328.581, L_rs: 0.002, L_pred: 1.363, L_clf: 0.819, 
L_clf_acc: 0.004

Ep: [290/300] (TRAIN) ---------------------
Loss: 822.988, L_rx: 329.121, L_kl: 0.155, L_c: 0.007, 
L_clf: 0.777, L_pred: 0.025
Ep: [290/300] (VALID)  ---------------------
L_rx: 328.581, L_rs: 0.002, L_pred: 1.383, L_clf: 0.839, 
L_clf_acc: 0.004

Ep: [300/300] (TRAIN) ---------------------
Loss: 822.988, L_rx: 329.121, L_kl: 0.155, L_c: 0.007, 
L_clf: 0.776, L_pred: 0.025
Ep: [300/300] (VALID)  ---------------------
L_rx: 328.580, L_rs: 0.001, L_pred: 1.375, L_clf: 0.816, 
L_clf_acc: 0.004
input_dim = 10, hidden_dim = 8, output_dim = 1
==============================================
1stage training start.
 dataset size: (17432, 10)
==============================================
len train loader :1
train tot iters :300
input_dim = 10, hidden_dim = 8, output_dim = 1

Ep: [10/300] (TRAIN) ---------------------
Loss: 345.566, L_rx: 338.719, L_kl: 0.780, L_c: 0.014, 
L_clf: 1.627, L_pred: 2.657
Ep: [10/300] (VALID)  ---------------------
L_rx: 334.409, L_rs: 0.855, L_pred: 1.950, L_clf: 1.403, 
L_clf_acc: 0.002

Ep: [20/300] (TRAIN) ---------------------
Loss: 833.871, L_rx: 332.942, L_kl: 1.119, L_c: 2.137, 
L_clf: 1.041, L_pred: 2.230
Ep: [20/300] (VALID)  ---------------------
L_rx: 328.512, L_rs: 0.918, L_pred: 2.452, L_clf: 0.981, 
L_clf_acc: 0.002

Ep: [30/300] (TRAIN) ---------------------
Loss: 830.476, L_rx: 330.710, L_kl: 1.410, L_c: 2.034, 
L_clf: 1.003, L_pred: 0.802
Ep: [30/300] (VALID)  ---------------------
L_rx: 326.943, L_rs: 0.696, L_pred: 0.876, L_clf: 0.941, 
L_clf_acc: 0.003

Ep: [40/300] (TRAIN) ---------------------
Loss: 828.155, L_rx: 330.600, L_kl: 0.771, L_c: 0.664, 
L_clf: 0.912, L_pred: 0.734
Ep: [40/300] (VALID)  ---------------------
L_rx: 326.882, L_rs: 0.691, L_pred: 0.843, L_clf: 0.838, 
L_clf_acc: 0.003

Ep: [50/300] (TRAIN) ---------------------
Loss: 826.981, L_rx: 330.636, L_kl: 0.290, L_c: 0.026, 
L_clf: 0.886, L_pred: 0.592
Ep: [50/300] (VALID)  ---------------------
L_rx: 326.945, L_rs: 0.695, L_pred: 0.837, L_clf: 0.826, 
L_clf_acc: 0.001

Ep: [60/300] (TRAIN) ---------------------
Loss: 826.439, L_rx: 330.597, L_kl: 0.192, L_c: 0.024, 
L_clf: 0.875, L_pred: 0.240
Ep: [60/300] (VALID)  ---------------------
L_rx: 326.890, L_rs: 0.687, L_pred: 0.913, L_clf: 0.813, 
L_clf_acc: 0.002

Ep: [70/300] (TRAIN) ---------------------
Loss: 826.158, L_rx: 330.558, L_kl: 0.188, L_c: 0.017, 
L_clf: 0.865, L_pred: 0.047
Ep: [70/300] (VALID)  ---------------------
L_rx: 326.883, L_rs: 0.655, L_pred: 1.232, L_clf: 0.800, 
L_clf_acc: 0.002

Ep: [80/300] (TRAIN) ---------------------
Loss: 825.831, L_rx: 330.354, L_kl: 0.196, L_c: 0.012, 
L_clf: 0.858, L_pred: 0.074
Ep: [80/300] (VALID)  ---------------------
L_rx: 326.886, L_rs: 0.398, L_pred: 1.535, L_clf: 0.794, 
L_clf_acc: 0.001

Ep: [90/300] (TRAIN) ---------------------
Loss: 825.494, L_rx: 330.165, L_kl: 0.245, L_c: 0.012, 
L_clf: 0.851, L_pred: 0.026
Ep: [90/300] (VALID)  ---------------------
L_rx: 326.885, L_rs: 0.068, L_pred: 1.249, L_clf: 0.800, 
L_clf_acc: 0.002

Ep: [100/300] (TRAIN) ---------------------
Loss: 825.055, L_rx: 329.928, L_kl: 0.219, L_c: 0.008, 
L_clf: 0.849, L_pred: 0.026
Ep: [100/300] (VALID)  ---------------------
L_rx: 326.873, L_rs: 0.080, L_pred: 1.186, L_clf: 0.793, 
L_clf_acc: 0.001

Ep: [110/300] (TRAIN) ---------------------
Loss: 825.015, L_rx: 329.912, L_kl: 0.213, L_c: 0.007, 
L_clf: 0.850, L_pred: 0.024
Ep: [110/300] (VALID)  ---------------------
L_rx: 326.883, L_rs: 0.014, L_pred: 1.210, L_clf: 0.779, 
L_clf_acc: 0.001

Ep: [120/300] (TRAIN) ---------------------
Loss: 824.987, L_rx: 329.908, L_kl: 0.193, L_c: 0.007, 
L_clf: 0.845, L_pred: 0.023
Ep: [120/300] (VALID)  ---------------------
L_rx: 326.874, L_rs: 0.022, L_pred: 1.163, L_clf: 0.791, 
L_clf_acc: 0.003

Ep: [130/300] (TRAIN) ---------------------
Loss: 824.964, L_rx: 329.902, L_kl: 0.183, L_c: 0.007, 
L_clf: 0.839, L_pred: 0.024
Ep: [130/300] (VALID)  ---------------------
L_rx: 326.875, L_rs: 0.011, L_pred: 1.108, L_clf: 0.781, 
L_clf_acc: 0.002

Ep: [140/300] (TRAIN) ---------------------
Loss: 824.962, L_rx: 329.902, L_kl: 0.180, L_c: 0.006, 
L_clf: 0.841, L_pred: 0.026
Ep: [140/300] (VALID)  ---------------------
L_rx: 326.875, L_rs: 0.009, L_pred: 1.178, L_clf: 0.780, 
L_clf_acc: 0.002

Ep: [150/300] (TRAIN) ---------------------
Loss: 824.951, L_rx: 329.900, L_kl: 0.172, L_c: 0.006, 
L_clf: 0.838, L_pred: 0.027
Ep: [150/300] (VALID)  ---------------------
L_rx: 326.875, L_rs: 0.005, L_pred: 1.125, L_clf: 0.771, 
L_clf_acc: 0.002

Ep: [160/300] (TRAIN) ---------------------
Loss: 824.939, L_rx: 329.898, L_kl: 0.166, L_c: 0.006, 
L_clf: 0.832, L_pred: 0.025
Ep: [160/300] (VALID)  ---------------------
L_rx: 326.875, L_rs: 0.004, L_pred: 1.156, L_clf: 0.769, 
L_clf_acc: 0.002

Ep: [170/300] (TRAIN) ---------------------
Loss: 824.930, L_rx: 329.897, L_kl: 0.160, L_c: 0.006, 
L_clf: 0.831, L_pred: 0.025
Ep: [170/300] (VALID)  ---------------------
L_rx: 326.874, L_rs: 0.004, L_pred: 1.182, L_clf: 0.763, 
L_clf_acc: 0.002

Ep: [180/300] (TRAIN) ---------------------
Loss: 824.923, L_rx: 329.895, L_kl: 0.157, L_c: 0.006, 
L_clf: 0.818, L_pred: 0.025
Ep: [180/300] (VALID)  ---------------------
L_rx: 326.872, L_rs: 0.004, L_pred: 1.178, L_clf: 0.772, 
L_clf_acc: 0.002

Ep: [190/300] (TRAIN) ---------------------
Loss: 824.914, L_rx: 329.892, L_kl: 0.154, L_c: 0.006, 
L_clf: 0.814, L_pred: 0.025
Ep: [190/300] (VALID)  ---------------------
L_rx: 326.865, L_rs: 0.003, L_pred: 1.151, L_clf: 0.757, 
L_clf_acc: 0.002

Ep: [200/300] (TRAIN) ---------------------
Loss: 824.900, L_rx: 329.887, L_kl: 0.154, L_c: 0.006, 
L_clf: 0.808, L_pred: 0.025
Ep: [200/300] (VALID)  ---------------------
L_rx: 326.859, L_rs: 0.003, L_pred: 1.158, L_clf: 0.756, 
L_clf_acc: 0.002

Ep: [210/300] (TRAIN) ---------------------
Loss: 824.890, L_rx: 329.883, L_kl: 0.154, L_c: 0.006, 
L_clf: 0.806, L_pred: 0.024
Ep: [210/300] (VALID)  ---------------------
L_rx: 326.854, L_rs: 0.004, L_pred: 1.219, L_clf: 0.738, 
L_clf_acc: 0.002

Ep: [220/300] (TRAIN) ---------------------
Loss: 824.885, L_rx: 329.883, L_kl: 0.151, L_c: 0.006, 
L_clf: 0.801, L_pred: 0.024
Ep: [220/300] (VALID)  ---------------------
L_rx: 326.853, L_rs: 0.003, L_pred: 1.235, L_clf: 0.746, 
L_clf_acc: 0.002

Ep: [230/300] (TRAIN) ---------------------
Loss: 824.883, L_rx: 329.882, L_kl: 0.150, L_c: 0.006, 
L_clf: 0.799, L_pred: 0.023
Ep: [230/300] (VALID)  ---------------------
L_rx: 326.853, L_rs: 0.004, L_pred: 1.224, L_clf: 0.746, 
L_clf_acc: 0.002

Ep: [240/300] (TRAIN) ---------------------
Loss: 824.881, L_rx: 329.882, L_kl: 0.149, L_c: 0.006, 
L_clf: 0.804, L_pred: 0.023
Ep: [240/300] (VALID)  ---------------------
L_rx: 326.852, L_rs: 0.003, L_pred: 1.209, L_clf: 0.750, 
L_clf_acc: 0.002

Ep: [250/300] (TRAIN) ---------------------
Loss: 824.881, L_rx: 329.883, L_kl: 0.148, L_c: 0.006, 
L_clf: 0.803, L_pred: 0.023
Ep: [250/300] (VALID)  ---------------------
L_rx: 326.853, L_rs: 0.004, L_pred: 1.212, L_clf: 0.747, 
L_clf_acc: 0.002

Ep: [260/300] (TRAIN) ---------------------
Loss: 824.879, L_rx: 329.882, L_kl: 0.147, L_c: 0.006, 
L_clf: 0.800, L_pred: 0.022
Ep: [260/300] (VALID)  ---------------------
L_rx: 326.853, L_rs: 0.003, L_pred: 1.233, L_clf: 0.745, 
L_clf_acc: 0.002

Ep: [270/300] (TRAIN) ---------------------
Loss: 824.877, L_rx: 329.882, L_kl: 0.146, L_c: 0.006, 
L_clf: 0.800, L_pred: 0.022
Ep: [270/300] (VALID)  ---------------------
L_rx: 326.852, L_rs: 0.003, L_pred: 1.227, L_clf: 0.731, 
L_clf_acc: 0.002

Ep: [280/300] (TRAIN) ---------------------
Loss: 824.876, L_rx: 329.882, L_kl: 0.146, L_c: 0.006, 
L_clf: 0.797, L_pred: 0.022
Ep: [280/300] (VALID)  ---------------------
L_rx: 326.852, L_rs: 0.003, L_pred: 1.225, L_clf: 0.741, 
L_clf_acc: 0.002

Ep: [290/300] (TRAIN) ---------------------
Loss: 824.876, L_rx: 329.882, L_kl: 0.145, L_c: 0.006, 
L_clf: 0.801, L_pred: 0.023
Ep: [290/300] (VALID)  ---------------------
L_rx: 326.852, L_rs: 0.003, L_pred: 1.216, L_clf: 0.754, 
L_clf_acc: 0.002

Ep: [300/300] (TRAIN) ---------------------
Loss: 824.876, L_rx: 329.882, L_kl: 0.145, L_c: 0.006, 
L_clf: 0.800, L_pred: 0.023
Ep: [300/300] (VALID)  ---------------------
L_rx: 326.852, L_rs: 0.004, L_pred: 1.221, L_clf: 0.740, 
L_clf_acc: 0.002
input_dim = 10, hidden_dim = 8, output_dim = 1
==============================================
1stage training start.
 dataset size: (17432, 10)
==============================================
len train loader :1
train tot iters :300
input_dim = 10, hidden_dim = 8, output_dim = 1

Ep: [10/300] (TRAIN) ---------------------
Loss: 350.133, L_rx: 336.618, L_kl: 0.402, L_c: 0.007, 
L_clf: 0.995, L_pred: 8.605
Ep: [10/300] (VALID)  ---------------------
L_rx: 336.944, L_rs: 0.745, L_pred: 6.326, L_clf: 0.965, 
L_clf_acc: 0.006

Ep: [20/300] (TRAIN) ---------------------
Loss: 990.140, L_rx: 330.956, L_kl: 0.340, L_c: 0.879, 
L_clf: 0.879, L_pred: 1.174
Ep: [20/300] (VALID)  ---------------------
L_rx: 331.361, L_rs: 0.688, L_pred: 1.039, L_clf: 0.894, 
L_clf_acc: 0.005

Ep: [30/300] (TRAIN) ---------------------
Loss: 989.779, L_rx: 329.554, L_kl: 0.945, L_c: 0.024, 
L_clf: 0.864, L_pred: 0.849
Ep: [30/300] (VALID)  ---------------------
L_rx: 330.377, L_rs: 0.684, L_pred: 0.859, L_clf: 0.871, 
L_clf_acc: 0.003

Ep: [40/300] (TRAIN) ---------------------
Loss: 988.781, L_rx: 329.507, L_kl: 0.237, L_c: 0.014, 
L_clf: 0.856, L_pred: 0.696
Ep: [40/300] (VALID)  ---------------------
L_rx: 330.370, L_rs: 0.687, L_pred: 0.846, L_clf: 0.862, 
L_clf_acc: 0.002

Ep: [50/300] (TRAIN) ---------------------
Loss: 988.182, L_rx: 329.446, L_kl: 0.168, L_c: 0.014, 
L_clf: 0.850, L_pred: 0.318
Ep: [50/300] (VALID)  ---------------------
L_rx: 330.318, L_rs: 0.643, L_pred: 0.930, L_clf: 0.851, 
L_clf_acc: 0.003

Ep: [60/300] (TRAIN) ---------------------
Loss: 987.680, L_rx: 329.319, L_kl: 0.118, L_c: 0.008, 
L_clf: 0.847, L_pred: 0.128
Ep: [60/300] (VALID)  ---------------------
L_rx: 330.325, L_rs: 0.491, L_pred: 1.668, L_clf: 0.857, 
L_clf_acc: 0.004

Ep: [70/300] (TRAIN) ---------------------
Loss: 987.073, L_rx: 329.053, L_kl: 0.113, L_c: 0.020, 
L_clf: 0.837, L_pred: 0.046
Ep: [70/300] (VALID)  ---------------------
L_rx: 330.319, L_rs: 0.293, L_pred: 1.486, L_clf: 0.838, 
L_clf_acc: 0.003

Ep: [80/300] (TRAIN) ---------------------
Loss: 986.688, L_rx: 328.867, L_kl: 0.121, L_c: 0.013, 
L_clf: 0.835, L_pred: 0.036
Ep: [80/300] (VALID)  ---------------------
L_rx: 330.315, L_rs: 0.124, L_pred: 1.208, L_clf: 0.842, 
L_clf_acc: 0.003

Ep: [90/300] (TRAIN) ---------------------
Loss: 986.585, L_rx: 328.821, L_kl: 0.119, L_c: 0.013, 
L_clf: 0.829, L_pred: 0.028
Ep: [90/300] (VALID)  ---------------------
L_rx: 330.313, L_rs: 0.044, L_pred: 1.495, L_clf: 0.840, 
L_clf_acc: 0.004

Ep: [100/300] (TRAIN) ---------------------
Loss: 986.577, L_rx: 328.797, L_kl: 0.104, L_c: 0.010, 
L_clf: 0.835, L_pred: 0.084
Ep: [100/300] (VALID)  ---------------------
L_rx: 330.315, L_rs: 0.013, L_pred: 1.068, L_clf: 0.835, 
L_clf_acc: 0.004

Ep: [110/300] (TRAIN) ---------------------
Loss: 986.514, L_rx: 328.792, L_kl: 0.099, L_c: 0.010, 
L_clf: 0.799, L_pred: 0.037
Ep: [110/300] (VALID)  ---------------------
L_rx: 330.314, L_rs: 0.008, L_pred: 1.415, L_clf: 0.803, 
L_clf_acc: 0.003

Ep: [120/300] (TRAIN) ---------------------
Loss: 986.510, L_rx: 328.789, L_kl: 0.091, L_c: 0.006, 
L_clf: 0.793, L_pred: 0.052
Ep: [120/300] (VALID)  ---------------------
L_rx: 330.312, L_rs: 0.006, L_pred: 1.125, L_clf: 0.799, 
L_clf_acc: 0.001

Ep: [130/300] (TRAIN) ---------------------
Loss: 986.496, L_rx: 328.787, L_kl: 0.088, L_c: 0.006, 
L_clf: 0.789, L_pred: 0.048
Ep: [130/300] (VALID)  ---------------------
L_rx: 330.307, L_rs: 0.007, L_pred: 1.523, L_clf: 0.784, 
L_clf_acc: 0.003

Ep: [140/300] (TRAIN) ---------------------
Loss: 986.471, L_rx: 328.784, L_kl: 0.085, L_c: 0.004, 
L_clf: 0.783, L_pred: 0.035
Ep: [140/300] (VALID)  ---------------------
L_rx: 330.304, L_rs: 0.005, L_pred: 1.185, L_clf: 0.784, 
L_clf_acc: 0.003

Ep: [150/300] (TRAIN) ---------------------
Loss: 986.460, L_rx: 328.782, L_kl: 0.083, L_c: 0.004, 
L_clf: 0.780, L_pred: 0.032
Ep: [150/300] (VALID)  ---------------------
L_rx: 330.302, L_rs: 0.004, L_pred: 1.468, L_clf: 0.796, 
L_clf_acc: 0.003

Ep: [160/300] (TRAIN) ---------------------
Loss: 986.443, L_rx: 328.780, L_kl: 0.081, L_c: 0.002, 
L_clf: 0.779, L_pred: 0.024
Ep: [160/300] (VALID)  ---------------------
L_rx: 330.300, L_rs: 0.005, L_pred: 1.229, L_clf: 0.786, 
L_clf_acc: 0.002

Ep: [170/300] (TRAIN) ---------------------
Loss: 986.438, L_rx: 328.779, L_kl: 0.079, L_c: 0.003, 
L_clf: 0.777, L_pred: 0.024
Ep: [170/300] (VALID)  ---------------------
L_rx: 330.300, L_rs: 0.003, L_pred: 1.459, L_clf: 0.793, 
L_clf_acc: 0.003

Ep: [180/300] (TRAIN) ---------------------
Loss: 986.425, L_rx: 328.777, L_kl: 0.076, L_c: 0.002, 
L_clf: 0.777, L_pred: 0.018
Ep: [180/300] (VALID)  ---------------------
L_rx: 330.299, L_rs: 0.003, L_pred: 1.245, L_clf: 0.785, 
L_clf_acc: 0.003

Ep: [190/300] (TRAIN) ---------------------
Loss: 986.421, L_rx: 328.777, L_kl: 0.074, L_c: 0.002, 
L_clf: 0.770, L_pred: 0.017
Ep: [190/300] (VALID)  ---------------------
L_rx: 330.298, L_rs: 0.003, L_pred: 1.399, L_clf: 0.784, 
L_clf_acc: 0.002

Ep: [200/300] (TRAIN) ---------------------
Loss: 986.413, L_rx: 328.776, L_kl: 0.072, L_c: 0.002, 
L_clf: 0.770, L_pred: 0.014
Ep: [200/300] (VALID)  ---------------------
L_rx: 330.298, L_rs: 0.002, L_pred: 1.285, L_clf: 0.780, 
L_clf_acc: 0.002

Ep: [210/300] (TRAIN) ---------------------
Loss: 986.408, L_rx: 328.776, L_kl: 0.070, L_c: 0.002, 
L_clf: 0.773, L_pred: 0.012
Ep: [210/300] (VALID)  ---------------------
L_rx: 330.298, L_rs: 0.002, L_pred: 1.403, L_clf: 0.783, 
L_clf_acc: 0.003

Ep: [220/300] (TRAIN) ---------------------
Loss: 986.405, L_rx: 328.775, L_kl: 0.068, L_c: 0.001, 
L_clf: 0.770, L_pred: 0.012
Ep: [220/300] (VALID)  ---------------------
L_rx: 330.298, L_rs: 0.002, L_pred: 1.285, L_clf: 0.782, 
L_clf_acc: 0.002

Ep: [230/300] (TRAIN) ---------------------
Loss: 986.401, L_rx: 328.775, L_kl: 0.067, L_c: 0.001, 
L_clf: 0.769, L_pred: 0.010
Ep: [230/300] (VALID)  ---------------------
L_rx: 330.298, L_rs: 0.001, L_pred: 1.391, L_clf: 0.787, 
L_clf_acc: 0.003

Ep: [240/300] (TRAIN) ---------------------
Loss: 986.399, L_rx: 328.775, L_kl: 0.066, L_c: 0.001, 
L_clf: 0.765, L_pred: 0.010
Ep: [240/300] (VALID)  ---------------------
L_rx: 330.297, L_rs: 0.002, L_pred: 1.363, L_clf: 0.778, 
L_clf_acc: 0.003

Ep: [250/300] (TRAIN) ---------------------
Loss: 986.397, L_rx: 328.774, L_kl: 0.065, L_c: 0.001, 
L_clf: 0.766, L_pred: 0.009
Ep: [250/300] (VALID)  ---------------------
L_rx: 330.297, L_rs: 0.002, L_pred: 1.351, L_clf: 0.782, 
L_clf_acc: 0.002

Ep: [260/300] (TRAIN) ---------------------
Loss: 986.396, L_rx: 328.774, L_kl: 0.064, L_c: 0.001, 
L_clf: 0.767, L_pred: 0.010
Ep: [260/300] (VALID)  ---------------------
L_rx: 330.297, L_rs: 0.002, L_pred: 1.326, L_clf: 0.773, 
L_clf_acc: 0.002

Ep: [270/300] (TRAIN) ---------------------
Loss: 986.394, L_rx: 328.774, L_kl: 0.063, L_c: 0.001, 
L_clf: 0.765, L_pred: 0.009
Ep: [270/300] (VALID)  ---------------------
L_rx: 330.297, L_rs: 0.001, L_pred: 1.326, L_clf: 0.780, 
L_clf_acc: 0.002

Ep: [280/300] (TRAIN) ---------------------
Loss: 986.394, L_rx: 328.774, L_kl: 0.063, L_c: 0.001, 
L_clf: 0.767, L_pred: 0.009
Ep: [280/300] (VALID)  ---------------------
L_rx: 330.297, L_rs: 0.001, L_pred: 1.340, L_clf: 0.776, 
L_clf_acc: 0.003

Ep: [290/300] (TRAIN) ---------------------
Loss: 986.393, L_rx: 328.774, L_kl: 0.063, L_c: 0.001, 
L_clf: 0.767, L_pred: 0.009
Ep: [290/300] (VALID)  ---------------------
L_rx: 330.297, L_rs: 0.001, L_pred: 1.309, L_clf: 0.774, 
L_clf_acc: 0.003

Ep: [300/300] (TRAIN) ---------------------
Loss: 986.394, L_rx: 328.774, L_kl: 0.062, L_c: 0.001, 
L_clf: 0.768, L_pred: 0.009
Ep: [300/300] (VALID)  ---------------------
L_rx: 330.297, L_rs: 0.001, L_pred: 1.321, L_clf: 0.777, 
L_clf_acc: 0.003
input_dim = 10, hidden_dim = 8, output_dim = 1
==============================================
1stage training start.
 dataset size: (17432, 10)
==============================================
len train loader :1
train tot iters :300
input_dim = 10, hidden_dim = 8, output_dim = 1

Ep: [10/300] (TRAIN) ---------------------
Loss: 351.821, L_rx: 337.612, L_kl: 0.195, L_c: 0.007, 
L_clf: 1.082, L_pred: 9.492
Ep: [10/300] (VALID)  ---------------------
L_rx: 339.635, L_rs: 0.861, L_pred: 7.650, L_clf: 1.081, 
L_clf_acc: 0.002

Ep: [20/300] (TRAIN) ---------------------
Loss: 1004.145, L_rx: 335.487, L_kl: 0.107, L_c: 1.006, 
L_clf: 0.958, L_pred: 1.834
Ep: [20/300] (VALID)  ---------------------
L_rx: 337.229, L_rs: 0.812, L_pred: 1.561, L_clf: 0.934, 
L_clf_acc: 0.002

Ep: [30/300] (TRAIN) ---------------------
Loss: 992.002, L_rx: 330.060, L_kl: 0.192, L_c: 0.991, 
L_clf: 0.871, L_pred: 1.377
Ep: [30/300] (VALID)  ---------------------
L_rx: 332.169, L_rs: 0.715, L_pred: 1.191, L_clf: 0.885, 
L_clf_acc: 0.001

Ep: [40/300] (TRAIN) ---------------------
Loss: 989.834, L_rx: 329.734, L_kl: 0.516, L_c: 0.046, 
L_clf: 0.866, L_pred: 0.775
Ep: [40/300] (VALID)  ---------------------
L_rx: 332.004, L_rs: 0.689, L_pred: 1.014, L_clf: 0.877, 
L_clf_acc: 0.003

Ep: [50/300] (TRAIN) ---------------------
Loss: 989.252, L_rx: 329.736, L_kl: 0.361, L_c: 0.039, 
L_clf: 0.864, L_pred: 0.357
Ep: [50/300] (VALID)  ---------------------
L_rx: 332.004, L_rs: 0.688, L_pred: 0.971, L_clf: 0.875, 
L_clf_acc: 0.001

Ep: [60/300] (TRAIN) ---------------------
Loss: 988.720, L_rx: 329.739, L_kl: 0.113, L_c: 0.017, 
L_clf: 0.858, L_pred: 0.063
Ep: [60/300] (VALID)  ---------------------
L_rx: 332.010, L_rs: 0.685, L_pred: 1.885, L_clf: 0.866, 
L_clf_acc: 0.002

Ep: [70/300] (TRAIN) ---------------------
Loss: 988.690, L_rx: 329.698, L_kl: 0.118, L_c: 0.009, 
L_clf: 0.858, L_pred: 0.130
Ep: [70/300] (VALID)  ---------------------
L_rx: 332.005, L_rs: 0.676, L_pred: 2.408, L_clf: 0.869, 
L_clf_acc: 0.001

Ep: [80/300] (TRAIN) ---------------------
Loss: 988.700, L_rx: 329.652, L_kl: 0.112, L_c: 0.011, 
L_clf: 0.851, L_pred: 0.238
Ep: [80/300] (VALID)  ---------------------
L_rx: 332.006, L_rs: 0.615, L_pred: 1.208, L_clf: 0.855, 
L_clf_acc: 0.001

Ep: [90/300] (TRAIN) ---------------------
Loss: 988.170, L_rx: 329.493, L_kl: 0.098, L_c: 0.008, 
L_clf: 0.842, L_pred: 0.040
Ep: [90/300] (VALID)  ---------------------
L_rx: 332.005, L_rs: 0.388, L_pred: 1.234, L_clf: 0.849, 
L_clf_acc: 0.001

Ep: [100/300] (TRAIN) ---------------------
Loss: 987.671, L_rx: 329.253, L_kl: 0.099, L_c: 0.007, 
L_clf: 0.836, L_pred: 0.025
Ep: [100/300] (VALID)  ---------------------
L_rx: 332.000, L_rs: 0.192, L_pred: 1.346, L_clf: 0.845, 
L_clf_acc: 0.001

Ep: [110/300] (TRAIN) ---------------------
Loss: 987.600, L_rx: 329.186, L_kl: 0.116, L_c: 0.009, 
L_clf: 0.824, L_pred: 0.071
Ep: [110/300] (VALID)  ---------------------
L_rx: 332.003, L_rs: 0.135, L_pred: 1.819, L_clf: 0.835, 
L_clf_acc: 0.001

Ep: [120/300] (TRAIN) ---------------------
Loss: 987.280, L_rx: 329.049, L_kl: 0.113, L_c: 0.004, 
L_clf: 0.803, L_pred: 0.033
Ep: [120/300] (VALID)  ---------------------
L_rx: 332.003, L_rs: 0.018, L_pred: 1.350, L_clf: 0.813, 
L_clf_acc: 0.001

Ep: [130/300] (TRAIN) ---------------------
Loss: 987.246, L_rx: 329.044, L_kl: 0.098, L_c: 0.004, 
L_clf: 0.802, L_pred: 0.024
Ep: [130/300] (VALID)  ---------------------
L_rx: 332.003, L_rs: 0.009, L_pred: 1.109, L_clf: 0.819, 
L_clf_acc: 0.002

Ep: [140/300] (TRAIN) ---------------------
Loss: 987.217, L_rx: 329.036, L_kl: 0.094, L_c: 0.002, 
L_clf: 0.797, L_pred: 0.019
Ep: [140/300] (VALID)  ---------------------
L_rx: 332.005, L_rs: 0.008, L_pred: 1.474, L_clf: 0.813, 
L_clf_acc: 0.001

Ep: [150/300] (TRAIN) ---------------------
Loss: 987.219, L_rx: 329.034, L_kl: 0.086, L_c: 0.002, 
L_clf: 0.793, L_pred: 0.034
Ep: [150/300] (VALID)  ---------------------
L_rx: 332.003, L_rs: 0.005, L_pred: 1.187, L_clf: 0.801, 
L_clf_acc: 0.002

Ep: [160/300] (TRAIN) ---------------------
Loss: 987.197, L_rx: 329.032, L_kl: 0.082, L_c: 0.002, 
L_clf: 0.791, L_pred: 0.021
Ep: [160/300] (VALID)  ---------------------
L_rx: 332.004, L_rs: 0.006, L_pred: 1.407, L_clf: 0.812, 
L_clf_acc: 0.002

Ep: [170/300] (TRAIN) ---------------------
Loss: 987.192, L_rx: 329.032, L_kl: 0.078, L_c: 0.002, 
L_clf: 0.790, L_pred: 0.022
Ep: [170/300] (VALID)  ---------------------
L_rx: 332.003, L_rs: 0.004, L_pred: 1.197, L_clf: 0.810, 
L_clf_acc: 0.001

Ep: [180/300] (TRAIN) ---------------------
Loss: 987.184, L_rx: 329.031, L_kl: 0.077, L_c: 0.002, 
L_clf: 0.791, L_pred: 0.019
Ep: [180/300] (VALID)  ---------------------
L_rx: 332.004, L_rs: 0.005, L_pred: 1.361, L_clf: 0.799, 
L_clf_acc: 0.002

Ep: [190/300] (TRAIN) ---------------------
Loss: 987.179, L_rx: 329.029, L_kl: 0.073, L_c: 0.001, 
L_clf: 0.789, L_pred: 0.020
Ep: [190/300] (VALID)  ---------------------
L_rx: 332.002, L_rs: 0.004, L_pred: 1.233, L_clf: 0.791, 
L_clf_acc: 0.001

Ep: [200/300] (TRAIN) ---------------------
Loss: 987.172, L_rx: 329.028, L_kl: 0.073, L_c: 0.001, 
L_clf: 0.785, L_pred: 0.017
Ep: [200/300] (VALID)  ---------------------
L_rx: 332.001, L_rs: 0.003, L_pred: 1.350, L_clf: 0.791, 
L_clf_acc: 0.001

Ep: [210/300] (TRAIN) ---------------------
Loss: 987.167, L_rx: 329.027, L_kl: 0.071, L_c: 0.001, 
L_clf: 0.783, L_pred: 0.016
Ep: [210/300] (VALID)  ---------------------
L_rx: 331.998, L_rs: 0.003, L_pred: 1.236, L_clf: 0.801, 
L_clf_acc: 0.001

Ep: [220/300] (TRAIN) ---------------------
Loss: 987.161, L_rx: 329.026, L_kl: 0.070, L_c: 0.001, 
L_clf: 0.786, L_pred: 0.015
Ep: [220/300] (VALID)  ---------------------
L_rx: 331.998, L_rs: 0.003, L_pred: 1.306, L_clf: 0.796, 
L_clf_acc: 0.001

Ep: [230/300] (TRAIN) ---------------------
Loss: 987.156, L_rx: 329.024, L_kl: 0.070, L_c: 0.001, 
L_clf: 0.785, L_pred: 0.014
Ep: [230/300] (VALID)  ---------------------
L_rx: 331.996, L_rs: 0.002, L_pred: 1.314, L_clf: 0.789, 
L_clf_acc: 0.001

Ep: [240/300] (TRAIN) ---------------------
Loss: 987.151, L_rx: 329.023, L_kl: 0.070, L_c: 0.001, 
L_clf: 0.781, L_pred: 0.014
Ep: [240/300] (VALID)  ---------------------
L_rx: 331.994, L_rs: 0.004, L_pred: 1.299, L_clf: 0.792, 
L_clf_acc: 0.001

Ep: [250/300] (TRAIN) ---------------------
Loss: 987.148, L_rx: 329.022, L_kl: 0.069, L_c: 0.001, 
L_clf: 0.779, L_pred: 0.013
Ep: [250/300] (VALID)  ---------------------
L_rx: 331.993, L_rs: 0.002, L_pred: 1.291, L_clf: 0.799, 
L_clf_acc: 0.001

Ep: [260/300] (TRAIN) ---------------------
Loss: 987.145, L_rx: 329.021, L_kl: 0.069, L_c: 0.001, 
L_clf: 0.779, L_pred: 0.013
Ep: [260/300] (VALID)  ---------------------
L_rx: 331.991, L_rs: 0.002, L_pred: 1.296, L_clf: 0.793, 
L_clf_acc: 0.001

Ep: [270/300] (TRAIN) ---------------------
Loss: 987.143, L_rx: 329.021, L_kl: 0.069, L_c: 0.001, 
L_clf: 0.785, L_pred: 0.012
Ep: [270/300] (VALID)  ---------------------
L_rx: 331.989, L_rs: 0.002, L_pred: 1.282, L_clf: 0.793, 
L_clf_acc: 0.002

Ep: [280/300] (TRAIN) ---------------------
Loss: 987.140, L_rx: 329.020, L_kl: 0.070, L_c: 0.001, 
L_clf: 0.779, L_pred: 0.012
Ep: [280/300] (VALID)  ---------------------
L_rx: 331.989, L_rs: 0.002, L_pred: 1.304, L_clf: 0.788, 
L_clf_acc: 0.001

Ep: [290/300] (TRAIN) ---------------------
Loss: 987.140, L_rx: 329.019, L_kl: 0.070, L_c: 0.001, 
L_clf: 0.779, L_pred: 0.012
Ep: [290/300] (VALID)  ---------------------
L_rx: 331.987, L_rs: 0.002, L_pred: 1.261, L_clf: 0.779, 
L_clf_acc: 0.001

Ep: [300/300] (TRAIN) ---------------------
Loss: 987.139, L_rx: 329.019, L_kl: 0.070, L_c: 0.001, 
L_clf: 0.779, L_pred: 0.012
Ep: [300/300] (VALID)  ---------------------
L_rx: 331.988, L_rs: 0.002, L_pred: 1.281, L_clf: 0.784, 
L_clf_acc: 0.001
input_dim = 10, hidden_dim = 8, output_dim = 1
==============================================
1stage training start.
 dataset size: (17432, 10)
==============================================
len train loader :1
train tot iters :300
input_dim = 10, hidden_dim = 8, output_dim = 1

Ep: [10/300] (TRAIN) ---------------------
Loss: 341.425, L_rx: 335.270, L_kl: 0.356, L_c: 0.007, 
L_clf: 1.046, L_pred: 1.311
Ep: [10/300] (VALID)  ---------------------
L_rx: 334.657, L_rs: 1.006, L_pred: 1.506, L_clf: 1.020, 
L_clf_acc: 0.002

Ep: [20/300] (TRAIN) ---------------------
Loss: 990.305, L_rx: 331.168, L_kl: 0.482, L_c: 0.971, 
L_clf: 0.917, L_pred: 0.855
Ep: [20/300] (VALID)  ---------------------
L_rx: 330.785, L_rs: 1.213, L_pred: 0.960, L_clf: 0.902, 
L_clf_acc: 0.001

Ep: [30/300] (TRAIN) ---------------------
Loss: 990.251, L_rx: 329.851, L_kl: 0.691, L_c: 0.096, 
L_clf: 0.884, L_pred: 0.630
Ep: [30/300] (VALID)  ---------------------
L_rx: 330.024, L_rs: 0.724, L_pred: 0.974, L_clf: 0.877, 
L_clf_acc: 0.001

Ep: [40/300] (TRAIN) ---------------------
Loss: 989.434, L_rx: 329.781, L_kl: 0.376, L_c: 0.010, 
L_clf: 0.876, L_pred: 0.424
Ep: [40/300] (VALID)  ---------------------
L_rx: 329.981, L_rs: 0.751, L_pred: 1.091, L_clf: 0.871, 
L_clf_acc: 0.002

Ep: [50/300] (TRAIN) ---------------------
Loss: 988.914, L_rx: 329.761, L_kl: 0.203, L_c: 0.013, 
L_clf: 0.868, L_pred: 0.105
Ep: [50/300] (VALID)  ---------------------
L_rx: 329.983, L_rs: 0.687, L_pred: 1.732, L_clf: 0.866, 
L_clf_acc: 0.001

Ep: [60/300] (TRAIN) ---------------------
Loss: 988.775, L_rx: 329.745, L_kl: 0.158, L_c: 0.013, 
L_clf: 0.851, L_pred: 0.046
Ep: [60/300] (VALID)  ---------------------
L_rx: 329.983, L_rs: 0.682, L_pred: 1.670, L_clf: 0.840, 
L_clf_acc: 0.001

Ep: [70/300] (TRAIN) ---------------------
Loss: 988.694, L_rx: 329.723, L_kl: 0.135, L_c: 0.009, 
L_clf: 0.822, L_pred: 0.040
Ep: [70/300] (VALID)  ---------------------
L_rx: 329.978, L_rs: 0.656, L_pred: 1.297, L_clf: 0.835, 
L_clf_acc: 0.001

Ep: [80/300] (TRAIN) ---------------------
Loss: 988.597, L_rx: 329.679, L_kl: 0.114, L_c: 0.011, 
L_clf: 0.811, L_pred: 0.048
Ep: [80/300] (VALID)  ---------------------
L_rx: 329.979, L_rs: 0.605, L_pred: 1.728, L_clf: 0.837, 
L_clf_acc: 0.002

Ep: [90/300] (TRAIN) ---------------------
Loss: 988.326, L_rx: 329.546, L_kl: 0.119, L_c: 0.015, 
L_clf: 0.809, L_pred: 0.043
Ep: [90/300] (VALID)  ---------------------
L_rx: 329.978, L_rs: 0.429, L_pred: 1.276, L_clf: 0.825, 
L_clf_acc: 0.002

Ep: [100/300] (TRAIN) ---------------------
Loss: 987.630, L_rx: 329.185, L_kl: 0.140, L_c: 0.018, 
L_clf: 0.803, L_pred: 0.040
Ep: [100/300] (VALID)  ---------------------
L_rx: 329.977, L_rs: 0.049, L_pred: 1.072, L_clf: 0.825, 
L_clf_acc: 0.002

Ep: [110/300] (TRAIN) ---------------------
Loss: 987.520, L_rx: 329.079, L_kl: 0.129, L_c: 0.020, 
L_clf: 0.800, L_pred: 0.151
Ep: [110/300] (VALID)  ---------------------
L_rx: 329.977, L_rs: 0.009, L_pred: 1.134, L_clf: 0.808, 
L_clf_acc: 0.002

Ep: [120/300] (TRAIN) ---------------------
Loss: 987.375, L_rx: 329.065, L_kl: 0.112, L_c: 0.009, 
L_clf: 0.802, L_pred: 0.063
Ep: [120/300] (VALID)  ---------------------
L_rx: 329.976, L_rs: 0.003, L_pred: 1.678, L_clf: 0.826, 
L_clf_acc: 0.002

Ep: [130/300] (TRAIN) ---------------------
Loss: 987.350, L_rx: 329.063, L_kl: 0.107, L_c: 0.007, 
L_clf: 0.798, L_pred: 0.049
Ep: [130/300] (VALID)  ---------------------
L_rx: 329.975, L_rs: 0.001, L_pred: 1.250, L_clf: 0.816, 
L_clf_acc: 0.002

Ep: [140/300] (TRAIN) ---------------------
Loss: 987.337, L_rx: 329.063, L_kl: 0.100, L_c: 0.007, 
L_clf: 0.791, L_pred: 0.043
Ep: [140/300] (VALID)  ---------------------
L_rx: 329.975, L_rs: 0.001, L_pred: 1.584, L_clf: 0.807, 
L_clf_acc: 0.002

Ep: [150/300] (TRAIN) ---------------------
Loss: 987.323, L_rx: 329.063, L_kl: 0.094, L_c: 0.006, 
L_clf: 0.798, L_pred: 0.038
Ep: [150/300] (VALID)  ---------------------
L_rx: 329.974, L_rs: 0.001, L_pred: 1.244, L_clf: 0.819, 
L_clf_acc: 0.003

Ep: [160/300] (TRAIN) ---------------------
Loss: 987.312, L_rx: 329.062, L_kl: 0.089, L_c: 0.006, 
L_clf: 0.792, L_pred: 0.032
Ep: [160/300] (VALID)  ---------------------
L_rx: 329.974, L_rs: 0.001, L_pred: 1.570, L_clf: 0.814, 
L_clf_acc: 0.002

Ep: [170/300] (TRAIN) ---------------------
Loss: 987.301, L_rx: 329.062, L_kl: 0.084, L_c: 0.005, 
L_clf: 0.788, L_pred: 0.028
Ep: [170/300] (VALID)  ---------------------
L_rx: 329.973, L_rs: 0.001, L_pred: 1.282, L_clf: 0.815, 
L_clf_acc: 0.002

Ep: [180/300] (TRAIN) ---------------------
Loss: 987.298, L_rx: 329.062, L_kl: 0.081, L_c: 0.006, 
L_clf: 0.782, L_pred: 0.028
Ep: [180/300] (VALID)  ---------------------
L_rx: 329.973, L_rs: 0.001, L_pred: 1.535, L_clf: 0.807, 
L_clf_acc: 0.002

Ep: [190/300] (TRAIN) ---------------------
Loss: 987.286, L_rx: 329.061, L_kl: 0.077, L_c: 0.004, 
L_clf: 0.783, L_pred: 0.022
Ep: [190/300] (VALID)  ---------------------
L_rx: 329.973, L_rs: 0.001, L_pred: 1.262, L_clf: 0.801, 
L_clf_acc: 0.002

Ep: [200/300] (TRAIN) ---------------------
Loss: 987.284, L_rx: 329.061, L_kl: 0.075, L_c: 0.005, 
L_clf: 0.784, L_pred: 0.023
Ep: [200/300] (VALID)  ---------------------
L_rx: 329.973, L_rs: 0.001, L_pred: 1.450, L_clf: 0.803, 
L_clf_acc: 0.002

Ep: [210/300] (TRAIN) ---------------------
Loss: 987.276, L_rx: 329.061, L_kl: 0.072, L_c: 0.004, 
L_clf: 0.780, L_pred: 0.019
Ep: [210/300] (VALID)  ---------------------
L_rx: 329.973, L_rs: 0.001, L_pred: 1.348, L_clf: 0.801, 
L_clf_acc: 0.002

Ep: [220/300] (TRAIN) ---------------------
Loss: 987.273, L_rx: 329.061, L_kl: 0.070, L_c: 0.004, 
L_clf: 0.780, L_pred: 0.019
Ep: [220/300] (VALID)  ---------------------
L_rx: 329.973, L_rs: 0.001, L_pred: 1.436, L_clf: 0.802, 
L_clf_acc: 0.002

Ep: [230/300] (TRAIN) ---------------------
Loss: 987.269, L_rx: 329.061, L_kl: 0.068, L_c: 0.003, 
L_clf: 0.781, L_pred: 0.017
Ep: [230/300] (VALID)  ---------------------
L_rx: 329.973, L_rs: 0.001, L_pred: 1.300, L_clf: 0.806, 
L_clf_acc: 0.002

Ep: [240/300] (TRAIN) ---------------------
Loss: 987.264, L_rx: 329.060, L_kl: 0.066, L_c: 0.003, 
L_clf: 0.778, L_pred: 0.015
Ep: [240/300] (VALID)  ---------------------
L_rx: 329.972, L_rs: 0.001, L_pred: 1.422, L_clf: 0.803, 
L_clf_acc: 0.002

Ep: [250/300] (TRAIN) ---------------------
Loss: 987.261, L_rx: 329.060, L_kl: 0.065, L_c: 0.003, 
L_clf: 0.774, L_pred: 0.014
Ep: [250/300] (VALID)  ---------------------
L_rx: 329.972, L_rs: 0.001, L_pred: 1.323, L_clf: 0.795, 
L_clf_acc: 0.002

Ep: [260/300] (TRAIN) ---------------------
Loss: 987.259, L_rx: 329.060, L_kl: 0.064, L_c: 0.002, 
L_clf: 0.773, L_pred: 0.014
Ep: [260/300] (VALID)  ---------------------
L_rx: 329.972, L_rs: 0.001, L_pred: 1.350, L_clf: 0.799, 
L_clf_acc: 0.002

Ep: [270/300] (TRAIN) ---------------------
Loss: 987.257, L_rx: 329.060, L_kl: 0.063, L_c: 0.002, 
L_clf: 0.774, L_pred: 0.013
Ep: [270/300] (VALID)  ---------------------
L_rx: 329.972, L_rs: 0.001, L_pred: 1.355, L_clf: 0.787, 
L_clf_acc: 0.001

Ep: [280/300] (TRAIN) ---------------------
Loss: 987.256, L_rx: 329.060, L_kl: 0.063, L_c: 0.002, 
L_clf: 0.778, L_pred: 0.013
Ep: [280/300] (VALID)  ---------------------
L_rx: 329.972, L_rs: 0.001, L_pred: 1.350, L_clf: 0.788, 
L_clf_acc: 0.002

Ep: [290/300] (TRAIN) ---------------------
Loss: 987.256, L_rx: 329.060, L_kl: 0.063, L_c: 0.002, 
L_clf: 0.774, L_pred: 0.012
Ep: [290/300] (VALID)  ---------------------
L_rx: 329.972, L_rs: 0.001, L_pred: 1.311, L_clf: 0.788, 
L_clf_acc: 0.002

Ep: [300/300] (TRAIN) ---------------------
Loss: 987.256, L_rx: 329.060, L_kl: 0.062, L_c: 0.001, 
L_clf: 0.777, L_pred: 0.013
Ep: [300/300] (VALID)  ---------------------
L_rx: 329.972, L_rs: 0.000, L_pred: 1.331, L_clf: 0.792, 
L_clf_acc: 0.002
input_dim = 10, hidden_dim = 8, output_dim = 1
==============================================
1stage training start.
 dataset size: (17432, 10)
==============================================
len train loader :1
train tot iters :300
input_dim = 10, hidden_dim = 8, output_dim = 1

Ep: [10/300] (TRAIN) ---------------------
Loss: 346.226, L_rx: 339.575, L_kl: 0.155, L_c: 0.007, 
L_clf: 1.267, L_pred: 1.949
Ep: [10/300] (VALID)  ---------------------
L_rx: 337.896, L_rs: 0.801, L_pred: 1.796, L_clf: 1.240, 
L_clf_acc: 0.003

Ep: [20/300] (TRAIN) ---------------------
Loss: 1002.610, L_rx: 335.150, L_kl: 0.117, L_c: 1.007, 
L_clf: 0.938, L_pred: 1.274
Ep: [20/300] (VALID)  ---------------------
L_rx: 333.145, L_rs: 0.808, L_pred: 1.357, L_clf: 0.954, 
L_clf_acc: 0.006

Ep: [30/300] (TRAIN) ---------------------
Loss: 992.466, L_rx: 330.137, L_kl: 0.453, L_c: 0.762, 
L_clf: 0.873, L_pred: 1.619
Ep: [30/300] (VALID)  ---------------------
L_rx: 328.734, L_rs: 0.797, L_pred: 1.365, L_clf: 0.903, 
L_clf_acc: 0.005

Ep: [40/300] (TRAIN) ---------------------
Loss: 990.586, L_rx: 329.817, L_kl: 0.868, L_c: 0.013, 
L_clf: 0.859, L_pred: 0.946
Ep: [40/300] (VALID)  ---------------------
L_rx: 328.583, L_rs: 0.709, L_pred: 0.936, L_clf: 0.902, 
L_clf_acc: 0.005

Ep: [50/300] (TRAIN) ---------------------
Loss: 989.861, L_rx: 329.836, L_kl: 0.209, L_c: 0.015, 
L_clf: 0.849, L_pred: 0.808
Ep: [50/300] (VALID)  ---------------------
L_rx: 328.644, L_rs: 0.684, L_pred: 0.963, L_clf: 0.886, 
L_clf_acc: 0.006

Ep: [60/300] (TRAIN) ---------------------
Loss: 989.536, L_rx: 329.810, L_kl: 0.161, L_c: 0.010, 
L_clf: 0.846, L_pred: 0.621
Ep: [60/300] (VALID)  ---------------------
L_rx: 328.589, L_rs: 0.683, L_pred: 0.927, L_clf: 0.892, 
L_clf_acc: 0.004

Ep: [70/300] (TRAIN) ---------------------
Loss: 988.789, L_rx: 329.663, L_kl: 0.123, L_c: 0.007, 
L_clf: 0.843, L_pred: 0.205
Ep: [70/300] (VALID)  ---------------------
L_rx: 328.586, L_rs: 0.509, L_pred: 0.996, L_clf: 0.887, 
L_clf_acc: 0.004

Ep: [80/300] (TRAIN) ---------------------
Loss: 987.949, L_rx: 329.276, L_kl: 0.153, L_c: 0.017, 
L_clf: 0.835, L_pred: 0.106
Ep: [80/300] (VALID)  ---------------------
L_rx: 328.583, L_rs: 0.106, L_pred: 1.343, L_clf: 0.874, 
L_clf_acc: 0.005

Ep: [90/300] (TRAIN) ---------------------
Loss: 987.672, L_rx: 329.170, L_kl: 0.148, L_c: 0.018, 
L_clf: 0.812, L_pred: 0.039
Ep: [90/300] (VALID)  ---------------------
L_rx: 328.588, L_rs: 0.040, L_pred: 1.260, L_clf: 0.859, 
L_clf_acc: 0.005

Ep: [100/300] (TRAIN) ---------------------
Loss: 987.565, L_rx: 329.130, L_kl: 0.142, L_c: 0.008, 
L_clf: 0.798, L_pred: 0.035
Ep: [100/300] (VALID)  ---------------------
L_rx: 328.581, L_rs: 0.010, L_pred: 1.238, L_clf: 0.847, 
L_clf_acc: 0.005

Ep: [110/300] (TRAIN) ---------------------
Loss: 987.512, L_rx: 329.127, L_kl: 0.119, L_c: 0.004, 
L_clf: 0.795, L_pred: 0.015
Ep: [110/300] (VALID)  ---------------------
L_rx: 328.582, L_rs: 0.003, L_pred: 1.305, L_clf: 0.837, 
L_clf_acc: 0.004

Ep: [120/300] (TRAIN) ---------------------
Loss: 987.497, L_rx: 329.123, L_kl: 0.095, L_c: 0.005, 
L_clf: 0.786, L_pred: 0.031
Ep: [120/300] (VALID)  ---------------------
L_rx: 328.581, L_rs: 0.004, L_pred: 1.538, L_clf: 0.823, 
L_clf_acc: 0.003

Ep: [130/300] (TRAIN) ---------------------
Loss: 987.468, L_rx: 329.122, L_kl: 0.087, L_c: 0.003, 
L_clf: 0.788, L_pred: 0.016
Ep: [130/300] (VALID)  ---------------------
L_rx: 328.580, L_rs: 0.002, L_pred: 1.345, L_clf: 0.835, 
L_clf_acc: 0.004

Ep: [140/300] (TRAIN) ---------------------
Loss: 987.472, L_rx: 329.121, L_kl: 0.080, L_c: 0.002, 
L_clf: 0.785, L_pred: 0.027
Ep: [140/300] (VALID)  ---------------------
L_rx: 328.580, L_rs: 0.002, L_pred: 1.492, L_clf: 0.836, 
L_clf_acc: 0.004

Ep: [150/300] (TRAIN) ---------------------
Loss: 987.461, L_rx: 329.121, L_kl: 0.077, L_c: 0.002, 
L_clf: 0.782, L_pred: 0.022
Ep: [150/300] (VALID)  ---------------------
L_rx: 328.580, L_rs: 0.002, L_pred: 1.271, L_clf: 0.835, 
L_clf_acc: 0.004

Ep: [160/300] (TRAIN) ---------------------
Loss: 987.445, L_rx: 329.118, L_kl: 0.075, L_c: 0.002, 
L_clf: 0.782, L_pred: 0.016
Ep: [160/300] (VALID)  ---------------------
L_rx: 328.574, L_rs: 0.002, L_pred: 1.458, L_clf: 0.829, 
L_clf_acc: 0.004

Ep: [170/300] (TRAIN) ---------------------
Loss: 987.429, L_rx: 329.114, L_kl: 0.080, L_c: 0.001, 
L_clf: 0.784, L_pred: 0.010
Ep: [170/300] (VALID)  ---------------------
L_rx: 328.568, L_rs: 0.003, L_pred: 1.343, L_clf: 0.834, 
L_clf_acc: 0.003

Ep: [180/300] (TRAIN) ---------------------
Loss: 987.421, L_rx: 329.111, L_kl: 0.076, L_c: 0.002, 
L_clf: 0.781, L_pred: 0.011
Ep: [180/300] (VALID)  ---------------------
L_rx: 328.567, L_rs: 0.001, L_pred: 1.388, L_clf: 0.827, 
L_clf_acc: 0.004

Ep: [190/300] (TRAIN) ---------------------
Loss: 987.414, L_rx: 329.111, L_kl: 0.073, L_c: 0.001, 
L_clf: 0.784, L_pred: 0.010
Ep: [190/300] (VALID)  ---------------------
L_rx: 328.567, L_rs: 0.001, L_pred: 1.392, L_clf: 0.824, 
L_clf_acc: 0.004

Ep: [200/300] (TRAIN) ---------------------
Loss: 987.411, L_rx: 329.110, L_kl: 0.071, L_c: 0.001, 
L_clf: 0.782, L_pred: 0.010
Ep: [200/300] (VALID)  ---------------------
L_rx: 328.566, L_rs: 0.002, L_pred: 1.377, L_clf: 0.822, 
L_clf_acc: 0.004

Ep: [210/300] (TRAIN) ---------------------
Loss: 987.407, L_rx: 329.109, L_kl: 0.070, L_c: 0.001, 
L_clf: 0.778, L_pred: 0.009
Ep: [210/300] (VALID)  ---------------------
L_rx: 328.567, L_rs: 0.001, L_pred: 1.364, L_clf: 0.833, 
L_clf_acc: 0.005

Ep: [220/300] (TRAIN) ---------------------
Loss: 987.404, L_rx: 329.109, L_kl: 0.068, L_c: 0.001, 
L_clf: 0.778, L_pred: 0.009
Ep: [220/300] (VALID)  ---------------------
L_rx: 328.566, L_rs: 0.001, L_pred: 1.381, L_clf: 0.823, 
L_clf_acc: 0.003

Ep: [230/300] (TRAIN) ---------------------
Loss: 987.402, L_rx: 329.109, L_kl: 0.067, L_c: 0.001, 
L_clf: 0.781, L_pred: 0.009
Ep: [230/300] (VALID)  ---------------------
L_rx: 328.566, L_rs: 0.001, L_pred: 1.374, L_clf: 0.831, 
L_clf_acc: 0.004

Ep: [240/300] (TRAIN) ---------------------
Loss: 987.401, L_rx: 329.109, L_kl: 0.066, L_c: 0.001, 
L_clf: 0.779, L_pred: 0.009
Ep: [240/300] (VALID)  ---------------------
L_rx: 328.566, L_rs: 0.001, L_pred: 1.383, L_clf: 0.823, 
L_clf_acc: 0.004

Ep: [250/300] (TRAIN) ---------------------
Loss: 987.399, L_rx: 329.109, L_kl: 0.065, L_c: 0.001, 
L_clf: 0.779, L_pred: 0.009
Ep: [250/300] (VALID)  ---------------------
L_rx: 328.566, L_rs: 0.001, L_pred: 1.383, L_clf: 0.819, 
L_clf_acc: 0.005

Ep: [260/300] (TRAIN) ---------------------
Loss: 987.398, L_rx: 329.109, L_kl: 0.064, L_c: 0.001, 
L_clf: 0.776, L_pred: 0.009
Ep: [260/300] (VALID)  ---------------------
L_rx: 328.566, L_rs: 0.001, L_pred: 1.375, L_clf: 0.829, 
L_clf_acc: 0.004

Ep: [270/300] (TRAIN) ---------------------
Loss: 987.398, L_rx: 329.108, L_kl: 0.064, L_c: 0.000, 
L_clf: 0.778, L_pred: 0.009
Ep: [270/300] (VALID)  ---------------------
L_rx: 328.566, L_rs: 0.001, L_pred: 1.406, L_clf: 0.832, 
L_clf_acc: 0.004

Ep: [280/300] (TRAIN) ---------------------
Loss: 987.398, L_rx: 329.108, L_kl: 0.064, L_c: 0.000, 
L_clf: 0.782, L_pred: 0.009
Ep: [280/300] (VALID)  ---------------------
L_rx: 328.566, L_rs: 0.002, L_pred: 1.373, L_clf: 0.819, 
L_clf_acc: 0.004

Ep: [290/300] (TRAIN) ---------------------
Loss: 987.397, L_rx: 329.108, L_kl: 0.064, L_c: 0.000, 
L_clf: 0.777, L_pred: 0.009
Ep: [290/300] (VALID)  ---------------------
L_rx: 328.566, L_rs: 0.001, L_pred: 1.384, L_clf: 0.839, 
L_clf_acc: 0.004

Ep: [300/300] (TRAIN) ---------------------
Loss: 987.397, L_rx: 329.108, L_kl: 0.064, L_c: 0.000, 
L_clf: 0.776, L_pred: 0.009
Ep: [300/300] (VALID)  ---------------------
L_rx: 328.566, L_rs: 0.001, L_pred: 1.380, L_clf: 0.816, 
L_clf_acc: 0.004
input_dim = 10, hidden_dim = 8, output_dim = 1
==============================================
1stage training start.
 dataset size: (17432, 10)
==============================================
len train loader :1
train tot iters :300
input_dim = 10, hidden_dim = 8, output_dim = 1

Ep: [10/300] (TRAIN) ---------------------
Loss: 345.987, L_rx: 338.359, L_kl: 0.439, L_c: 0.007, 
L_clf: 1.627, L_pred: 2.658
Ep: [10/300] (VALID)  ---------------------
L_rx: 333.920, L_rs: 0.905, L_pred: 1.954, L_clf: 1.403, 
L_clf_acc: 0.002

Ep: [20/300] (TRAIN) ---------------------
Loss: 996.571, L_rx: 332.347, L_kl: 1.931, L_c: 1.045, 
L_clf: 1.041, L_pred: 2.134
Ep: [20/300] (VALID)  ---------------------
L_rx: 327.829, L_rs: 1.252, L_pred: 2.309, L_clf: 0.981, 
L_clf_acc: 0.002

Ep: [30/300] (TRAIN) ---------------------
Loss: 994.084, L_rx: 330.778, L_kl: 0.688, L_c: 1.010, 
L_clf: 1.003, L_pred: 0.827
Ep: [30/300] (VALID)  ---------------------
L_rx: 326.945, L_rs: 0.772, L_pred: 0.820, L_clf: 0.941, 
L_clf_acc: 0.003

Ep: [40/300] (TRAIN) ---------------------
Loss: 992.150, L_rx: 330.599, L_kl: 0.315, L_c: 0.020, 
L_clf: 0.912, L_pred: 0.704
Ep: [40/300] (VALID)  ---------------------
L_rx: 326.880, L_rs: 0.721, L_pred: 0.843, L_clf: 0.838, 
L_clf_acc: 0.003

Ep: [50/300] (TRAIN) ---------------------
Loss: 991.799, L_rx: 330.608, L_kl: 0.128, L_c: 0.013, 
L_clf: 0.886, L_pred: 0.519
Ep: [50/300] (VALID)  ---------------------
L_rx: 326.892, L_rs: 0.707, L_pred: 0.820, L_clf: 0.826, 
L_clf_acc: 0.001

Ep: [60/300] (TRAIN) ---------------------
Loss: 991.348, L_rx: 330.597, L_kl: 0.098, L_c: 0.010, 
L_clf: 0.875, L_pred: 0.138
Ep: [60/300] (VALID)  ---------------------
L_rx: 326.883, L_rs: 0.704, L_pred: 0.984, L_clf: 0.813, 
L_clf_acc: 0.002

Ep: [70/300] (TRAIN) ---------------------
Loss: 991.218, L_rx: 330.588, L_kl: 0.083, L_c: 0.006, 
L_clf: 0.865, L_pred: 0.048
Ep: [70/300] (VALID)  ---------------------
L_rx: 326.884, L_rs: 0.682, L_pred: 1.350, L_clf: 0.800, 
L_clf_acc: 0.002

Ep: [80/300] (TRAIN) ---------------------
Loss: 991.130, L_rx: 330.559, L_kl: 0.075, L_c: 0.007, 
L_clf: 0.858, L_pred: 0.029
Ep: [80/300] (VALID)  ---------------------
L_rx: 326.877, L_rs: 0.657, L_pred: 1.476, L_clf: 0.794, 
L_clf_acc: 0.001

Ep: [90/300] (TRAIN) ---------------------
Loss: 991.212, L_rx: 330.604, L_kl: 0.089, L_c: 0.011, 
L_clf: 0.851, L_pred: 0.013
Ep: [90/300] (VALID)  ---------------------
L_rx: 326.887, L_rs: 0.736, L_pred: 1.309, L_clf: 0.800, 
L_clf_acc: 0.002

Ep: [100/300] (TRAIN) ---------------------
Loss: 990.153, L_rx: 330.064, L_kl: 0.089, L_c: 0.012, 
L_clf: 0.849, L_pred: 0.020
Ep: [100/300] (VALID)  ---------------------
L_rx: 326.889, L_rs: 0.077, L_pred: 1.191, L_clf: 0.793, 
L_clf_acc: 0.001

Ep: [110/300] (TRAIN) ---------------------
Loss: 989.867, L_rx: 329.918, L_kl: 0.112, L_c: 0.006, 
L_clf: 0.850, L_pred: 0.013
Ep: [110/300] (VALID)  ---------------------
L_rx: 326.875, L_rs: 0.010, L_pred: 1.224, L_clf: 0.779, 
L_clf_acc: 0.001

Ep: [120/300] (TRAIN) ---------------------
Loss: 989.819, L_rx: 329.906, L_kl: 0.082, L_c: 0.004, 
L_clf: 0.845, L_pred: 0.018
Ep: [120/300] (VALID)  ---------------------
L_rx: 326.878, L_rs: 0.003, L_pred: 1.202, L_clf: 0.791, 
L_clf_acc: 0.003

Ep: [130/300] (TRAIN) ---------------------
Loss: 989.797, L_rx: 329.900, L_kl: 0.077, L_c: 0.001, 
L_clf: 0.839, L_pred: 0.019
Ep: [130/300] (VALID)  ---------------------
L_rx: 326.874, L_rs: 0.002, L_pred: 1.165, L_clf: 0.781, 
L_clf_acc: 0.002

Ep: [140/300] (TRAIN) ---------------------
Loss: 989.786, L_rx: 329.898, L_kl: 0.071, L_c: 0.001, 
L_clf: 0.841, L_pred: 0.023
Ep: [140/300] (VALID)  ---------------------
L_rx: 326.873, L_rs: 0.002, L_pred: 1.206, L_clf: 0.780, 
L_clf_acc: 0.002

Ep: [150/300] (TRAIN) ---------------------
Loss: 989.768, L_rx: 329.895, L_kl: 0.070, L_c: 0.001, 
L_clf: 0.838, L_pred: 0.014
Ep: [150/300] (VALID)  ---------------------
L_rx: 326.871, L_rs: 0.002, L_pred: 1.162, L_clf: 0.771, 
L_clf_acc: 0.002

Ep: [160/300] (TRAIN) ---------------------
Loss: 989.754, L_rx: 329.892, L_kl: 0.067, L_c: 0.001, 
L_clf: 0.832, L_pred: 0.013
Ep: [160/300] (VALID)  ---------------------
L_rx: 326.867, L_rs: 0.002, L_pred: 1.208, L_clf: 0.769, 
L_clf_acc: 0.002

Ep: [170/300] (TRAIN) ---------------------
Loss: 989.743, L_rx: 329.889, L_kl: 0.066, L_c: 0.001, 
L_clf: 0.831, L_pred: 0.012
Ep: [170/300] (VALID)  ---------------------
L_rx: 326.861, L_rs: 0.001, L_pred: 1.213, L_clf: 0.763, 
L_clf_acc: 0.002

Ep: [180/300] (TRAIN) ---------------------
Loss: 989.730, L_rx: 329.885, L_kl: 0.064, L_c: 0.001, 
L_clf: 0.818, L_pred: 0.011
Ep: [180/300] (VALID)  ---------------------
L_rx: 326.857, L_rs: 0.001, L_pred: 1.201, L_clf: 0.772, 
L_clf_acc: 0.002

Ep: [190/300] (TRAIN) ---------------------
Loss: 989.721, L_rx: 329.883, L_kl: 0.064, L_c: 0.001, 
L_clf: 0.814, L_pred: 0.009
Ep: [190/300] (VALID)  ---------------------
L_rx: 326.854, L_rs: 0.001, L_pred: 1.163, L_clf: 0.757, 
L_clf_acc: 0.002

Ep: [200/300] (TRAIN) ---------------------
Loss: 989.713, L_rx: 329.881, L_kl: 0.062, L_c: 0.001, 
L_clf: 0.808, L_pred: 0.009
Ep: [200/300] (VALID)  ---------------------
L_rx: 326.853, L_rs: 0.001, L_pred: 1.188, L_clf: 0.756, 
L_clf_acc: 0.002

Ep: [210/300] (TRAIN) ---------------------
Loss: 989.709, L_rx: 329.880, L_kl: 0.060, L_c: 0.001, 
L_clf: 0.806, L_pred: 0.009
Ep: [210/300] (VALID)  ---------------------
L_rx: 326.853, L_rs: 0.001, L_pred: 1.247, L_clf: 0.738, 
L_clf_acc: 0.002

Ep: [220/300] (TRAIN) ---------------------
Loss: 989.706, L_rx: 329.880, L_kl: 0.058, L_c: 0.001, 
L_clf: 0.801, L_pred: 0.009
Ep: [220/300] (VALID)  ---------------------
L_rx: 326.852, L_rs: 0.001, L_pred: 1.256, L_clf: 0.746, 
L_clf_acc: 0.002

Ep: [230/300] (TRAIN) ---------------------
Loss: 989.704, L_rx: 329.880, L_kl: 0.056, L_c: 0.001, 
L_clf: 0.799, L_pred: 0.009
Ep: [230/300] (VALID)  ---------------------
L_rx: 326.852, L_rs: 0.001, L_pred: 1.239, L_clf: 0.746, 
L_clf_acc: 0.002

Ep: [240/300] (TRAIN) ---------------------
Loss: 989.703, L_rx: 329.880, L_kl: 0.055, L_c: 0.001, 
L_clf: 0.804, L_pred: 0.008
Ep: [240/300] (VALID)  ---------------------
L_rx: 326.852, L_rs: 0.001, L_pred: 1.221, L_clf: 0.750, 
L_clf_acc: 0.002

Ep: [250/300] (TRAIN) ---------------------
Loss: 989.702, L_rx: 329.880, L_kl: 0.055, L_c: 0.001, 
L_clf: 0.803, L_pred: 0.008
Ep: [250/300] (VALID)  ---------------------
L_rx: 326.852, L_rs: 0.001, L_pred: 1.225, L_clf: 0.747, 
L_clf_acc: 0.002

Ep: [260/300] (TRAIN) ---------------------
Loss: 989.701, L_rx: 329.879, L_kl: 0.054, L_c: 0.001, 
L_clf: 0.800, L_pred: 0.008
Ep: [260/300] (VALID)  ---------------------
L_rx: 326.852, L_rs: 0.001, L_pred: 1.248, L_clf: 0.745, 
L_clf_acc: 0.002

Ep: [270/300] (TRAIN) ---------------------
Loss: 989.700, L_rx: 329.880, L_kl: 0.054, L_c: 0.001, 
L_clf: 0.800, L_pred: 0.008
Ep: [270/300] (VALID)  ---------------------
L_rx: 326.852, L_rs: 0.001, L_pred: 1.243, L_clf: 0.731, 
L_clf_acc: 0.002

Ep: [280/300] (TRAIN) ---------------------
Loss: 989.700, L_rx: 329.879, L_kl: 0.054, L_c: 0.001, 
L_clf: 0.797, L_pred: 0.008
Ep: [280/300] (VALID)  ---------------------
L_rx: 326.852, L_rs: 0.001, L_pred: 1.241, L_clf: 0.741, 
L_clf_acc: 0.002

Ep: [290/300] (TRAIN) ---------------------
Loss: 989.700, L_rx: 329.880, L_kl: 0.054, L_c: 0.001, 
L_clf: 0.801, L_pred: 0.008
Ep: [290/300] (VALID)  ---------------------
L_rx: 326.852, L_rs: 0.001, L_pred: 1.230, L_clf: 0.754, 
L_clf_acc: 0.002

Ep: [300/300] (TRAIN) ---------------------
Loss: 989.700, L_rx: 329.879, L_kl: 0.053, L_c: 0.001, 
L_clf: 0.800, L_pred: 0.008
Ep: [300/300] (VALID)  ---------------------
L_rx: 326.852, L_rs: 0.001, L_pred: 1.232, L_clf: 0.740, 
L_clf_acc: 0.002
input_dim = 10, hidden_dim = 8, output_dim = 1
Epoch: [10/300]
Train - L_pred_s: 0.685, L_pred_y: 0.935, L_adv: -2.746, s_acc: 0.563
Test - L_pred_s: 0.915, L_pred_y: 0.686, L_adv: -0.686, s_acc: 0.557
Epoch: [20/300]
Train - L_pred_s: 0.684, L_pred_y: 0.884, L_adv: -2.739, s_acc: 0.564
Test - L_pred_s: 0.874, L_pred_y: 0.686, L_adv: -0.686, s_acc: 0.557
Epoch: [30/300]
Train - L_pred_s: 0.682, L_pred_y: 0.863, L_adv: -2.750, s_acc: 0.566
Test - L_pred_s: 0.849, L_pred_y: 0.684, L_adv: -0.689, s_acc: 0.557
Epoch: [40/300]
Train - L_pred_s: 0.680, L_pred_y: 0.850, L_adv: -2.741, s_acc: 0.571
Test - L_pred_s: 0.846, L_pred_y: 0.683, L_adv: -0.691, s_acc: 0.567
Epoch: [50/300]
Train - L_pred_s: 0.678, L_pred_y: 0.840, L_adv: -2.739, s_acc: 0.574
Test - L_pred_s: 0.834, L_pred_y: 0.680, L_adv: -0.690, s_acc: 0.587
Epoch: [60/300]
Train - L_pred_s: 0.684, L_pred_y: 0.808, L_adv: -2.689, s_acc: 0.564
Test - L_pred_s: 0.795, L_pred_y: 0.684, L_adv: -0.689, s_acc: 0.557
Epoch: [70/300]
Train - L_pred_s: 0.684, L_pred_y: 0.803, L_adv: -2.759, s_acc: 0.564
Test - L_pred_s: 0.797, L_pred_y: 0.685, L_adv: -0.680, s_acc: 0.557
Epoch: [80/300]
Train - L_pred_s: 0.683, L_pred_y: 0.806, L_adv: -2.758, s_acc: 0.565
Test - L_pred_s: 0.793, L_pred_y: 0.684, L_adv: -0.682, s_acc: 0.557
Epoch: [90/300]
Train - L_pred_s: 0.684, L_pred_y: 0.804, L_adv: -2.759, s_acc: 0.564
Test - L_pred_s: 0.816, L_pred_y: 0.685, L_adv: -0.683, s_acc: 0.557
Epoch: [100/300]
Train - L_pred_s: 0.682, L_pred_y: 0.827, L_adv: -2.735, s_acc: 0.574
Test - L_pred_s: 0.804, L_pred_y: 0.682, L_adv: -0.689, s_acc: 0.577
Epoch: [110/300]
Train - L_pred_s: 0.682, L_pred_y: 0.790, L_adv: -2.736, s_acc: 0.570
Test - L_pred_s: 0.786, L_pred_y: 0.681, L_adv: -0.687, s_acc: 0.575
Epoch: [120/300]
Train - L_pred_s: 0.682, L_pred_y: 0.781, L_adv: -2.744, s_acc: 0.572
Test - L_pred_s: 0.768, L_pred_y: 0.682, L_adv: -0.685, s_acc: 0.578
Epoch: [130/300]
Train - L_pred_s: 0.683, L_pred_y: 0.820, L_adv: -2.752, s_acc: 0.568
Test - L_pred_s: 0.811, L_pred_y: 0.683, L_adv: -0.683, s_acc: 0.564
Epoch: [140/300]
Train - L_pred_s: 0.682, L_pred_y: 0.782, L_adv: -2.735, s_acc: 0.572
Test - L_pred_s: 0.780, L_pred_y: 0.681, L_adv: -0.687, s_acc: 0.580
Epoch: [150/300]
Train - L_pred_s: 0.682, L_pred_y: 0.791, L_adv: -2.736, s_acc: 0.574
Test - L_pred_s: 0.774, L_pred_y: 0.682, L_adv: -0.683, s_acc: 0.575
Epoch: [160/300]
Train - L_pred_s: 0.682, L_pred_y: 0.778, L_adv: -2.735, s_acc: 0.570
Test - L_pred_s: 0.767, L_pred_y: 0.682, L_adv: -0.684, s_acc: 0.574
Epoch: [170/300]
Train - L_pred_s: 0.683, L_pred_y: 0.785, L_adv: -2.734, s_acc: 0.571
Test - L_pred_s: 0.770, L_pred_y: 0.683, L_adv: -0.684, s_acc: 0.567
Epoch: [180/300]
Train - L_pred_s: 0.683, L_pred_y: 0.773, L_adv: -2.734, s_acc: 0.571
Test - L_pred_s: 0.762, L_pred_y: 0.683, L_adv: -0.684, s_acc: 0.571
Epoch: [190/300]
Train - L_pred_s: 0.683, L_pred_y: 0.771, L_adv: -2.733, s_acc: 0.571
Test - L_pred_s: 0.759, L_pred_y: 0.682, L_adv: -0.684, s_acc: 0.575
Epoch: [200/300]
Train - L_pred_s: 0.682, L_pred_y: 0.768, L_adv: -2.733, s_acc: 0.572
Test - L_pred_s: 0.760, L_pred_y: 0.682, L_adv: -0.684, s_acc: 0.576
Epoch: [210/300]
Train - L_pred_s: 0.683, L_pred_y: 0.768, L_adv: -2.735, s_acc: 0.570
Test - L_pred_s: 0.760, L_pred_y: 0.683, L_adv: -0.684, s_acc: 0.571
Epoch: [220/300]
Train - L_pred_s: 0.683, L_pred_y: 0.767, L_adv: -2.733, s_acc: 0.570
Test - L_pred_s: 0.758, L_pred_y: 0.683, L_adv: -0.684, s_acc: 0.575
Epoch: [230/300]
Train - L_pred_s: 0.683, L_pred_y: 0.770, L_adv: -2.733, s_acc: 0.569
Test - L_pred_s: 0.759, L_pred_y: 0.683, L_adv: -0.684, s_acc: 0.573
Epoch: [240/300]
Train - L_pred_s: 0.682, L_pred_y: 0.768, L_adv: -2.732, s_acc: 0.571
Test - L_pred_s: 0.758, L_pred_y: 0.682, L_adv: -0.684, s_acc: 0.576
Epoch: [250/300]
Train - L_pred_s: 0.682, L_pred_y: 0.768, L_adv: -2.733, s_acc: 0.571
Test - L_pred_s: 0.758, L_pred_y: 0.682, L_adv: -0.684, s_acc: 0.573
Epoch: [260/300]
Train - L_pred_s: 0.683, L_pred_y: 0.768, L_adv: -2.733, s_acc: 0.570
Test - L_pred_s: 0.758, L_pred_y: 0.683, L_adv: -0.684, s_acc: 0.570
Epoch: [270/300]
Train - L_pred_s: 0.682, L_pred_y: 0.767, L_adv: -2.733, s_acc: 0.572
Test - L_pred_s: 0.757, L_pred_y: 0.682, L_adv: -0.684, s_acc: 0.576
Epoch: [280/300]
Train - L_pred_s: 0.682, L_pred_y: 0.769, L_adv: -2.733, s_acc: 0.570
Test - L_pred_s: 0.757, L_pred_y: 0.683, L_adv: -0.684, s_acc: 0.575
Epoch: [290/300]
Train - L_pred_s: 0.683, L_pred_y: 0.774, L_adv: -2.733, s_acc: 0.571
Test - L_pred_s: 0.757, L_pred_y: 0.683, L_adv: -0.684, s_acc: 0.576
Epoch: [300/300]
Train - L_pred_s: 0.683, L_pred_y: 0.766, L_adv: -2.733, s_acc: 0.571
Test - L_pred_s: 0.757, L_pred_y: 0.683, L_adv: -0.684, s_acc: 0.573
Epoch: [10/300]
Train - L_pred_s: 0.686, L_pred_y: 0.928, L_adv: -2.743, s_acc: 0.564
Test - L_pred_s: 0.886, L_pred_y: 0.684, L_adv: -0.686, s_acc: 0.565
Epoch: [20/300]
Train - L_pred_s: 0.684, L_pred_y: 0.876, L_adv: -2.741, s_acc: 0.564
Test - L_pred_s: 0.874, L_pred_y: 0.684, L_adv: -0.685, s_acc: 0.565
Epoch: [30/300]
Train - L_pred_s: 0.681, L_pred_y: 0.870, L_adv: -2.740, s_acc: 0.569
Test - L_pred_s: 0.885, L_pred_y: 0.681, L_adv: -0.686, s_acc: 0.574
Epoch: [40/300]
Train - L_pred_s: 0.686, L_pred_y: 0.868, L_adv: -2.750, s_acc: 0.564
Test - L_pred_s: 0.874, L_pred_y: 0.685, L_adv: -0.677, s_acc: 0.565
Epoch: [50/300]
Train - L_pred_s: 0.685, L_pred_y: 0.869, L_adv: -2.739, s_acc: 0.564
Test - L_pred_s: 0.877, L_pred_y: 0.685, L_adv: -0.685, s_acc: 0.565
Epoch: [60/300]
Train - L_pred_s: 0.684, L_pred_y: 0.863, L_adv: -2.739, s_acc: 0.564
Test - L_pred_s: 0.873, L_pred_y: 0.685, L_adv: -0.688, s_acc: 0.565
Epoch: [70/300]
Train - L_pred_s: 0.685, L_pred_y: 0.855, L_adv: -2.744, s_acc: 0.564
Test - L_pred_s: 0.856, L_pred_y: 0.685, L_adv: -0.686, s_acc: 0.565
Epoch: [80/300]
Train - L_pred_s: 0.685, L_pred_y: 0.860, L_adv: -2.751, s_acc: 0.564
Test - L_pred_s: 0.857, L_pred_y: 0.685, L_adv: -0.684, s_acc: 0.565
Epoch: [90/300]
Train - L_pred_s: 0.685, L_pred_y: 0.858, L_adv: -2.747, s_acc: 0.564
Test - L_pred_s: 0.856, L_pred_y: 0.685, L_adv: -0.686, s_acc: 0.565
Epoch: [100/300]
Train - L_pred_s: 0.684, L_pred_y: 0.840, L_adv: -2.753, s_acc: 0.564
Test - L_pred_s: 0.851, L_pred_y: 0.686, L_adv: -0.689, s_acc: 0.565
Epoch: [110/300]
Train - L_pred_s: 0.684, L_pred_y: 0.855, L_adv: -2.747, s_acc: 0.564
Test - L_pred_s: 0.854, L_pred_y: 0.684, L_adv: -0.687, s_acc: 0.565
Epoch: [120/300]
Train - L_pred_s: 0.683, L_pred_y: 0.847, L_adv: -2.723, s_acc: 0.570
Test - L_pred_s: 0.831, L_pred_y: 0.683, L_adv: -0.685, s_acc: 0.569
Epoch: [130/300]
Train - L_pred_s: 0.681, L_pred_y: 0.813, L_adv: -2.726, s_acc: 0.575
Test - L_pred_s: 0.814, L_pred_y: 0.684, L_adv: -0.683, s_acc: 0.568
Epoch: [140/300]
Train - L_pred_s: 0.682, L_pred_y: 0.806, L_adv: -2.731, s_acc: 0.570
Test - L_pred_s: 0.807, L_pred_y: 0.684, L_adv: -0.683, s_acc: 0.559
Epoch: [150/300]
Train - L_pred_s: 0.682, L_pred_y: 0.801, L_adv: -2.728, s_acc: 0.573
Test - L_pred_s: 0.789, L_pred_y: 0.684, L_adv: -0.683, s_acc: 0.559
Epoch: [160/300]
Train - L_pred_s: 0.682, L_pred_y: 0.798, L_adv: -2.734, s_acc: 0.571
Test - L_pred_s: 0.787, L_pred_y: 0.684, L_adv: -0.682, s_acc: 0.564
Epoch: [170/300]
Train - L_pred_s: 0.682, L_pred_y: 0.793, L_adv: -2.726, s_acc: 0.571
Test - L_pred_s: 0.781, L_pred_y: 0.684, L_adv: -0.684, s_acc: 0.564
Epoch: [180/300]
Train - L_pred_s: 0.683, L_pred_y: 0.784, L_adv: -2.729, s_acc: 0.570
Test - L_pred_s: 0.776, L_pred_y: 0.684, L_adv: -0.684, s_acc: 0.563
Epoch: [190/300]
Train - L_pred_s: 0.683, L_pred_y: 0.786, L_adv: -2.731, s_acc: 0.571
Test - L_pred_s: 0.779, L_pred_y: 0.684, L_adv: -0.682, s_acc: 0.565
Epoch: [200/300]
Train - L_pred_s: 0.682, L_pred_y: 0.819, L_adv: -2.726, s_acc: 0.572
Test - L_pred_s: 0.812, L_pred_y: 0.683, L_adv: -0.683, s_acc: 0.568
Epoch: [210/300]
Train - L_pred_s: 0.682, L_pred_y: 0.801, L_adv: -2.730, s_acc: 0.571
Test - L_pred_s: 0.800, L_pred_y: 0.684, L_adv: -0.684, s_acc: 0.564
Epoch: [220/300]
Train - L_pred_s: 0.682, L_pred_y: 0.791, L_adv: -2.729, s_acc: 0.574
Test - L_pred_s: 0.778, L_pred_y: 0.683, L_adv: -0.683, s_acc: 0.565
Epoch: [230/300]
Train - L_pred_s: 0.682, L_pred_y: 0.788, L_adv: -2.733, s_acc: 0.572
Test - L_pred_s: 0.777, L_pred_y: 0.683, L_adv: -0.683, s_acc: 0.574
Epoch: [240/300]
Train - L_pred_s: 0.682, L_pred_y: 0.783, L_adv: -2.729, s_acc: 0.574
Test - L_pred_s: 0.767, L_pred_y: 0.683, L_adv: -0.683, s_acc: 0.573
Epoch: [250/300]
Train - L_pred_s: 0.682, L_pred_y: 0.785, L_adv: -2.731, s_acc: 0.572
Test - L_pred_s: 0.772, L_pred_y: 0.683, L_adv: -0.683, s_acc: 0.565
Epoch: [260/300]
Train - L_pred_s: 0.682, L_pred_y: 0.784, L_adv: -2.730, s_acc: 0.573
Test - L_pred_s: 0.768, L_pred_y: 0.683, L_adv: -0.683, s_acc: 0.569
Epoch: [270/300]
Train - L_pred_s: 0.682, L_pred_y: 0.780, L_adv: -2.731, s_acc: 0.573
Test - L_pred_s: 0.769, L_pred_y: 0.684, L_adv: -0.683, s_acc: 0.565
Epoch: [280/300]
Train - L_pred_s: 0.682, L_pred_y: 0.786, L_adv: -2.731, s_acc: 0.572
Test - L_pred_s: 0.768, L_pred_y: 0.683, L_adv: -0.684, s_acc: 0.566
Epoch: [290/300]
Train - L_pred_s: 0.683, L_pred_y: 0.783, L_adv: -2.731, s_acc: 0.571
Test - L_pred_s: 0.767, L_pred_y: 0.683, L_adv: -0.683, s_acc: 0.571
Epoch: [300/300]
Train - L_pred_s: 0.682, L_pred_y: 0.785, L_adv: -2.731, s_acc: 0.572
Test - L_pred_s: 0.767, L_pred_y: 0.683, L_adv: -0.683, s_acc: 0.572
Epoch: [10/300]
Train - L_pred_s: 0.685, L_pred_y: 0.892, L_adv: -2.743, s_acc: 0.564
Test - L_pred_s: 0.873, L_pred_y: 0.689, L_adv: -0.686, s_acc: 0.548
Epoch: [20/300]
Train - L_pred_s: 0.685, L_pred_y: 0.880, L_adv: -2.740, s_acc: 0.564
Test - L_pred_s: 0.868, L_pred_y: 0.689, L_adv: -0.685, s_acc: 0.548
Epoch: [30/300]
Train - L_pred_s: 0.685, L_pred_y: 0.876, L_adv: -2.754, s_acc: 0.564
Test - L_pred_s: 0.869, L_pred_y: 0.689, L_adv: -0.684, s_acc: 0.548
Epoch: [40/300]
Train - L_pred_s: 0.685, L_pred_y: 0.870, L_adv: -2.741, s_acc: 0.564
Test - L_pred_s: 0.861, L_pred_y: 0.689, L_adv: -0.685, s_acc: 0.548
Epoch: [50/300]
Train - L_pred_s: 0.684, L_pred_y: 0.856, L_adv: -2.758, s_acc: 0.564
Test - L_pred_s: 0.844, L_pred_y: 0.688, L_adv: -0.691, s_acc: 0.548
Epoch: [60/300]
Train - L_pred_s: 0.685, L_pred_y: 0.878, L_adv: -2.739, s_acc: 0.564
Test - L_pred_s: 0.869, L_pred_y: 0.689, L_adv: -0.685, s_acc: 0.548
Epoch: [70/300]
Train - L_pred_s: 0.684, L_pred_y: 0.862, L_adv: -2.748, s_acc: 0.564
Test - L_pred_s: 0.857, L_pred_y: 0.686, L_adv: -0.687, s_acc: 0.548
Epoch: [80/300]
Train - L_pred_s: 0.684, L_pred_y: 0.857, L_adv: -2.745, s_acc: 0.565
Test - L_pred_s: 0.848, L_pred_y: 0.687, L_adv: -0.690, s_acc: 0.548
Epoch: [90/300]
Train - L_pred_s: 0.684, L_pred_y: 0.885, L_adv: -2.758, s_acc: 0.564
Test - L_pred_s: 0.873, L_pred_y: 0.687, L_adv: -0.687, s_acc: 0.548
Epoch: [100/300]
Train - L_pred_s: 0.685, L_pred_y: 0.877, L_adv: -2.711, s_acc: 0.564
Test - L_pred_s: 0.873, L_pred_y: 0.689, L_adv: -0.687, s_acc: 0.548
Epoch: [110/300]
Train - L_pred_s: 0.685, L_pred_y: 0.877, L_adv: -2.744, s_acc: 0.564
Test - L_pred_s: 0.870, L_pred_y: 0.689, L_adv: -0.684, s_acc: 0.548
Epoch: [120/300]
Train - L_pred_s: 0.685, L_pred_y: 0.876, L_adv: -2.740, s_acc: 0.564
Test - L_pred_s: 0.870, L_pred_y: 0.689, L_adv: -0.685, s_acc: 0.548
Epoch: [130/300]
Train - L_pred_s: 0.685, L_pred_y: 0.879, L_adv: -2.740, s_acc: 0.564
Test - L_pred_s: 0.872, L_pred_y: 0.689, L_adv: -0.685, s_acc: 0.548
Epoch: [140/300]
Train - L_pred_s: 0.685, L_pred_y: 0.876, L_adv: -2.740, s_acc: 0.564
Test - L_pred_s: 0.871, L_pred_y: 0.689, L_adv: -0.685, s_acc: 0.548
Epoch: [150/300]
Train - L_pred_s: 0.685, L_pred_y: 0.876, L_adv: -2.740, s_acc: 0.564
Test - L_pred_s: 0.871, L_pred_y: 0.689, L_adv: -0.685, s_acc: 0.548
Epoch: [160/300]
Train - L_pred_s: 0.685, L_pred_y: 0.876, L_adv: -2.740, s_acc: 0.564
Test - L_pred_s: 0.870, L_pred_y: 0.689, L_adv: -0.685, s_acc: 0.548
Epoch: [170/300]
Train - L_pred_s: 0.685, L_pred_y: 0.873, L_adv: -2.740, s_acc: 0.564
Test - L_pred_s: 0.867, L_pred_y: 0.689, L_adv: -0.685, s_acc: 0.548
Epoch: [180/300]
Train - L_pred_s: 0.685, L_pred_y: 0.877, L_adv: -2.741, s_acc: 0.564
Test - L_pred_s: 0.872, L_pred_y: 0.689, L_adv: -0.685, s_acc: 0.548
Epoch: [190/300]
Train - L_pred_s: 0.685, L_pred_y: 0.876, L_adv: -2.740, s_acc: 0.564
Test - L_pred_s: 0.870, L_pred_y: 0.689, L_adv: -0.685, s_acc: 0.548
Epoch: [200/300]
Train - L_pred_s: 0.685, L_pred_y: 0.876, L_adv: -2.740, s_acc: 0.564
Test - L_pred_s: 0.870, L_pred_y: 0.689, L_adv: -0.685, s_acc: 0.548
Epoch: [210/300]
Train - L_pred_s: 0.685, L_pred_y: 0.876, L_adv: -2.740, s_acc: 0.564
Test - L_pred_s: 0.871, L_pred_y: 0.689, L_adv: -0.685, s_acc: 0.548
Epoch: [220/300]
Train - L_pred_s: 0.685, L_pred_y: 0.876, L_adv: -2.740, s_acc: 0.564
Test - L_pred_s: 0.871, L_pred_y: 0.689, L_adv: -0.685, s_acc: 0.548
Epoch: [230/300]
Train - L_pred_s: 0.685, L_pred_y: 0.876, L_adv: -2.740, s_acc: 0.564
Test - L_pred_s: 0.870, L_pred_y: 0.689, L_adv: -0.685, s_acc: 0.548
Epoch: [240/300]
Train - L_pred_s: 0.685, L_pred_y: 0.876, L_adv: -2.740, s_acc: 0.564
Test - L_pred_s: 0.870, L_pred_y: 0.689, L_adv: -0.685, s_acc: 0.548
Epoch: [250/300]
Train - L_pred_s: 0.685, L_pred_y: 0.876, L_adv: -2.740, s_acc: 0.564
Test - L_pred_s: 0.870, L_pred_y: 0.689, L_adv: -0.685, s_acc: 0.548
Epoch: [260/300]
Train - L_pred_s: 0.685, L_pred_y: 0.876, L_adv: -2.740, s_acc: 0.564
Test - L_pred_s: 0.870, L_pred_y: 0.689, L_adv: -0.685, s_acc: 0.548
Epoch: [270/300]
Train - L_pred_s: 0.685, L_pred_y: 0.876, L_adv: -2.740, s_acc: 0.564
Test - L_pred_s: 0.870, L_pred_y: 0.689, L_adv: -0.685, s_acc: 0.548
Epoch: [280/300]
Train - L_pred_s: 0.685, L_pred_y: 0.876, L_adv: -2.740, s_acc: 0.564
Test - L_pred_s: 0.870, L_pred_y: 0.689, L_adv: -0.685, s_acc: 0.548
Epoch: [290/300]
Train - L_pred_s: 0.685, L_pred_y: 0.876, L_adv: -2.740, s_acc: 0.564
Test - L_pred_s: 0.870, L_pred_y: 0.689, L_adv: -0.685, s_acc: 0.548
Epoch: [300/300]
Train - L_pred_s: 0.685, L_pred_y: 0.876, L_adv: -2.740, s_acc: 0.564
Test - L_pred_s: 0.870, L_pred_y: 0.689, L_adv: -0.685, s_acc: 0.548
Epoch: [10/300]
Train - L_pred_s: 0.687, L_pred_y: 0.880, L_adv: -2.753, s_acc: 0.553
Test - L_pred_s: 0.908, L_pred_y: 0.685, L_adv: -0.690, s_acc: 0.566
Epoch: [20/300]
Train - L_pred_s: 0.685, L_pred_y: 0.867, L_adv: -2.745, s_acc: 0.561
Test - L_pred_s: 0.895, L_pred_y: 0.684, L_adv: -0.687, s_acc: 0.566
Epoch: [30/300]
Train - L_pred_s: 0.685, L_pred_y: 0.857, L_adv: -2.748, s_acc: 0.561
Test - L_pred_s: 0.894, L_pred_y: 0.682, L_adv: -0.687, s_acc: 0.566
Epoch: [40/300]
Train - L_pred_s: 0.683, L_pred_y: 0.844, L_adv: -2.755, s_acc: 0.562
Test - L_pred_s: 0.863, L_pred_y: 0.683, L_adv: -0.689, s_acc: 0.566
Epoch: [50/300]
Train - L_pred_s: 0.684, L_pred_y: 0.810, L_adv: -2.736, s_acc: 0.562
Test - L_pred_s: 0.843, L_pred_y: 0.681, L_adv: -0.685, s_acc: 0.566
Epoch: [60/300]
Train - L_pred_s: 0.684, L_pred_y: 0.806, L_adv: -2.750, s_acc: 0.567
Test - L_pred_s: 0.828, L_pred_y: 0.682, L_adv: -0.686, s_acc: 0.566
Epoch: [70/300]
Train - L_pred_s: 0.684, L_pred_y: 0.821, L_adv: -2.754, s_acc: 0.567
Test - L_pred_s: 0.879, L_pred_y: 0.683, L_adv: -0.689, s_acc: 0.575
Epoch: [80/300]
Train - L_pred_s: 0.684, L_pred_y: 0.791, L_adv: -2.743, s_acc: 0.565
Test - L_pred_s: 0.799, L_pred_y: 0.682, L_adv: -0.686, s_acc: 0.578
Epoch: [90/300]
Train - L_pred_s: 0.683, L_pred_y: 0.807, L_adv: -2.745, s_acc: 0.568
Test - L_pred_s: 0.865, L_pred_y: 0.683, L_adv: -0.689, s_acc: 0.569
Epoch: [100/300]
Train - L_pred_s: 0.684, L_pred_y: 0.874, L_adv: -2.733, s_acc: 0.561
Test - L_pred_s: 0.896, L_pred_y: 0.686, L_adv: -0.691, s_acc: 0.566
Epoch: [110/300]
Train - L_pred_s: 0.686, L_pred_y: 0.865, L_adv: -2.744, s_acc: 0.561
Test - L_pred_s: 0.904, L_pred_y: 0.684, L_adv: -0.686, s_acc: 0.566
Epoch: [120/300]
Train - L_pred_s: 0.686, L_pred_y: 0.865, L_adv: -2.743, s_acc: 0.561
Test - L_pred_s: 0.904, L_pred_y: 0.684, L_adv: -0.686, s_acc: 0.566
Epoch: [130/300]
Train - L_pred_s: 0.686, L_pred_y: 0.864, L_adv: -2.743, s_acc: 0.561
Test - L_pred_s: 0.904, L_pred_y: 0.684, L_adv: -0.686, s_acc: 0.566
Epoch: [140/300]
Train - L_pred_s: 0.686, L_pred_y: 0.864, L_adv: -2.743, s_acc: 0.561
Test - L_pred_s: 0.904, L_pred_y: 0.684, L_adv: -0.686, s_acc: 0.566
Epoch: [150/300]
Train - L_pred_s: 0.686, L_pred_y: 0.864, L_adv: -2.743, s_acc: 0.561
Test - L_pred_s: 0.903, L_pred_y: 0.684, L_adv: -0.686, s_acc: 0.566
Epoch: [160/300]
Train - L_pred_s: 0.686, L_pred_y: 0.862, L_adv: -2.743, s_acc: 0.561
Test - L_pred_s: 0.902, L_pred_y: 0.684, L_adv: -0.686, s_acc: 0.566
Epoch: [170/300]
Train - L_pred_s: 0.686, L_pred_y: 0.860, L_adv: -2.743, s_acc: 0.561
Test - L_pred_s: 0.899, L_pred_y: 0.684, L_adv: -0.686, s_acc: 0.566
Epoch: [180/300]
Train - L_pred_s: 0.686, L_pred_y: 0.853, L_adv: -2.743, s_acc: 0.561
Test - L_pred_s: 0.891, L_pred_y: 0.684, L_adv: -0.686, s_acc: 0.566
Epoch: [190/300]
Train - L_pred_s: 0.686, L_pred_y: 0.836, L_adv: -2.743, s_acc: 0.561
Test - L_pred_s: 0.869, L_pred_y: 0.684, L_adv: -0.686, s_acc: 0.566
Epoch: [200/300]
Train - L_pred_s: 0.686, L_pred_y: 0.793, L_adv: -2.743, s_acc: 0.561
Test - L_pred_s: 0.828, L_pred_y: 0.684, L_adv: -0.686, s_acc: 0.566
Epoch: [210/300]
Train - L_pred_s: 0.686, L_pred_y: 0.782, L_adv: -2.743, s_acc: 0.561
Test - L_pred_s: 0.817, L_pred_y: 0.684, L_adv: -0.686, s_acc: 0.566
Epoch: [220/300]
Train - L_pred_s: 0.686, L_pred_y: 0.774, L_adv: -2.743, s_acc: 0.561
Test - L_pred_s: 0.801, L_pred_y: 0.684, L_adv: -0.686, s_acc: 0.566
Epoch: [230/300]
Train - L_pred_s: 0.686, L_pred_y: 0.769, L_adv: -2.743, s_acc: 0.561
Test - L_pred_s: 0.795, L_pred_y: 0.684, L_adv: -0.686, s_acc: 0.566
Epoch: [240/300]
Train - L_pred_s: 0.686, L_pred_y: 0.770, L_adv: -2.743, s_acc: 0.561
Test - L_pred_s: 0.794, L_pred_y: 0.684, L_adv: -0.686, s_acc: 0.566
Epoch: [250/300]
Train - L_pred_s: 0.686, L_pred_y: 0.767, L_adv: -2.743, s_acc: 0.561
Test - L_pred_s: 0.793, L_pred_y: 0.684, L_adv: -0.686, s_acc: 0.566
Epoch: [260/300]
Train - L_pred_s: 0.686, L_pred_y: 0.764, L_adv: -2.743, s_acc: 0.561
Test - L_pred_s: 0.792, L_pred_y: 0.684, L_adv: -0.686, s_acc: 0.566
Epoch: [270/300]
Train - L_pred_s: 0.686, L_pred_y: 0.764, L_adv: -2.743, s_acc: 0.561
Test - L_pred_s: 0.792, L_pred_y: 0.684, L_adv: -0.686, s_acc: 0.566
Epoch: [280/300]
Train - L_pred_s: 0.686, L_pred_y: 0.768, L_adv: -2.743, s_acc: 0.561
Test - L_pred_s: 0.792, L_pred_y: 0.684, L_adv: -0.686, s_acc: 0.566
Epoch: [290/300]
Train - L_pred_s: 0.686, L_pred_y: 0.763, L_adv: -2.743, s_acc: 0.561
Test - L_pred_s: 0.792, L_pred_y: 0.684, L_adv: -0.686, s_acc: 0.566
Epoch: [300/300]
Train - L_pred_s: 0.686, L_pred_y: 0.763, L_adv: -2.743, s_acc: 0.561
Test - L_pred_s: 0.792, L_pred_y: 0.684, L_adv: -0.686, s_acc: 0.566
Epoch: [10/300]
Train - L_pred_s: 0.688, L_pred_y: 0.875, L_adv: -2.743, s_acc: 0.544
Test - L_pred_s: 0.815, L_pred_y: 0.684, L_adv: -0.690, s_acc: 0.571
Epoch: [20/300]
Train - L_pred_s: 0.687, L_pred_y: 0.869, L_adv: -2.743, s_acc: 0.555
Test - L_pred_s: 0.809, L_pred_y: 0.682, L_adv: -0.688, s_acc: 0.571
Epoch: [30/300]
Train - L_pred_s: 0.684, L_pred_y: 0.869, L_adv: -2.759, s_acc: 0.565
Test - L_pred_s: 0.807, L_pred_y: 0.689, L_adv: -0.693, s_acc: 0.573
Epoch: [40/300]
Train - L_pred_s: 0.686, L_pred_y: 0.862, L_adv: -2.756, s_acc: 0.560
Test - L_pred_s: 0.800, L_pred_y: 0.683, L_adv: -0.689, s_acc: 0.571
Epoch: [50/300]
Train - L_pred_s: 0.686, L_pred_y: 0.855, L_adv: -2.742, s_acc: 0.560
Test - L_pred_s: 0.791, L_pred_y: 0.683, L_adv: -0.686, s_acc: 0.571
Epoch: [60/300]
Train - L_pred_s: 0.685, L_pred_y: 0.835, L_adv: -2.742, s_acc: 0.560
Test - L_pred_s: 0.770, L_pred_y: 0.683, L_adv: -0.687, s_acc: 0.571
Epoch: [70/300]
Train - L_pred_s: 0.683, L_pred_y: 0.821, L_adv: -2.707, s_acc: 0.570
Test - L_pred_s: 0.750, L_pred_y: 0.681, L_adv: -0.685, s_acc: 0.576
Epoch: [80/300]
Train - L_pred_s: 0.682, L_pred_y: 0.807, L_adv: -2.728, s_acc: 0.573
Test - L_pred_s: 0.732, L_pred_y: 0.678, L_adv: -0.684, s_acc: 0.587
Epoch: [90/300]
Train - L_pred_s: 0.686, L_pred_y: 0.845, L_adv: -2.714, s_acc: 0.560
Test - L_pred_s: 0.827, L_pred_y: 0.682, L_adv: -0.681, s_acc: 0.571
Epoch: [100/300]
Train - L_pred_s: 0.684, L_pred_y: 0.835, L_adv: -2.745, s_acc: 0.565
Test - L_pred_s: 0.866, L_pred_y: 0.682, L_adv: -0.680, s_acc: 0.571
Epoch: [110/300]
Train - L_pred_s: 0.682, L_pred_y: 0.865, L_adv: -2.725, s_acc: 0.570
Test - L_pred_s: 0.805, L_pred_y: 0.679, L_adv: -0.683, s_acc: 0.576
Epoch: [120/300]
Train - L_pred_s: 0.683, L_pred_y: 0.855, L_adv: -2.740, s_acc: 0.568
Test - L_pred_s: 0.788, L_pred_y: 0.681, L_adv: -0.689, s_acc: 0.580
Epoch: [130/300]
Train - L_pred_s: 0.684, L_pred_y: 0.825, L_adv: -2.740, s_acc: 0.565
Test - L_pred_s: 0.750, L_pred_y: 0.679, L_adv: -0.684, s_acc: 0.580
Epoch: [140/300]
Train - L_pred_s: 0.683, L_pred_y: 0.810, L_adv: -2.742, s_acc: 0.567
Test - L_pred_s: 0.722, L_pred_y: 0.681, L_adv: -0.688, s_acc: 0.582
Epoch: [150/300]
Train - L_pred_s: 0.686, L_pred_y: 0.848, L_adv: -2.745, s_acc: 0.560
Test - L_pred_s: 0.783, L_pred_y: 0.683, L_adv: -0.686, s_acc: 0.571
Epoch: [160/300]
Train - L_pred_s: 0.685, L_pred_y: 0.817, L_adv: -2.744, s_acc: 0.560
Test - L_pred_s: 0.748, L_pred_y: 0.682, L_adv: -0.686, s_acc: 0.571
Epoch: [170/300]
Train - L_pred_s: 0.684, L_pred_y: 0.805, L_adv: -2.738, s_acc: 0.564
Test - L_pred_s: 0.736, L_pred_y: 0.680, L_adv: -0.685, s_acc: 0.586
Epoch: [180/300]
Train - L_pred_s: 0.684, L_pred_y: 0.807, L_adv: -2.737, s_acc: 0.567
Test - L_pred_s: 0.728, L_pred_y: 0.679, L_adv: -0.684, s_acc: 0.583
Epoch: [190/300]
Train - L_pred_s: 0.683, L_pred_y: 0.800, L_adv: -2.738, s_acc: 0.567
Test - L_pred_s: 0.727, L_pred_y: 0.680, L_adv: -0.687, s_acc: 0.578
Epoch: [200/300]
Train - L_pred_s: 0.683, L_pred_y: 0.809, L_adv: -2.751, s_acc: 0.566
Test - L_pred_s: 0.735, L_pred_y: 0.682, L_adv: -0.690, s_acc: 0.581
Epoch: [210/300]
Train - L_pred_s: 0.684, L_pred_y: 0.806, L_adv: -2.737, s_acc: 0.566
Test - L_pred_s: 0.727, L_pred_y: 0.681, L_adv: -0.687, s_acc: 0.579
Epoch: [220/300]
Train - L_pred_s: 0.684, L_pred_y: 0.812, L_adv: -2.743, s_acc: 0.566
Test - L_pred_s: 0.731, L_pred_y: 0.682, L_adv: -0.688, s_acc: 0.571
Epoch: [230/300]
Train - L_pred_s: 0.684, L_pred_y: 0.805, L_adv: -2.737, s_acc: 0.566
Test - L_pred_s: 0.733, L_pred_y: 0.681, L_adv: -0.685, s_acc: 0.587
Epoch: [240/300]
Train - L_pred_s: 0.684, L_pred_y: 0.798, L_adv: -2.739, s_acc: 0.566
Test - L_pred_s: 0.720, L_pred_y: 0.681, L_adv: -0.685, s_acc: 0.585
Epoch: [250/300]
Train - L_pred_s: 0.684, L_pred_y: 0.792, L_adv: -2.742, s_acc: 0.566
Test - L_pred_s: 0.718, L_pred_y: 0.681, L_adv: -0.686, s_acc: 0.583
Epoch: [260/300]
Train - L_pred_s: 0.684, L_pred_y: 0.793, L_adv: -2.742, s_acc: 0.564
Test - L_pred_s: 0.711, L_pred_y: 0.680, L_adv: -0.685, s_acc: 0.581
Epoch: [270/300]
Train - L_pred_s: 0.685, L_pred_y: 0.793, L_adv: -2.738, s_acc: 0.562
Test - L_pred_s: 0.709, L_pred_y: 0.680, L_adv: -0.686, s_acc: 0.580
Epoch: [280/300]
Train - L_pred_s: 0.684, L_pred_y: 0.785, L_adv: -2.739, s_acc: 0.567
Test - L_pred_s: 0.714, L_pred_y: 0.681, L_adv: -0.685, s_acc: 0.584
Epoch: [290/300]
Train - L_pred_s: 0.684, L_pred_y: 0.788, L_adv: -2.739, s_acc: 0.566
Test - L_pred_s: 0.713, L_pred_y: 0.680, L_adv: -0.685, s_acc: 0.582
Epoch: [300/300]
Train - L_pred_s: 0.684, L_pred_y: 0.790, L_adv: -2.737, s_acc: 0.564
Test - L_pred_s: 0.712, L_pred_y: 0.680, L_adv: -0.685, s_acc: 0.585
Epoch: [10/300]
Train - L_pred_s: 0.686, L_pred_y: 0.933, L_adv: -2.742, s_acc: 0.563
Test - L_pred_s: 0.894, L_pred_y: 0.686, L_adv: -0.686, s_acc: 0.557
Epoch: [20/300]
Train - L_pred_s: 0.684, L_pred_y: 0.876, L_adv: -2.739, s_acc: 0.564
Test - L_pred_s: 0.871, L_pred_y: 0.686, L_adv: -0.686, s_acc: 0.557
Epoch: [30/300]
Train - L_pred_s: 0.684, L_pred_y: 0.870, L_adv: -2.738, s_acc: 0.564
Test - L_pred_s: 0.850, L_pred_y: 0.685, L_adv: -0.685, s_acc: 0.557
Epoch: [40/300]
Train - L_pred_s: 0.681, L_pred_y: 0.855, L_adv: -2.729, s_acc: 0.567
Test - L_pred_s: 0.852, L_pred_y: 0.681, L_adv: -0.681, s_acc: 0.559
Epoch: [50/300]
Train - L_pred_s: 0.684, L_pred_y: 0.852, L_adv: -2.739, s_acc: 0.564
Test - L_pred_s: 0.841, L_pred_y: 0.683, L_adv: -0.683, s_acc: 0.565
Epoch: [60/300]
Train - L_pred_s: 0.682, L_pred_y: 0.852, L_adv: -2.733, s_acc: 0.568
Test - L_pred_s: 0.860, L_pred_y: 0.681, L_adv: -0.681, s_acc: 0.575
Epoch: [70/300]
Train - L_pred_s: 0.685, L_pred_y: 0.933, L_adv: -2.739, s_acc: 0.564
Test - L_pred_s: 0.899, L_pred_y: 0.686, L_adv: -0.686, s_acc: 0.557
Epoch: [80/300]
Train - L_pred_s: 0.683, L_pred_y: 0.857, L_adv: -2.735, s_acc: 0.564
Test - L_pred_s: 0.854, L_pred_y: 0.683, L_adv: -0.683, s_acc: 0.557
Epoch: [90/300]
Train - L_pred_s: 0.684, L_pred_y: 0.853, L_adv: -2.737, s_acc: 0.564
Test - L_pred_s: 0.852, L_pred_y: 0.684, L_adv: -0.684, s_acc: 0.557
Epoch: [100/300]
Train - L_pred_s: 0.684, L_pred_y: 0.853, L_adv: -2.745, s_acc: 0.564
Test - L_pred_s: 0.852, L_pred_y: 0.685, L_adv: -0.685, s_acc: 0.557
Epoch: [110/300]
Train - L_pred_s: 0.684, L_pred_y: 0.848, L_adv: -2.738, s_acc: 0.564
Test - L_pred_s: 0.851, L_pred_y: 0.685, L_adv: -0.685, s_acc: 0.557
Epoch: [120/300]
Train - L_pred_s: 0.684, L_pred_y: 0.848, L_adv: -2.734, s_acc: 0.564
Test - L_pred_s: 0.847, L_pred_y: 0.686, L_adv: -0.686, s_acc: 0.557
Epoch: [130/300]
Train - L_pred_s: 0.684, L_pred_y: 0.842, L_adv: -2.735, s_acc: 0.564
Test - L_pred_s: 0.849, L_pred_y: 0.687, L_adv: -0.687, s_acc: 0.557
Epoch: [140/300]
Train - L_pred_s: 0.684, L_pred_y: 0.841, L_adv: -2.735, s_acc: 0.564
Test - L_pred_s: 0.846, L_pred_y: 0.686, L_adv: -0.686, s_acc: 0.557
Epoch: [150/300]
Train - L_pred_s: 0.684, L_pred_y: 0.833, L_adv: -2.732, s_acc: 0.564
Test - L_pred_s: 0.846, L_pred_y: 0.686, L_adv: -0.686, s_acc: 0.557
Epoch: [160/300]
Train - L_pred_s: 0.683, L_pred_y: 0.837, L_adv: -2.733, s_acc: 0.564
Test - L_pred_s: 0.850, L_pred_y: 0.686, L_adv: -0.686, s_acc: 0.557
Epoch: [170/300]
Train - L_pred_s: 0.683, L_pred_y: 0.834, L_adv: -2.732, s_acc: 0.564
Test - L_pred_s: 0.844, L_pred_y: 0.686, L_adv: -0.686, s_acc: 0.557
Epoch: [180/300]
Train - L_pred_s: 0.684, L_pred_y: 0.834, L_adv: -2.735, s_acc: 0.564
Test - L_pred_s: 0.847, L_pred_y: 0.686, L_adv: -0.686, s_acc: 0.557
Epoch: [190/300]
Train - L_pred_s: 0.683, L_pred_y: 0.830, L_adv: -2.734, s_acc: 0.564
Test - L_pred_s: 0.851, L_pred_y: 0.685, L_adv: -0.685, s_acc: 0.557
Epoch: [200/300]
Train - L_pred_s: 0.682, L_pred_y: 0.829, L_adv: -2.732, s_acc: 0.566
Test - L_pred_s: 0.846, L_pred_y: 0.685, L_adv: -0.685, s_acc: 0.557
Epoch: [210/300]
Train - L_pred_s: 0.682, L_pred_y: 0.831, L_adv: -2.726, s_acc: 0.569
Test - L_pred_s: 0.843, L_pred_y: 0.683, L_adv: -0.683, s_acc: 0.575
Epoch: [220/300]
Train - L_pred_s: 0.682, L_pred_y: 0.826, L_adv: -2.727, s_acc: 0.571
Test - L_pred_s: 0.851, L_pred_y: 0.682, L_adv: -0.682, s_acc: 0.572
Epoch: [230/300]
Train - L_pred_s: 0.682, L_pred_y: 0.825, L_adv: -2.732, s_acc: 0.570
Test - L_pred_s: 0.856, L_pred_y: 0.685, L_adv: -0.685, s_acc: 0.559
Epoch: [240/300]
Train - L_pred_s: 0.683, L_pred_y: 0.822, L_adv: -2.731, s_acc: 0.565
Test - L_pred_s: 0.859, L_pred_y: 0.685, L_adv: -0.685, s_acc: 0.561
Epoch: [250/300]
Train - L_pred_s: 0.683, L_pred_y: 0.825, L_adv: -2.732, s_acc: 0.565
Test - L_pred_s: 0.862, L_pred_y: 0.684, L_adv: -0.684, s_acc: 0.562
Epoch: [260/300]
Train - L_pred_s: 0.683, L_pred_y: 0.824, L_adv: -2.732, s_acc: 0.568
Test - L_pred_s: 0.866, L_pred_y: 0.684, L_adv: -0.684, s_acc: 0.563
Epoch: [270/300]
Train - L_pred_s: 0.683, L_pred_y: 0.822, L_adv: -2.731, s_acc: 0.564
Test - L_pred_s: 0.862, L_pred_y: 0.684, L_adv: -0.684, s_acc: 0.562
Epoch: [280/300]
Train - L_pred_s: 0.683, L_pred_y: 0.822, L_adv: -2.732, s_acc: 0.565
Test - L_pred_s: 0.865, L_pred_y: 0.684, L_adv: -0.684, s_acc: 0.563
Epoch: [290/300]
Train - L_pred_s: 0.683, L_pred_y: 0.822, L_adv: -2.730, s_acc: 0.565
Test - L_pred_s: 0.866, L_pred_y: 0.684, L_adv: -0.684, s_acc: 0.564
Epoch: [300/300]
Train - L_pred_s: 0.683, L_pred_y: 0.822, L_adv: -2.730, s_acc: 0.566
Test - L_pred_s: 0.866, L_pred_y: 0.684, L_adv: -0.684, s_acc: 0.562
Epoch: [10/300]
Train - L_pred_s: 0.686, L_pred_y: 0.928, L_adv: -2.743, s_acc: 0.564
Test - L_pred_s: 0.885, L_pred_y: 0.684, L_adv: -0.684, s_acc: 0.565
Epoch: [20/300]
Train - L_pred_s: 0.685, L_pred_y: 0.876, L_adv: -2.738, s_acc: 0.564
Test - L_pred_s: 0.875, L_pred_y: 0.684, L_adv: -0.684, s_acc: 0.565
Epoch: [30/300]
Train - L_pred_s: 0.682, L_pred_y: 0.865, L_adv: -2.727, s_acc: 0.564
Test - L_pred_s: 0.877, L_pred_y: 0.683, L_adv: -0.683, s_acc: 0.570
Epoch: [40/300]
Train - L_pred_s: 0.681, L_pred_y: 0.874, L_adv: -2.745, s_acc: 0.565
Test - L_pred_s: 0.880, L_pred_y: 0.684, L_adv: -0.684, s_acc: 0.568
Epoch: [50/300]
Train - L_pred_s: 0.685, L_pred_y: 0.872, L_adv: -2.742, s_acc: 0.564
Test - L_pred_s: 0.875, L_pred_y: 0.685, L_adv: -0.685, s_acc: 0.565
Epoch: [60/300]
Train - L_pred_s: 0.685, L_pred_y: 0.861, L_adv: -2.740, s_acc: 0.564
Test - L_pred_s: 0.865, L_pred_y: 0.685, L_adv: -0.685, s_acc: 0.565
Epoch: [70/300]
Train - L_pred_s: 0.685, L_pred_y: 0.852, L_adv: -2.739, s_acc: 0.564
Test - L_pred_s: 0.849, L_pred_y: 0.684, L_adv: -0.684, s_acc: 0.565
Epoch: [80/300]
Train - L_pred_s: 0.685, L_pred_y: 0.839, L_adv: -2.740, s_acc: 0.564
Test - L_pred_s: 0.828, L_pred_y: 0.685, L_adv: -0.685, s_acc: 0.565
Epoch: [90/300]
Train - L_pred_s: 0.684, L_pred_y: 0.831, L_adv: -2.738, s_acc: 0.564
Test - L_pred_s: 0.829, L_pred_y: 0.685, L_adv: -0.685, s_acc: 0.565
Epoch: [100/300]
Train - L_pred_s: 0.684, L_pred_y: 0.812, L_adv: -2.736, s_acc: 0.564
Test - L_pred_s: 0.805, L_pred_y: 0.685, L_adv: -0.685, s_acc: 0.565
Epoch: [110/300]
Train - L_pred_s: 0.684, L_pred_y: 0.801, L_adv: -2.736, s_acc: 0.564
Test - L_pred_s: 0.805, L_pred_y: 0.685, L_adv: -0.685, s_acc: 0.565
Epoch: [120/300]
Train - L_pred_s: 0.683, L_pred_y: 0.793, L_adv: -2.733, s_acc: 0.568
Test - L_pred_s: 0.785, L_pred_y: 0.684, L_adv: -0.684, s_acc: 0.564
Epoch: [130/300]
Train - L_pred_s: 0.683, L_pred_y: 0.781, L_adv: -2.730, s_acc: 0.570
Test - L_pred_s: 0.771, L_pred_y: 0.684, L_adv: -0.684, s_acc: 0.567
Epoch: [140/300]
Train - L_pred_s: 0.683, L_pred_y: 0.777, L_adv: -2.732, s_acc: 0.570
Test - L_pred_s: 0.772, L_pred_y: 0.684, L_adv: -0.684, s_acc: 0.564
Epoch: [150/300]
Train - L_pred_s: 0.682, L_pred_y: 0.779, L_adv: -2.729, s_acc: 0.573
Test - L_pred_s: 0.766, L_pred_y: 0.684, L_adv: -0.684, s_acc: 0.566
Epoch: [160/300]
Train - L_pred_s: 0.683, L_pred_y: 0.775, L_adv: -2.733, s_acc: 0.569
Test - L_pred_s: 0.769, L_pred_y: 0.685, L_adv: -0.685, s_acc: 0.563
Epoch: [170/300]
Train - L_pred_s: 0.683, L_pred_y: 0.775, L_adv: -2.729, s_acc: 0.572
Test - L_pred_s: 0.767, L_pred_y: 0.684, L_adv: -0.684, s_acc: 0.567
Epoch: [180/300]
Train - L_pred_s: 0.683, L_pred_y: 0.770, L_adv: -2.733, s_acc: 0.569
Test - L_pred_s: 0.760, L_pred_y: 0.684, L_adv: -0.684, s_acc: 0.563
Epoch: [190/300]
Train - L_pred_s: 0.683, L_pred_y: 0.770, L_adv: -2.732, s_acc: 0.571
Test - L_pred_s: 0.761, L_pred_y: 0.684, L_adv: -0.684, s_acc: 0.562
Epoch: [200/300]
Train - L_pred_s: 0.684, L_pred_y: 0.776, L_adv: -2.734, s_acc: 0.568
Test - L_pred_s: 0.759, L_pred_y: 0.685, L_adv: -0.685, s_acc: 0.563
Epoch: [210/300]
Train - L_pred_s: 0.683, L_pred_y: 0.767, L_adv: -2.731, s_acc: 0.570
Test - L_pred_s: 0.762, L_pred_y: 0.685, L_adv: -0.685, s_acc: 0.562
Epoch: [220/300]
Train - L_pred_s: 0.683, L_pred_y: 0.767, L_adv: -2.733, s_acc: 0.569
Test - L_pred_s: 0.759, L_pred_y: 0.685, L_adv: -0.685, s_acc: 0.563
Epoch: [230/300]
Train - L_pred_s: 0.683, L_pred_y: 0.772, L_adv: -2.733, s_acc: 0.568
Test - L_pred_s: 0.760, L_pred_y: 0.685, L_adv: -0.685, s_acc: 0.564
Epoch: [240/300]
Train - L_pred_s: 0.683, L_pred_y: 0.769, L_adv: -2.731, s_acc: 0.571
Test - L_pred_s: 0.760, L_pred_y: 0.684, L_adv: -0.684, s_acc: 0.564
Epoch: [250/300]
Train - L_pred_s: 0.683, L_pred_y: 0.770, L_adv: -2.731, s_acc: 0.571
Test - L_pred_s: 0.759, L_pred_y: 0.684, L_adv: -0.684, s_acc: 0.565
Epoch: [260/300]
Train - L_pred_s: 0.683, L_pred_y: 0.770, L_adv: -2.733, s_acc: 0.568
Test - L_pred_s: 0.757, L_pred_y: 0.684, L_adv: -0.684, s_acc: 0.562
Epoch: [270/300]
Train - L_pred_s: 0.683, L_pred_y: 0.769, L_adv: -2.733, s_acc: 0.569
Test - L_pred_s: 0.758, L_pred_y: 0.684, L_adv: -0.684, s_acc: 0.563
Epoch: [280/300]
Train - L_pred_s: 0.683, L_pred_y: 0.769, L_adv: -2.733, s_acc: 0.568
Test - L_pred_s: 0.759, L_pred_y: 0.684, L_adv: -0.684, s_acc: 0.565
Epoch: [290/300]
Train - L_pred_s: 0.683, L_pred_y: 0.770, L_adv: -2.732, s_acc: 0.572
Test - L_pred_s: 0.758, L_pred_y: 0.684, L_adv: -0.684, s_acc: 0.564
Epoch: [300/300]
Train - L_pred_s: 0.683, L_pred_y: 0.768, L_adv: -2.732, s_acc: 0.570
Test - L_pred_s: 0.758, L_pred_y: 0.684, L_adv: -0.684, s_acc: 0.564
Epoch: [10/300]
Train - L_pred_s: 0.685, L_pred_y: 0.894, L_adv: -2.741, s_acc: 0.564
Test - L_pred_s: 0.874, L_pred_y: 0.689, L_adv: -0.689, s_acc: 0.548
Epoch: [20/300]
Train - L_pred_s: 0.685, L_pred_y: 0.879, L_adv: -2.739, s_acc: 0.564
Test - L_pred_s: 0.867, L_pred_y: 0.689, L_adv: -0.689, s_acc: 0.548
Epoch: [30/300]
Train - L_pred_s: 0.685, L_pred_y: 0.876, L_adv: -2.739, s_acc: 0.564
Test - L_pred_s: 0.870, L_pred_y: 0.689, L_adv: -0.689, s_acc: 0.548
Epoch: [40/300]
Train - L_pred_s: 0.684, L_pred_y: 0.874, L_adv: -2.740, s_acc: 0.564
Test - L_pred_s: 0.865, L_pred_y: 0.688, L_adv: -0.688, s_acc: 0.548
Epoch: [50/300]
Train - L_pred_s: 0.684, L_pred_y: 0.868, L_adv: -2.738, s_acc: 0.564
Test - L_pred_s: 0.863, L_pred_y: 0.688, L_adv: -0.688, s_acc: 0.548
Epoch: [60/300]
Train - L_pred_s: 0.684, L_pred_y: 0.857, L_adv: -2.739, s_acc: 0.564
Test - L_pred_s: 0.848, L_pred_y: 0.688, L_adv: -0.688, s_acc: 0.548
Epoch: [70/300]
Train - L_pred_s: 0.684, L_pred_y: 0.873, L_adv: -2.736, s_acc: 0.564
Test - L_pred_s: 0.864, L_pred_y: 0.688, L_adv: -0.688, s_acc: 0.548
Epoch: [80/300]
Train - L_pred_s: 0.685, L_pred_y: 0.865, L_adv: -2.741, s_acc: 0.564
Test - L_pred_s: 0.869, L_pred_y: 0.688, L_adv: -0.688, s_acc: 0.548
Epoch: [90/300]
Train - L_pred_s: 0.685, L_pred_y: 0.853, L_adv: -2.739, s_acc: 0.564
Test - L_pred_s: 0.858, L_pred_y: 0.688, L_adv: -0.688, s_acc: 0.548
Epoch: [100/300]
Train - L_pred_s: 0.685, L_pred_y: 0.882, L_adv: -2.740, s_acc: 0.564
Test - L_pred_s: 0.902, L_pred_y: 0.688, L_adv: -0.688, s_acc: 0.548
Epoch: [110/300]
Train - L_pred_s: 0.684, L_pred_y: 0.836, L_adv: -2.738, s_acc: 0.564
Test - L_pred_s: 0.860, L_pred_y: 0.688, L_adv: -0.688, s_acc: 0.548
Epoch: [120/300]
Train - L_pred_s: 0.684, L_pred_y: 0.869, L_adv: -2.737, s_acc: 0.564
Test - L_pred_s: 0.865, L_pred_y: 0.687, L_adv: -0.687, s_acc: 0.548
Epoch: [130/300]
Train - L_pred_s: 0.683, L_pred_y: 0.845, L_adv: -2.734, s_acc: 0.567
Test - L_pred_s: 0.848, L_pred_y: 0.685, L_adv: -0.685, s_acc: 0.561
Epoch: [140/300]
Train - L_pred_s: 0.682, L_pred_y: 0.824, L_adv: -2.728, s_acc: 0.570
Test - L_pred_s: 0.876, L_pred_y: 0.684, L_adv: -0.684, s_acc: 0.568
Epoch: [150/300]
Train - L_pred_s: 0.682, L_pred_y: 0.819, L_adv: -2.730, s_acc: 0.570
Test - L_pred_s: 0.845, L_pred_y: 0.683, L_adv: -0.683, s_acc: 0.572
Epoch: [160/300]
Train - L_pred_s: 0.683, L_pred_y: 0.799, L_adv: -2.730, s_acc: 0.571
Test - L_pred_s: 0.816, L_pred_y: 0.684, L_adv: -0.684, s_acc: 0.572
Epoch: [170/300]
Train - L_pred_s: 0.682, L_pred_y: 0.785, L_adv: -2.730, s_acc: 0.571
Test - L_pred_s: 0.814, L_pred_y: 0.683, L_adv: -0.683, s_acc: 0.569
Epoch: [180/300]
Train - L_pred_s: 0.682, L_pred_y: 0.779, L_adv: -2.727, s_acc: 0.571
Test - L_pred_s: 0.786, L_pred_y: 0.684, L_adv: -0.684, s_acc: 0.564
Epoch: [190/300]
Train - L_pred_s: 0.682, L_pred_y: 0.798, L_adv: -2.732, s_acc: 0.569
Test - L_pred_s: 0.807, L_pred_y: 0.685, L_adv: -0.685, s_acc: 0.563
Epoch: [200/300]
Train - L_pred_s: 0.683, L_pred_y: 0.791, L_adv: -2.729, s_acc: 0.570
Test - L_pred_s: 0.790, L_pred_y: 0.684, L_adv: -0.684, s_acc: 0.567
Epoch: [210/300]
Train - L_pred_s: 0.682, L_pred_y: 0.776, L_adv: -2.731, s_acc: 0.573
Test - L_pred_s: 0.802, L_pred_y: 0.684, L_adv: -0.684, s_acc: 0.567
Epoch: [220/300]
Train - L_pred_s: 0.683, L_pred_y: 0.772, L_adv: -2.730, s_acc: 0.571
Test - L_pred_s: 0.787, L_pred_y: 0.684, L_adv: -0.684, s_acc: 0.565
Epoch: [230/300]
Train - L_pred_s: 0.683, L_pred_y: 0.774, L_adv: -2.729, s_acc: 0.571
Test - L_pred_s: 0.787, L_pred_y: 0.684, L_adv: -0.684, s_acc: 0.567
Epoch: [240/300]
Train - L_pred_s: 0.682, L_pred_y: 0.771, L_adv: -2.729, s_acc: 0.571
Test - L_pred_s: 0.781, L_pred_y: 0.685, L_adv: -0.685, s_acc: 0.561
Epoch: [250/300]
Train - L_pred_s: 0.682, L_pred_y: 0.771, L_adv: -2.731, s_acc: 0.573
Test - L_pred_s: 0.783, L_pred_y: 0.684, L_adv: -0.684, s_acc: 0.566
Epoch: [260/300]
Train - L_pred_s: 0.683, L_pred_y: 0.769, L_adv: -2.731, s_acc: 0.570
Test - L_pred_s: 0.784, L_pred_y: 0.685, L_adv: -0.685, s_acc: 0.564
Epoch: [270/300]
Train - L_pred_s: 0.683, L_pred_y: 0.769, L_adv: -2.733, s_acc: 0.570
Test - L_pred_s: 0.781, L_pred_y: 0.685, L_adv: -0.685, s_acc: 0.561
Epoch: [280/300]
Train - L_pred_s: 0.683, L_pred_y: 0.770, L_adv: -2.732, s_acc: 0.569
Test - L_pred_s: 0.781, L_pred_y: 0.685, L_adv: -0.685, s_acc: 0.561
Epoch: [290/300]
Train - L_pred_s: 0.683, L_pred_y: 0.773, L_adv: -2.732, s_acc: 0.569
Test - L_pred_s: 0.782, L_pred_y: 0.685, L_adv: -0.685, s_acc: 0.562
Epoch: [300/300]
Train - L_pred_s: 0.683, L_pred_y: 0.771, L_adv: -2.732, s_acc: 0.568
Test - L_pred_s: 0.782, L_pred_y: 0.685, L_adv: -0.685, s_acc: 0.563
Epoch: [10/300]
Train - L_pred_s: 0.687, L_pred_y: 0.880, L_adv: -2.749, s_acc: 0.556
Test - L_pred_s: 0.905, L_pred_y: 0.684, L_adv: -0.684, s_acc: 0.566
Epoch: [20/300]
Train - L_pred_s: 0.685, L_pred_y: 0.868, L_adv: -2.742, s_acc: 0.561
Test - L_pred_s: 0.885, L_pred_y: 0.683, L_adv: -0.683, s_acc: 0.566
Epoch: [30/300]
Train - L_pred_s: 0.685, L_pred_y: 0.855, L_adv: -2.741, s_acc: 0.561
Test - L_pred_s: 0.883, L_pred_y: 0.682, L_adv: -0.682, s_acc: 0.566
Epoch: [40/300]
Train - L_pred_s: 0.684, L_pred_y: 0.838, L_adv: -2.737, s_acc: 0.563
Test - L_pred_s: 0.863, L_pred_y: 0.682, L_adv: -0.682, s_acc: 0.566
Epoch: [50/300]
Train - L_pred_s: 0.684, L_pred_y: 0.821, L_adv: -2.739, s_acc: 0.564
Test - L_pred_s: 0.825, L_pred_y: 0.682, L_adv: -0.682, s_acc: 0.568
Epoch: [60/300]
Train - L_pred_s: 0.684, L_pred_y: 0.796, L_adv: -2.735, s_acc: 0.565
Test - L_pred_s: 0.802, L_pred_y: 0.681, L_adv: -0.681, s_acc: 0.577
Epoch: [70/300]
Train - L_pred_s: 0.684, L_pred_y: 0.778, L_adv: -2.734, s_acc: 0.566
Test - L_pred_s: 0.805, L_pred_y: 0.681, L_adv: -0.681, s_acc: 0.578
Epoch: [80/300]
Train - L_pred_s: 0.684, L_pred_y: 0.770, L_adv: -2.737, s_acc: 0.566
Test - L_pred_s: 0.800, L_pred_y: 0.681, L_adv: -0.681, s_acc: 0.580
Epoch: [90/300]
Train - L_pred_s: 0.684, L_pred_y: 0.772, L_adv: -2.737, s_acc: 0.568
Test - L_pred_s: 0.804, L_pred_y: 0.682, L_adv: -0.682, s_acc: 0.572
Epoch: [100/300]
Train - L_pred_s: 0.684, L_pred_y: 0.768, L_adv: -2.735, s_acc: 0.567
Test - L_pred_s: 0.801, L_pred_y: 0.682, L_adv: -0.682, s_acc: 0.579
Epoch: [110/300]
Train - L_pred_s: 0.684, L_pred_y: 0.766, L_adv: -2.737, s_acc: 0.566
Test - L_pred_s: 0.795, L_pred_y: 0.682, L_adv: -0.682, s_acc: 0.574
Epoch: [120/300]
Train - L_pred_s: 0.684, L_pred_y: 0.764, L_adv: -2.736, s_acc: 0.565
Test - L_pred_s: 0.799, L_pred_y: 0.682, L_adv: -0.682, s_acc: 0.576
Epoch: [130/300]
Train - L_pred_s: 0.684, L_pred_y: 0.767, L_adv: -2.736, s_acc: 0.565
Test - L_pred_s: 0.791, L_pred_y: 0.682, L_adv: -0.682, s_acc: 0.576
Epoch: [140/300]
Train - L_pred_s: 0.684, L_pred_y: 0.763, L_adv: -2.737, s_acc: 0.566
Test - L_pred_s: 0.792, L_pred_y: 0.682, L_adv: -0.682, s_acc: 0.575
Epoch: [150/300]
Train - L_pred_s: 0.683, L_pred_y: 0.763, L_adv: -2.735, s_acc: 0.567
Test - L_pred_s: 0.795, L_pred_y: 0.682, L_adv: -0.682, s_acc: 0.578
Epoch: [160/300]
Train - L_pred_s: 0.684, L_pred_y: 0.761, L_adv: -2.737, s_acc: 0.564
Test - L_pred_s: 0.792, L_pred_y: 0.682, L_adv: -0.682, s_acc: 0.575
Epoch: [170/300]
Train - L_pred_s: 0.685, L_pred_y: 0.761, L_adv: -2.738, s_acc: 0.563
Test - L_pred_s: 0.791, L_pred_y: 0.682, L_adv: -0.682, s_acc: 0.575
Epoch: [180/300]
Train - L_pred_s: 0.685, L_pred_y: 0.760, L_adv: -2.738, s_acc: 0.564
Test - L_pred_s: 0.791, L_pred_y: 0.682, L_adv: -0.682, s_acc: 0.578
Epoch: [190/300]
Train - L_pred_s: 0.684, L_pred_y: 0.761, L_adv: -2.737, s_acc: 0.566
Test - L_pred_s: 0.791, L_pred_y: 0.682, L_adv: -0.682, s_acc: 0.577
Epoch: [200/300]
Train - L_pred_s: 0.685, L_pred_y: 0.761, L_adv: -2.738, s_acc: 0.564
Test - L_pred_s: 0.790, L_pred_y: 0.682, L_adv: -0.682, s_acc: 0.571
Epoch: [210/300]
Train - L_pred_s: 0.684, L_pred_y: 0.759, L_adv: -2.737, s_acc: 0.565
Test - L_pred_s: 0.791, L_pred_y: 0.682, L_adv: -0.682, s_acc: 0.577
Epoch: [220/300]
Train - L_pred_s: 0.684, L_pred_y: 0.762, L_adv: -2.738, s_acc: 0.564
Test - L_pred_s: 0.793, L_pred_y: 0.683, L_adv: -0.683, s_acc: 0.570
Epoch: [230/300]
Train - L_pred_s: 0.684, L_pred_y: 0.761, L_adv: -2.736, s_acc: 0.564
Test - L_pred_s: 0.791, L_pred_y: 0.682, L_adv: -0.682, s_acc: 0.578
Epoch: [240/300]
Train - L_pred_s: 0.684, L_pred_y: 0.761, L_adv: -2.736, s_acc: 0.566
Test - L_pred_s: 0.792, L_pred_y: 0.682, L_adv: -0.682, s_acc: 0.574
Epoch: [250/300]
Train - L_pred_s: 0.684, L_pred_y: 0.762, L_adv: -2.738, s_acc: 0.564
Test - L_pred_s: 0.791, L_pred_y: 0.683, L_adv: -0.683, s_acc: 0.575
Epoch: [260/300]
Train - L_pred_s: 0.684, L_pred_y: 0.758, L_adv: -2.737, s_acc: 0.564
Test - L_pred_s: 0.791, L_pred_y: 0.682, L_adv: -0.682, s_acc: 0.575
Epoch: [270/300]
Train - L_pred_s: 0.684, L_pred_y: 0.759, L_adv: -2.738, s_acc: 0.564
Test - L_pred_s: 0.791, L_pred_y: 0.683, L_adv: -0.683, s_acc: 0.571
Epoch: [280/300]
Train - L_pred_s: 0.684, L_pred_y: 0.762, L_adv: -2.739, s_acc: 0.564
Test - L_pred_s: 0.791, L_pred_y: 0.683, L_adv: -0.683, s_acc: 0.573
Epoch: [290/300]
Train - L_pred_s: 0.684, L_pred_y: 0.758, L_adv: -2.738, s_acc: 0.564
Test - L_pred_s: 0.790, L_pred_y: 0.682, L_adv: -0.682, s_acc: 0.571
Epoch: [300/300]
Train - L_pred_s: 0.685, L_pred_y: 0.758, L_adv: -2.738, s_acc: 0.563
Test - L_pred_s: 0.791, L_pred_y: 0.682, L_adv: -0.682, s_acc: 0.575
Epoch: [10/300]
Train - L_pred_s: 0.690, L_pred_y: 0.884, L_adv: -2.763, s_acc: 0.530
Test - L_pred_s: 0.812, L_pred_y: 0.685, L_adv: -0.685, s_acc: 0.573
Epoch: [20/300]
Train - L_pred_s: 0.687, L_pred_y: 0.871, L_adv: -2.749, s_acc: 0.557
Test - L_pred_s: 0.807, L_pred_y: 0.682, L_adv: -0.682, s_acc: 0.573
Epoch: [30/300]
Train - L_pred_s: 0.686, L_pred_y: 0.869, L_adv: -2.744, s_acc: 0.558
Test - L_pred_s: 0.801, L_pred_y: 0.685, L_adv: -0.685, s_acc: 0.571
Epoch: [40/300]
Train - L_pred_s: 0.686, L_pred_y: 0.860, L_adv: -2.742, s_acc: 0.560
Test - L_pred_s: 0.790, L_pred_y: 0.683, L_adv: -0.683, s_acc: 0.571
Epoch: [50/300]
Train - L_pred_s: 0.685, L_pred_y: 0.851, L_adv: -2.741, s_acc: 0.560
Test - L_pred_s: 0.776, L_pred_y: 0.683, L_adv: -0.683, s_acc: 0.571
Epoch: [60/300]
Train - L_pred_s: 0.685, L_pred_y: 0.853, L_adv: -2.739, s_acc: 0.560
Test - L_pred_s: 0.789, L_pred_y: 0.683, L_adv: -0.683, s_acc: 0.571
Epoch: [70/300]
Train - L_pred_s: 0.685, L_pred_y: 0.851, L_adv: -2.742, s_acc: 0.560
Test - L_pred_s: 0.788, L_pred_y: 0.682, L_adv: -0.682, s_acc: 0.571
Epoch: [80/300]
Train - L_pred_s: 0.685, L_pred_y: 0.836, L_adv: -2.739, s_acc: 0.561
Test - L_pred_s: 0.771, L_pred_y: 0.682, L_adv: -0.682, s_acc: 0.571
Epoch: [90/300]
Train - L_pred_s: 0.684, L_pred_y: 0.862, L_adv: -2.739, s_acc: 0.564
Test - L_pred_s: 0.759, L_pred_y: 0.680, L_adv: -0.680, s_acc: 0.575
Epoch: [100/300]
Train - L_pred_s: 0.684, L_pred_y: 0.814, L_adv: -2.734, s_acc: 0.566
Test - L_pred_s: 0.735, L_pred_y: 0.680, L_adv: -0.680, s_acc: 0.579
Epoch: [110/300]
Train - L_pred_s: 0.683, L_pred_y: 0.808, L_adv: -2.739, s_acc: 0.567
Test - L_pred_s: 0.749, L_pred_y: 0.681, L_adv: -0.681, s_acc: 0.576
Epoch: [120/300]
Train - L_pred_s: 0.684, L_pred_y: 0.800, L_adv: -2.734, s_acc: 0.567
Test - L_pred_s: 0.725, L_pred_y: 0.680, L_adv: -0.680, s_acc: 0.581
Epoch: [130/300]
Train - L_pred_s: 0.684, L_pred_y: 0.791, L_adv: -2.734, s_acc: 0.568
Test - L_pred_s: 0.732, L_pred_y: 0.680, L_adv: -0.680, s_acc: 0.578
Epoch: [140/300]
Train - L_pred_s: 0.683, L_pred_y: 0.792, L_adv: -2.734, s_acc: 0.567
Test - L_pred_s: 0.723, L_pred_y: 0.681, L_adv: -0.681, s_acc: 0.574
Epoch: [150/300]
Train - L_pred_s: 0.683, L_pred_y: 0.783, L_adv: -2.734, s_acc: 0.565
Test - L_pred_s: 0.728, L_pred_y: 0.680, L_adv: -0.680, s_acc: 0.576
Epoch: [160/300]
Train - L_pred_s: 0.683, L_pred_y: 0.785, L_adv: -2.731, s_acc: 0.565
Test - L_pred_s: 0.716, L_pred_y: 0.680, L_adv: -0.680, s_acc: 0.573
Epoch: [170/300]
Train - L_pred_s: 0.684, L_pred_y: 0.789, L_adv: -2.734, s_acc: 0.565
Test - L_pred_s: 0.718, L_pred_y: 0.680, L_adv: -0.680, s_acc: 0.580
Epoch: [180/300]
Train - L_pred_s: 0.684, L_pred_y: 0.789, L_adv: -2.734, s_acc: 0.566
Test - L_pred_s: 0.722, L_pred_y: 0.681, L_adv: -0.681, s_acc: 0.572
Epoch: [190/300]
Train - L_pred_s: 0.684, L_pred_y: 0.785, L_adv: -2.735, s_acc: 0.566
Test - L_pred_s: 0.722, L_pred_y: 0.680, L_adv: -0.680, s_acc: 0.577
Epoch: [200/300]
Train - L_pred_s: 0.684, L_pred_y: 0.784, L_adv: -2.733, s_acc: 0.569
Test - L_pred_s: 0.723, L_pred_y: 0.680, L_adv: -0.680, s_acc: 0.581
Epoch: [210/300]
Train - L_pred_s: 0.681, L_pred_y: 0.784, L_adv: -2.735, s_acc: 0.568
Test - L_pred_s: 0.720, L_pred_y: 0.680, L_adv: -0.680, s_acc: 0.578
Epoch: [220/300]
Train - L_pred_s: 0.683, L_pred_y: 0.780, L_adv: -2.733, s_acc: 0.567
Test - L_pred_s: 0.720, L_pred_y: 0.680, L_adv: -0.680, s_acc: 0.575
Epoch: [230/300]
Train - L_pred_s: 0.684, L_pred_y: 0.780, L_adv: -2.734, s_acc: 0.565
Test - L_pred_s: 0.714, L_pred_y: 0.681, L_adv: -0.681, s_acc: 0.575
Epoch: [240/300]
Train - L_pred_s: 0.683, L_pred_y: 0.776, L_adv: -2.734, s_acc: 0.568
Test - L_pred_s: 0.716, L_pred_y: 0.681, L_adv: -0.681, s_acc: 0.577
Epoch: [250/300]
Train - L_pred_s: 0.684, L_pred_y: 0.773, L_adv: -2.734, s_acc: 0.566
Test - L_pred_s: 0.713, L_pred_y: 0.681, L_adv: -0.681, s_acc: 0.579
Epoch: [260/300]
Train - L_pred_s: 0.683, L_pred_y: 0.780, L_adv: -2.733, s_acc: 0.569
Test - L_pred_s: 0.715, L_pred_y: 0.681, L_adv: -0.681, s_acc: 0.581
Epoch: [270/300]
Train - L_pred_s: 0.684, L_pred_y: 0.779, L_adv: -2.732, s_acc: 0.566
Test - L_pred_s: 0.711, L_pred_y: 0.681, L_adv: -0.681, s_acc: 0.579
Epoch: [280/300]
Train - L_pred_s: 0.683, L_pred_y: 0.773, L_adv: -2.733, s_acc: 0.565
Test - L_pred_s: 0.714, L_pred_y: 0.681, L_adv: -0.681, s_acc: 0.575
Epoch: [290/300]
Train - L_pred_s: 0.684, L_pred_y: 0.775, L_adv: -2.733, s_acc: 0.566
Test - L_pred_s: 0.713, L_pred_y: 0.681, L_adv: -0.681, s_acc: 0.576
Epoch: [300/300]
Train - L_pred_s: 0.683, L_pred_y: 0.776, L_adv: -2.734, s_acc: 0.566
Test - L_pred_s: 0.712, L_pred_y: 0.681, L_adv: -0.681, s_acc: 0.576
FarconVAE-t
mse = 0.8058248604928 + 0.027710136220568067, te = 0.7142339944839478 + 0.400793194770813, s_acc = 0.6208256880733944 + 0.400793194770813
FarconVAE-G
mse = 0.7866085832479558 + 0.02199122490992489, te = 0.9833999872207642 + 0.34605419635772705, s_acc = 0.7101834862385321 + 0.34605419635772705
MaxEnt-ARL
mse = 0.7553170163527263 + 0.02524220840298837, te = 5.014281272888184 + 4.897889614105225, s_acc = 0.5677064220183486 + 4.897889614105225
CI
mse = 0.7553554555125197 + 0.026315016106862772, te = 1.7139732837677002 + 0.3954184055328369, s_acc = 0.569908256880734 + 0.3954184055328369
UF
mse = 0.7939742335257101 + 0.022219075514861356, te = 0.0263324231943933 + 0.0020053985239918354, s_acc = 1.0 + 0.0020053985239918354
ours
mse = 0.7942616371615072 + 0.022311971198141614, te = 0.0 + 0.0, s_acc = 0.5761467889908257 + 0.0
File Found: Loading Fitted Inference Model...
Traceback (most recent call last):
  File "cf_generation.py", line 232, in <module>
    main()
  File "cf_generation.py", line 190, in main
    law_train_valid_dic_final, law_test_dic_final = abduction_preprocess(law_train_valid_dic, 
  File "cf_generation.py", line 106, in abduction_preprocess
    law_train_dic_final = get_abduction_dict(law_train_dic, parameter_dic)
  File "cf_generation.py", line 61, in get_abduction_dict
    for key in post_samps.keys():
AttributeError: 'NoneType' object has no attribute 'keys'
model code =
 data {
    int<lower = 0> N; // number of observation
    int<lower = 0> K; // number of covariates
    matrix[N, K]   a; // sensitive attributes
    array[N] real  ugpa; // UGPA
    array[N] real   lsat; //LSAT
    array[N] real  zfya; // ZFYA
}

transformed data {
    vector[K] zero_K;
    vector[K] one_K;

    zero_K = rep_vector(0, K);
    one_K = rep_vector(1, K);
}

parameters {
    vector[N] u;

    real ugpa0;
    real eta_u_ugpa;
    real lsat0;
    real eta_u_lsat;
    real eta_u_zfya;

    vector[K] eta_a_ugpa;
    vector[K] eta_a_lsat;
    vector[K] eta_a_zfya;

    real<lower = 0> sigma_g_Sq_gpa;
}

model {
    u ~ uniform(-2, 2);

    ugpa0 ~ normal(0, 1);
    eta_u_ugpa ~ normal(0, 1);
    lsat0 ~ normal(0, 1);
    eta_u_lsat ~ normal(0, 1);
    eta_u_zfya ~ normal(0, 1);

    eta_a_ugpa ~ normal(zero_K, one_K);
    eta_a_lsat ~ normal(zero_K, one_K);
    eta_a_zfya ~ normal(zero_K, one_K);

    //ugpa ~ normal(ugpa0 + eta_u_ugpa * u + a * eta_a_ugpa, 0.00001);
    //lsat ~ normal(exp(lsat0 + eta_u_lsat * u + a * eta_a_lsat), 0.00001);
    //zfya ~ normal(eta_u_zfya * u + a * eta_a_zfya, 0.00001);
    ugpa ~ normal(ugpa0 + eta_u_ugpa * u + a * eta_a_ugpa, 0.1);
    lsat ~ normal(exp(lsat0 + eta_u_lsat * u + a * eta_a_lsat), 0.1);
    zfya ~ normal(eta_u_zfya * u + a * eta_a_zfya, 0.1);
}
Building...

Building: found in cache, done.Messages from stanc:
Warning in '/tmp/httpstan_1gnn9d5y/model_diqii4mv.stan', line 35, column 4: Parameter
    u is given a uniform distribution. The uniform distribution is not
    recommended, for two reasons: (a) Except when there are logical or
    physical constraints, it is very unusual for you to be sure that a
    parameter will fall inside a specified range, and (b) The infinite
    gradient induced by a uniform density can cause difficulties for Stan's
    sampling algorithm. As a consequence, we recommend soft constraints
    rather than hard constraints; for example, instead of giving an
    elasticity parameter a uniform(0,1) distribution, try normal(0.5,0.5).
Warning: The parameter sigma_g_Sq_gpa was declared but was not used in the
    density calculation.
Warning: The parameter eta_a_zfya has no priors. This means either no prior
    is provided, or the prior(s) depend on data variables. In the later case,
    this may be a false positive.
Warning: The parameter eta_a_ugpa has no priors. This means either no prior
    is provided, or the prior(s) depend on data variables. In the later case,
    this may be a false positive.
Warning: The parameter eta_a_lsat has no priors. This means either no prior
    is provided, or the prior(s) depend on data variables. In the later case,
    this may be a false positive.
Sampling:   0%Finished compiling model!

Sampling:   0% (1/8000)
Sampling:   0% (2/8000)
Sampling:   0% (3/8000)
Sampling:   0% (4/8000)
Sampling:   1% (103/8000)
Sampling:   3% (202/8000)
Sampling:   4% (301/8000)
Sampling:   5% (401/8000)
Sampling:   6% (501/8000)model code =
 data {
    int<lower = 0> N; // number of observation
    int<lower = 0> K; // number of covariates
    matrix[N, K]   a; // sensitive attributes
    array[N] real  ugpa; // UGPA
    array[N] real   lsat; //LSAT
    array[N] real  zfya; // ZFYA
}

transformed data {
    vector[K] zero_K;
    vector[K] one_K;

    zero_K = rep_vector(0, K);
    one_K = rep_vector(1, K);
}

parameters {
    vector[N] u;

    real ugpa0;
    real eta_u_ugpa;
    real lsat0;
    real eta_u_lsat;
    real eta_u_zfya;

    vector[K] eta_a_ugpa;
    vector[K] eta_a_lsat;
    vector[K] eta_a_zfya;

    real<lower = 0> sigma_g_Sq_gpa;
}

model {
    u ~ uniform(-2, 2);

    ugpa0 ~ normal(0, 1);
    eta_u_ugpa ~ normal(0, 1);
    lsat0 ~ normal(0, 1);
    eta_u_lsat ~ normal(0, 1);
    eta_u_zfya ~ normal(0, 1);

    eta_a_ugpa ~ normal(zero_K, one_K);
    eta_a_lsat ~ normal(zero_K, one_K);
    eta_a_zfya ~ normal(zero_K, one_K);

    //ugpa ~ normal(ugpa0 + eta_u_ugpa * u + a * eta_a_ugpa, 0.00001);
    //lsat ~ normal(exp(lsat0 + eta_u_lsat * u + a * eta_a_lsat), 0.00001);
    //zfya ~ normal(eta_u_zfya * u + a * eta_a_zfya, 0.00001);
    ugpa ~ normal(ugpa0 + eta_u_ugpa * u + a * eta_a_ugpa, 0.1);
    lsat ~ normal(exp(lsat0 + eta_u_lsat * u + a * eta_a_lsat), 0.1);
    zfya ~ normal(eta_u_zfya * u + a * eta_a_zfya, 0.1);
}
Building...

Building: found in cache, done.Messages from stanc:
Warning in '/tmp/httpstan_1gnn9d5y/model_diqii4mv.stan', line 35, column 4: Parameter
    u is given a uniform distribution. The uniform distribution is not
    recommended, for two reasons: (a) Except when there are logical or
    physical constraints, it is very unusual for you to be sure that a
    parameter will fall inside a specified range, and (b) The infinite
    gradient induced by a uniform density can cause difficulties for Stan's
    sampling algorithm. As a consequence, we recommend soft constraints
    rather than hard constraints; for example, instead of giving an
    elasticity parameter a uniform(0,1) distribution, try normal(0.5,0.5).
Warning: The parameter sigma_g_Sq_gpa was declared but was not used in the
    density calculation.
Warning: The parameter eta_a_zfya has no priors. This means either no prior
    is provided, or the prior(s) depend on data variables. In the later case,
    this may be a false positive.
Warning: The parameter eta_a_ugpa has no priors. This means either no prior
    is provided, or the prior(s) depend on data variables. In the later case,
    this may be a false positive.
Warning: The parameter eta_a_lsat has no priors. This means either no prior
    is provided, or the prior(s) depend on data variables. In the later case,
    this may be a false positive.
Sampling:   0%Finished compiling model!

Sampling:   0% (1/8000)
Sampling:   0% (2/8000)
Sampling:   0% (3/8000)
Sampling:   0% (4/8000)model code =
 data {
    int<lower = 0> N; // number of observation
    int<lower = 0> K; // number of covariates
    matrix[N, K]   a; // sensitive attributes
    array[N] real  ugpa; // UGPA
    array[N] real   lsat; //LSAT
    array[N] real  zfya; // ZFYA
}

transformed data {
    vector[K] zero_K;
    vector[K] one_K;

    zero_K = rep_vector(0, K);
    one_K = rep_vector(1, K);
}

parameters {
    vector[N] u;

    real ugpa0;
    real eta_u_ugpa;
    real lsat0;
    real eta_u_lsat;
    real eta_u_zfya;

    vector[K] eta_a_ugpa;
    vector[K] eta_a_lsat;
    vector[K] eta_a_zfya;

    real<lower = 0> sigma_g_Sq_gpa;
}

model {
    u ~ uniform(-2, 2);

    ugpa0 ~ normal(0, 1);
    eta_u_ugpa ~ normal(0, 1);
    lsat0 ~ normal(0, 1);
    eta_u_lsat ~ normal(0, 1);
    eta_u_zfya ~ normal(0, 1);

    eta_a_ugpa ~ normal(zero_K, one_K);
    eta_a_lsat ~ normal(zero_K, one_K);
    eta_a_zfya ~ normal(zero_K, one_K);

    //ugpa ~ normal(ugpa0 + eta_u_ugpa * u + a * eta_a_ugpa, 0.00001);
    //lsat ~ normal(exp(lsat0 + eta_u_lsat * u + a * eta_a_lsat), 0.00001);
    //zfya ~ normal(eta_u_zfya * u + a * eta_a_zfya, 0.00001);
    ugpa ~ normal(ugpa0 + eta_u_ugpa * u + a * eta_a_ugpa, 0.1);
    lsat ~ normal(exp(lsat0 + eta_u_lsat * u + a * eta_a_lsat), 0.1);
    zfya ~ normal(eta_u_zfya * u + a * eta_a_zfya, 0.1);
}
Building...

Building: found in cache, done.Messages from stanc:
Warning in '/tmp/httpstan_1gnn9d5y/model_diqii4mv.stan', line 35, column 4: Parameter
    u is given a uniform distribution. The uniform distribution is not
    recommended, for two reasons: (a) Except when there are logical or
    physical constraints, it is very unusual for you to be sure that a
    parameter will fall inside a specified range, and (b) The infinite
    gradient induced by a uniform density can cause difficulties for Stan's
    sampling algorithm. As a consequence, we recommend soft constraints
    rather than hard constraints; for example, instead of giving an
    elasticity parameter a uniform(0,1) distribution, try normal(0.5,0.5).
Warning: The parameter sigma_g_Sq_gpa was declared but was not used in the
    density calculation.
Warning: The parameter eta_a_zfya has no priors. This means either no prior
    is provided, or the prior(s) depend on data variables. In the later case,
    this may be a false positive.
Warning: The parameter eta_a_ugpa has no priors. This means either no prior
    is provided, or the prior(s) depend on data variables. In the later case,
    this may be a false positive.
Warning: The parameter eta_a_lsat has no priors. This means either no prior
    is provided, or the prior(s) depend on data variables. In the later case,
    this may be a false positive.
Sampling:   0%Finished compiling model!
model code =
 data {
    int<lower = 0> N; // number of observation
    int<lower = 0> K; // number of covariates
    matrix[N, K]   a; // sensitive attributes
    array[N] real  ugpa; // UGPA
    array[N] real   lsat; //LSAT
    array[N] real  zfya; // ZFYA
}

transformed data {
    vector[K] zero_K;
    vector[K] one_K;

    zero_K = rep_vector(0, K);
    one_K = rep_vector(1, K);
}

parameters {
    vector[N] u;

    real ugpa0;
    real eta_u_ugpa;
    real lsat0;
    real eta_u_lsat;
    real eta_u_zfya;

    vector[K] eta_a_ugpa;
    vector[K] eta_a_lsat;
    vector[K] eta_a_zfya;

    real<lower = 0> sigma_g_Sq_gpa;
}

model {
    u ~ uniform(-2, 2);

    ugpa0 ~ normal(0, 1);
    eta_u_ugpa ~ normal(0, 1);
    lsat0 ~ normal(0, 1);
    eta_u_lsat ~ normal(0, 1);
    eta_u_zfya ~ normal(0, 1);

    eta_a_ugpa ~ normal(zero_K, one_K);
    eta_a_lsat ~ normal(zero_K, one_K);
    eta_a_zfya ~ normal(zero_K, one_K);

    //ugpa ~ normal(ugpa0 + eta_u_ugpa * u + a * eta_a_ugpa, 0.00001);
    //lsat ~ normal(exp(lsat0 + eta_u_lsat * u + a * eta_a_lsat), 0.00001);
    //zfya ~ normal(eta_u_zfya * u + a * eta_a_zfya, 0.00001);
    ugpa ~ normal(ugpa0 + eta_u_ugpa * u + a * eta_a_ugpa, 0.1);
    lsat ~ normal(exp(lsat0 + eta_u_lsat * u + a * eta_a_lsat), 0.1);
    zfya ~ normal(eta_u_zfya * u + a * eta_a_zfya, 0.1);
}
Building...

Building: found in cache, done.Messages from stanc:
Warning in '/tmp/httpstan_1gnn9d5y/model_diqii4mv.stan', line 35, column 4: Parameter
    u is given a uniform distribution. The uniform distribution is not
    recommended, for two reasons: (a) Except when there are logical or
    physical constraints, it is very unusual for you to be sure that a
    parameter will fall inside a specified range, and (b) The infinite
    gradient induced by a uniform density can cause difficulties for Stan's
    sampling algorithm. As a consequence, we recommend soft constraints
    rather than hard constraints; for example, instead of giving an
    elasticity parameter a uniform(0,1) distribution, try normal(0.5,0.5).
Warning: The parameter sigma_g_Sq_gpa was declared but was not used in the
    density calculation.
Warning: The parameter eta_a_zfya has no priors. This means either no prior
    is provided, or the prior(s) depend on data variables. In the later case,
    this may be a false positive.
Warning: The parameter eta_a_ugpa has no priors. This means either no prior
    is provided, or the prior(s) depend on data variables. In the later case,
    this may be a false positive.
Warning: The parameter eta_a_lsat has no priors. This means either no prior
    is provided, or the prior(s) depend on data variables. In the later case,
    this may be a false positive.

Sampling:   0% (1/8000)
Sampling:   0% (2/8000)
Sampling:   0% (3/8000)
Sampling:   0% (4/8000)Sampling:   0%model code =
 data {
    int<lower = 0> N; // number of observation
    int<lower = 0> K; // number of covariates
    matrix[N, K]   a; // sensitive attributes
    array[N] real  ugpa; // UGPA
    array[N] real   lsat; //LSAT
    array[N] real  zfya; // ZFYA
}

transformed data {
    vector[K] zero_K;
    vector[K] one_K;

    zero_K = rep_vector(0, K);
    one_K = rep_vector(1, K);
}

parameters {
    vector[N] u;

    real ugpa0;
    real eta_u_ugpa;
    real lsat0;
    real eta_u_lsat;
    real eta_u_zfya;

    vector[K] eta_a_ugpa;
    vector[K] eta_a_lsat;
    vector[K] eta_a_zfya;

    real<lower = 0> sigma_g_Sq_gpa;
}

model {
    u ~ uniform(-2, 2);

    ugpa0 ~ normal(0, 1);
    eta_u_ugpa ~ normal(0, 1);
    lsat0 ~ normal(0, 1);
    eta_u_lsat ~ normal(0, 1);
    eta_u_zfya ~ normal(0, 1);

    eta_a_ugpa ~ normal(zero_K, one_K);
    eta_a_lsat ~ normal(zero_K, one_K);
    eta_a_zfya ~ normal(zero_K, one_K);

    //ugpa ~ normal(ugpa0 + eta_u_ugpa * u + a * eta_a_ugpa, 0.00001);
    //lsat ~ normal(exp(lsat0 + eta_u_lsat * u + a * eta_a_lsat), 0.00001);
    //zfya ~ normal(eta_u_zfya * u + a * eta_a_zfya, 0.00001);
    ugpa ~ normal(ugpa0 + eta_u_ugpa * u + a * eta_a_ugpa, 0.1);
    lsat ~ normal(exp(lsat0 + eta_u_lsat * u + a * eta_a_lsat), 0.1);
    zfya ~ normal(eta_u_zfya * u + a * eta_a_zfya, 0.1);
}
Building...Finished compiling model!


Building: found in cache, done.Messages from stanc:
Warning in '/tmp/httpstan_1gnn9d5y/model_diqii4mv.stan', line 35, column 4: Parameter
    u is given a uniform distribution. The uniform distribution is not
    recommended, for two reasons: (a) Except when there are logical or
    physical constraints, it is very unusual for you to be sure that a
    parameter will fall inside a specified range, and (b) The infinite
    gradient induced by a uniform density can cause difficulties for Stan's
    sampling algorithm. As a consequence, we recommend soft constraints
    rather than hard constraints; for example, instead of giving an
    elasticity parameter a uniform(0,1) distribution, try normal(0.5,0.5).
Warning: The parameter sigma_g_Sq_gpa was declared but was not used in the
    density calculation.
Warning: The parameter eta_a_zfya has no priors. This means either no prior
    is provided, or the prior(s) depend on data variables. In the later case,
    this may be a false positive.
Warning: The parameter eta_a_ugpa has no priors. This means either no prior
    is provided, or the prior(s) depend on data variables. In the later case,
    this may be a false positive.
Warning: The parameter eta_a_lsat has no priors. This means either no prior
    is provided, or the prior(s) depend on data variables. In the later case,
    this may be a false positive.

Sampling:   0% (1/8000)
Sampling:   0% (2/8000)
Sampling:   0% (3/8000)
Sampling:   0% (4/8000)model code =
 data {
    int<lower = 0> N; // number of observation
    int<lower = 0> K; // number of covariates
    matrix[N, K]   a; // sensitive attributes
    array[N] real  ugpa; // UGPA
    array[N] real   lsat; //LSAT
    array[N] real  zfya; // ZFYA
}

transformed data {
    vector[K] zero_K;
    vector[K] one_K;

    zero_K = rep_vector(0, K);
    one_K = rep_vector(1, K);
}

parameters {
    vector[N] u;

    real ugpa0;
    real eta_u_ugpa;
    real lsat0;
    real eta_u_lsat;
    real eta_u_zfya;

    vector[K] eta_a_ugpa;
    vector[K] eta_a_lsat;
    vector[K] eta_a_zfya;

    real<lower = 0> sigma_g_Sq_gpa;
}

model {
    u ~ uniform(-2, 2);

    ugpa0 ~ normal(0, 1);
    eta_u_ugpa ~ normal(0, 1);
    lsat0 ~ normal(0, 1);
    eta_u_lsat ~ normal(0, 1);
    eta_u_zfya ~ normal(0, 1);

    eta_a_ugpa ~ normal(zero_K, one_K);
    eta_a_lsat ~ normal(zero_K, one_K);
    eta_a_zfya ~ normal(zero_K, one_K);

    //ugpa ~ normal(ugpa0 + eta_u_ugpa * u + a * eta_a_ugpa, 0.00001);
    //lsat ~ normal(exp(lsat0 + eta_u_lsat * u + a * eta_a_lsat), 0.00001);
    //zfya ~ normal(eta_u_zfya * u + a * eta_a_zfya, 0.00001);
    ugpa ~ normal(ugpa0 + eta_u_ugpa * u + a * eta_a_ugpa, 0.1);
    lsat ~ normal(exp(lsat0 + eta_u_lsat * u + a * eta_a_lsat), 0.1);
    zfya ~ normal(eta_u_zfya * u + a * eta_a_zfya, 0.1);
}
Building...Sampling:   0%Finished compiling model!


Building: found in cache, done.Messages from stanc:
Warning in '/tmp/httpstan_1gnn9d5y/model_diqii4mv.stan', line 35, column 4: Parameter
    u is given a uniform distribution. The uniform distribution is not
    recommended, for two reasons: (a) Except when there are logical or
    physical constraints, it is very unusual for you to be sure that a
    parameter will fall inside a specified range, and (b) The infinite
    gradient induced by a uniform density can cause difficulties for Stan's
    sampling algorithm. As a consequence, we recommend soft constraints
    rather than hard constraints; for example, instead of giving an
    elasticity parameter a uniform(0,1) distribution, try normal(0.5,0.5).
Warning: The parameter sigma_g_Sq_gpa was declared but was not used in the
    density calculation.
Warning: The parameter eta_a_zfya has no priors. This means either no prior
    is provided, or the prior(s) depend on data variables. In the later case,
    this may be a false positive.
Warning: The parameter eta_a_ugpa has no priors. This means either no prior
    is provided, or the prior(s) depend on data variables. In the later case,
    this may be a false positive.
Warning: The parameter eta_a_lsat has no priors. This means either no prior
    is provided, or the prior(s) depend on data variables. In the later case,
    this may be a false positive.

Sampling:   1% (103/8000)
Sampling:   0% (1/8000)
Sampling:   0% (2/8000)Sampling:   0%
Sampling:   0% (3/8000)Finished compiling model!

Sampling:   0% (4/8000)
Sampling:   0% (1/8000)
Sampling:   0% (2/8000)
Sampling:   0% (3/8000)
Sampling:   3% (202/8000)
Sampling:   0% (4/8000)
Sampling:   1% (103/8000)
Sampling:   1% (103/8000)
Sampling:   3% (202/8000)
Sampling:   3% (202/8000)
Sampling:   1% (103/8000)
Sampling:   1% (103/8000)
Sampling:   3% (202/8000)
Sampling:   4% (301/8000)
Sampling:   4% (301/8000)
Sampling:   5% (400/8000)
Sampling:   6% (500/8000)
Sampling:   5% (400/8000)
Sampling:   4% (301/8000)
Sampling:   5% (401/8000)
Sampling:   4% (301/8000)
Sampling:   5% (401/8000)
Sampling:   3% (202/8000)
Sampling:   8% (600/8000)
Sampling:   6% (501/8000)
Sampling:   4% (301/8000)
Sampling:   6% (501/8000)
Sampling:   6% (500/8000)
Sampling:   8% (600/8000)
Sampling:   9% (700/8000)
Sampling:   8% (600/8000)
Sampling:   8% (600/8000)
Sampling:   9% (700/8000)
Sampling:   5% (400/8000)
Sampling:  10% (800/8000)
Sampling:   9% (700/8000)
Sampling:  10% (800/8000)
Sampling:   6% (500/8000)
Sampling:  10% (800/8000)
Sampling:  11% (900/8000)
Sampling:   9% (700/8000)
Sampling:  11% (900/8000)
Sampling:  10% (800/8000)
Sampling:  12% (1000/8000)
Sampling:  11% (900/8000)
Sampling:  11% (900/8000)
Sampling:   8% (600/8000)
Sampling:  12% (1000/8000)
Sampling:  12% (1000/8000)
Sampling:  14% (1100/8000)
Sampling:   9% (700/8000)
Sampling:  14% (1100/8000)
Sampling:  14% (1100/8000)
Sampling:  15% (1200/8000)
Sampling:  12% (1000/8000)
Sampling:  15% (1200/8000)
Sampling:  14% (1100/8000)
Sampling:  16% (1300/8000)
Sampling:  15% (1200/8000)
Sampling:  15% (1200/8000)
Sampling:  16% (1300/8000)
Sampling:  10% (800/8000)
Sampling:  16% (1300/8000)
Sampling:  16% (1300/8000)
Sampling:  11% (900/8000)
Sampling:  18% (1400/8000)
Sampling:  18% (1400/8000)
Sampling:  19% (1500/8000)
Sampling:  18% (1400/8000)
Sampling:  18% (1400/8000)
Sampling:  20% (1600/8000)
Sampling:  19% (1500/8000)
Sampling:  20% (1600/8000)
Sampling:  19% (1500/8000)
Sampling:  12% (1000/8000)
Sampling:  19% (1500/8000)
Sampling:  20% (1600/8000)
Sampling:  21% (1700/8000)
Sampling:  21% (1700/8000)
Sampling:  21% (1700/8000)
Sampling:  20% (1600/8000)
Sampling:  21% (1700/8000)
Sampling:  14% (1100/8000)
Sampling:  22% (1800/8000)
Sampling:  22% (1800/8000)
Sampling:  24% (1900/8000)
Sampling:  22% (1800/8000)
Sampling:  22% (1800/8000)
Sampling:  24% (1900/8000)
Sampling:  15% (1200/8000)
Sampling:  24% (1900/8000)
Sampling:  25% (2000/8000)
Sampling:  16% (1300/8000)
Sampling:  18% (1400/8000)
Sampling:  25% (2000/8000)
Sampling:  24% (1900/8000)
Sampling:  25% (2000/8000)
Sampling:  26% (2100/8000)
Sampling:  26% (2101/8000)
Sampling:  25% (2000/8000)
Sampling:  26% (2100/8000)
Sampling:  26% (2100/8000)
Sampling:  28% (2200/8000)
Sampling:  26% (2100/8000)
Sampling:  29% (2300/8000)
Sampling:  28% (2200/8000)
Sampling:  28% (2201/8000)
Sampling:  28% (2200/8000)
Sampling:  19% (1500/8000)
Sampling:  30% (2400/8000)
Sampling:  29% (2300/8000)
Sampling:  30% (2400/8000)
Sampling:  29% (2300/8000)
Sampling:  31% (2500/8000)
Sampling:  29% (2301/8000)
Sampling:  31% (2500/8000)
Sampling:  30% (2400/8000)
Sampling:  30% (2401/8000)
Sampling:  20% (1600/8000)
Sampling:  32% (2600/8000)
Sampling:  34% (2700/8000)
Sampling:  32% (2600/8000)
Sampling:  33% (2601/8000)
Sampling:  34% (2701/8000)
Sampling:  21% (1700/8000)
Sampling:  31% (2500/8000)
Sampling:  31% (2500/8000)
Sampling:  22% (1800/8000)
Sampling:  35% (2801/8000)
Sampling:  32% (2600/8000)
Sampling:  24% (1900/8000)
Sampling:  34% (2700/8000)
Sampling:  35% (2800/8000)
Sampling:  32% (2600/8000)
Sampling:  34% (2700/8000)
Sampling:  36% (2900/8000)
Sampling:  35% (2800/8000)
Sampling:  38% (3000/8000)
Sampling:  25% (2000/8000)
Sampling:  26% (2100/8000)
Sampling:  36% (2900/8000)
Sampling:  36% (2900/8000)
Sampling:  28% (2201/8000)
Sampling:  29% (2301/8000)
Sampling:  39% (3100/8000)
Sampling:  35% (2800/8000)
Sampling:  38% (3000/8000)
Sampling:  38% (3001/8000)
Sampling:  38% (3000/8000)
Sampling:  39% (3100/8000)
Sampling:  36% (2900/8000)
Sampling:  39% (3101/8000)
Sampling:  40% (3200/8000)
Sampling:  39% (3101/8000)
Sampling:  40% (3201/8000)
Sampling:  30% (2401/8000)
Sampling:  40% (3201/8000)
Sampling:  31% (2500/8000)
Sampling:  41% (3300/8000)
Sampling:  38% (3000/8000)
Sampling:  32% (2600/8000)